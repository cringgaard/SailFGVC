Loading cached split indices for dataset at C:\Users\chris\.cache\huggingface\datasets\cringgaard___boats_dataset\default\0.0.0\cf118af5708518fea28486aed25e2f1632c5b8d5e716255a840c5c012a2b161b\cache-dd4e4efd60e5a362.arrow and C:\Users\chris\.cache\huggingface\datasets\cringgaard___boats_dataset\default\0.0.0\cf118af5708518fea28486aed25e2f1632c5b8d5e716255a840c5c012a2b161b\cache-5ba6bd0f06b77e36.arrow
Loading cached processed dataset at C:\Users\chris\.cache\huggingface\datasets\cringgaard___boats_dataset\default\0.0.0\cf118af5708518fea28486aed25e2f1632c5b8d5e716255a840c5c012a2b161b\cache-a018fc98cfdb169d.arrow
Loading cached processed dataset at C:\Users\chris\.cache\huggingface\datasets\cringgaard___boats_dataset\default\0.0.0\cf118af5708518fea28486aed25e2f1632c5b8d5e716255a840c5c012a2b161b\cache-4ebb0ed8bf80df99.arrow
loading configuration file config.json from cache at C:\Users\chris/.cache\huggingface\hub\models--google--vit-base-patch16-224\snapshots\2ddc9d4e473d7ba52128f0df4723e478fa14fb80\config.json
Model config ViTConfig {
  "_name_or_path": "google/vit-base-patch16-224",
  "architectures": [
    "ViTForImageClassification"
  ],
  "attention_probs_dropout_prob": 0.0,
  "encoder_stride": 16,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31",
    "32": "LABEL_32",
    "33": "LABEL_33",
    "34": "LABEL_34",
    "35": "LABEL_35",
    "36": "LABEL_36",
    "37": "LABEL_37",
    "38": "LABEL_38",
    "39": "LABEL_39",
    "40": "LABEL_40",
    "41": "LABEL_41",
    "42": "LABEL_42",
    "43": "LABEL_43",
    "44": "LABEL_44",
    "45": "LABEL_45",
    "46": "LABEL_46",
    "47": "LABEL_47",
    "48": "LABEL_48",
    "49": "LABEL_49",
    "50": "LABEL_50",
    "51": "LABEL_51",
    "52": "LABEL_52",
    "53": "LABEL_53",
    "54": "LABEL_54",
    "55": "LABEL_55",
    "56": "LABEL_56",
    "57": "LABEL_57",
    "58": "LABEL_58",
    "59": "LABEL_59",
    "60": "LABEL_60",
    "61": "LABEL_61",
    "62": "LABEL_62",
    "63": "LABEL_63",
    "64": "LABEL_64",
    "65": "LABEL_65",
    "66": "LABEL_66",
    "67": "LABEL_67",
    "68": "LABEL_68",
    "69": "LABEL_69",
    "70": "LABEL_70",
    "71": "LABEL_71",
    "72": "LABEL_72",
    "73": "LABEL_73",
    "74": "LABEL_74",
    "75": "LABEL_75",
    "76": "LABEL_76",
    "77": "LABEL_77",
    "78": "LABEL_78",
    "79": "LABEL_79",
    "80": "LABEL_80",
    "81": "LABEL_81",
    "82": "LABEL_82",
    "83": "LABEL_83",
    "84": "LABEL_84",
    "85": "LABEL_85",
    "86": "LABEL_86",
    "87": "LABEL_87",
    "88": "LABEL_88",
    "89": "LABEL_89",
    "90": "LABEL_90",
    "91": "LABEL_91",
    "92": "LABEL_92",
    "93": "LABEL_93",
    "94": "LABEL_94",
    "95": "LABEL_95"
  },
  "image_size": 224,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_32": 32,
    "LABEL_33": 33,
    "LABEL_34": 34,
    "LABEL_35": 35,
    "LABEL_36": 36,
    "LABEL_37": 37,
    "LABEL_38": 38,
    "LABEL_39": 39,
    "LABEL_4": 4,
    "LABEL_40": 40,
    "LABEL_41": 41,
    "LABEL_42": 42,
    "LABEL_43": 43,
    "LABEL_44": 44,
    "LABEL_45": 45,
    "LABEL_46": 46,
    "LABEL_47": 47,
    "LABEL_48": 48,
    "LABEL_49": 49,
    "LABEL_5": 5,
    "LABEL_50": 50,
    "LABEL_51": 51,
    "LABEL_52": 52,
    "LABEL_53": 53,
    "LABEL_54": 54,
    "LABEL_55": 55,
    "LABEL_56": 56,
    "LABEL_57": 57,
    "LABEL_58": 58,
    "LABEL_59": 59,
    "LABEL_6": 6,
    "LABEL_60": 60,
    "LABEL_61": 61,
    "LABEL_62": 62,
    "LABEL_63": 63,
    "LABEL_64": 64,
    "LABEL_65": 65,
    "LABEL_66": 66,
    "LABEL_67": 67,
    "LABEL_68": 68,
    "LABEL_69": 69,
    "LABEL_7": 7,
    "LABEL_70": 70,
    "LABEL_71": 71,
    "LABEL_72": 72,
    "LABEL_73": 73,
    "LABEL_74": 74,
    "LABEL_75": 75,
    "LABEL_76": 76,
    "LABEL_77": 77,
    "LABEL_78": 78,
    "LABEL_79": 79,
    "LABEL_8": 8,
    "LABEL_80": 80,
    "LABEL_81": 81,
    "LABEL_82": 82,
    "LABEL_83": 83,
    "LABEL_84": 84,
    "LABEL_85": 85,
    "LABEL_86": 86,
    "LABEL_87": 87,
    "LABEL_88": 88,
    "LABEL_89": 89,
    "LABEL_9": 9,
    "LABEL_90": 90,
    "LABEL_91": 91,
    "LABEL_92": 92,
    "LABEL_93": 93,
    "LABEL_94": 94,
    "LABEL_95": 95
  },
  "layer_norm_eps": 1e-12,
  "model_type": "vit",
  "num_attention_heads": 12,
  "num_channels": 3,
  "num_hidden_layers": 12,
  "patch_size": 16,
  "qkv_bias": true,
  "transformers_version": "4.26.1"
}
loading weights file pytorch_model.bin from cache at C:\Users\chris/.cache\huggingface\hub\models--google--vit-base-patch16-224\snapshots\2ddc9d4e473d7ba52128f0df4723e478fa14fb80\pytorch_model.bin
All model checkpoint weights were used when initializing ViTForImageClassification.
Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:
- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([96, 768]) in the model instantiated
- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([96]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
Setting `WANDB_LOG_MODEL` from true to `end` instead
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 8323
  Num Epochs = 15
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 4
  Total optimization steps = 1950
  Number of trainable parameters = 85872480
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
[0, 1, 2, 4, 5, 6, 7, 9, 11, 15, 19, 20, 21, 22, 23, 25, 26, 27, 28, 30, 31, 34, 37, 44, 45, 49, 53, 56, 57, 58, 59, 60, 61, 69, 76, 81, 82, 83, 85, 89, 90]
[81, 56, 85, 30, 23]
['Lead', 'Iron', 'NaN', 'Lead + Water', 'Iron or Lead', 'Lead/Water', 'Lead (internal)', 'Lead or Iron', 'Aluminum Centerboard', 'Cast iron', 'Cement and Ferrous material', 'lead', 'Lead/Iron', 'Lead bulb with cast iron fin', 'Optional sand bags', 'Steel fin / lead bulb', 'Blade: cast iron; Bulb: lead', 'Keel blade cast iron; bulb lead', 'Steel fin with lead bulb', 'Cast Iron', 'Galvanized and polyester shot', 'iron', 'Iron/Lead', 'Lead w/ steel cb', 'Galvanized iron', 'Water', 'Steel centerboard', 'Concrete', 'Lead bulb', 'Cast iron with lead fill', 'Lead and water', 'Steel', 'Lead; galvanized steel center plate', 'Iron /Lead', 'Lead/ Iron', 'Cast iron (standard keel)', 'Gal. steel', 'Lead Bulb', 'Lea', 'Case iron', 'Aluminum centerboard', 'cast iron', '1994.0', 'Iron / Steel', 'Cast iron keel with mixed cast iron/lead ballast', 'water + iron', 'Cast iron with lead bulb', 'Iron keel with lead bulb', 'Keel built in lead, iron and epoxy GRP', 'Cast iron fin with lead bulb', 'Steel and Lead', 'Iron fin with lead bulb', 'Cast iron + lead', 'Cast iron fin with head bulb', 'Lead ballast', 'Incapsulated steel', 'Galv. steel', 'Lead and iron', 'Iron or lead', 'Cast iron fin / lead bulb', 'Stone', 'Lead internal', 'Iron/lead', 'Stainless steel fin / lead bulb', '2000.0', 'lead/iron', 'Cast iron centerboard', 'Cast iron keel', 'Galvanized steel centerboard', 'Internal: cast iron; Centerboard: galvanized steel', 'Cast iron and galvanized steel', 'Cast iron and lead', 'Lead,water', 'Concrete*', 'Lead/water', 'Cast', 'Cast lead', 'Iron.', 'Lead on Iron', 'Cast steel and/pr lead', 'Cast steel and/or lead', 'Cast iron and/or lead', 'Cast iron fin/lead bulb', 'Cast iron fin; lead bulb', 'Iron/SS', 'lead/concrete', 'Iron (lead optional)', 'Cast iron shot', 'Anything', 'Iron and steel', 'Varies', 'Cast iron swing keel', 'Stainless steel', '87.5% cast iron, 12.5% lead', 'Lead + iron', 'Water Ballast']
{'loss': 4.4648, 'learning_rate': 2.564102564102564e-06, 'epoch': 0.08}
{'loss': 4.1479, 'learning_rate': 5.128205128205128e-06, 'epoch': 0.15}
{'loss': 3.563, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.23}
{'loss': 2.794, 'learning_rate': 1.0256410256410256e-05, 'epoch': 0.31}
{'loss': 2.1619, 'learning_rate': 1.282051282051282e-05, 'epoch': 0.38}
{'loss': 1.528, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.46}
{'loss': 1.2714, 'learning_rate': 1.794871794871795e-05, 'epoch': 0.54}
{'loss': 1.1861, 'learning_rate': 2.0512820512820512e-05, 'epoch': 0.61}
{'loss': 1.1525, 'learning_rate': 2.307692307692308e-05, 'epoch': 0.69}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 1.252, 'learning_rate': 2.564102564102564e-05, 'epoch': 0.77}
{'loss': 1.0867, 'learning_rate': 2.8205128205128207e-05, 'epoch': 0.84}
{'loss': 1.2508, 'learning_rate': 3.0769230769230774e-05, 'epoch': 0.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 1.0987, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Ballast Type\checkpoint-130
Configuration saved in models/ViT_Ballast Type\checkpoint-130\config.json
Model weights saved in models/ViT_Ballast Type\checkpoint-130\pytorch_model.bin
Image processor saved in models/ViT_Ballast Type\checkpoint-130\preprocessor_config.json
{'eval_loss': 1.2063839435577393, 'eval_accuracy': 0.493993272465161, 'eval_f1': 0.024508849075309633, 'eval_precision': 0.02416026415949095, 'eval_recall': 0.028471782771941876, 'eval_runtime': 16.4075, 'eval_samples_per_second': 126.832, 'eval_steps_per_second': 7.984, 'epoch': 1.0}
{'loss': 1.235, 'learning_rate': 3.58974358974359e-05, 'epoch': 1.08}
{'loss': 1.1624, 'learning_rate': 3.846153846153846e-05, 'epoch': 1.15}
{'loss': 1.1094, 'learning_rate': 4.1025641025641023e-05, 'epoch': 1.23}
{'loss': 1.1454, 'learning_rate': 4.358974358974359e-05, 'epoch': 1.31}
{'loss': 1.117, 'learning_rate': 4.615384615384616e-05, 'epoch': 1.38}
{'loss': 1.1547, 'learning_rate': 4.871794871794872e-05, 'epoch': 1.46}
{'loss': 1.0886, 'learning_rate': 4.985754985754986e-05, 'epoch': 1.54}
{'loss': 1.1708, 'learning_rate': 4.9572649572649575e-05, 'epoch': 1.61}
{'loss': 1.057, 'learning_rate': 4.928774928774929e-05, 'epoch': 1.69}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 1.1725, 'learning_rate': 4.9002849002849004e-05, 'epoch': 1.77}
{'loss': 1.0539, 'learning_rate': 4.871794871794872e-05, 'epoch': 1.84}
{'loss': 1.1275, 'learning_rate': 4.8433048433048433e-05, 'epoch': 1.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 1.1206, 'learning_rate': 4.814814814814815e-05, 'epoch': 2.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Ballast Type\checkpoint-260
Configuration saved in models/ViT_Ballast Type\checkpoint-260\config.json
Model weights saved in models/ViT_Ballast Type\checkpoint-260\pytorch_model.bin
Image processor saved in models/ViT_Ballast Type\checkpoint-260\preprocessor_config.json
{'eval_loss': 1.0888159275054932, 'eval_accuracy': 0.6444017299375301, 'eval_f1': 0.02399299648122908, 'eval_precision': 0.02621802309573087, 'eval_recall': 0.026373231903186342, 'eval_runtime': 16.5528, 'eval_samples_per_second': 125.719, 'eval_steps_per_second': 7.914, 'epoch': 2.0}
{'loss': 1.1236, 'learning_rate': 4.786324786324787e-05, 'epoch': 2.08}
{'loss': 1.2023, 'learning_rate': 4.7578347578347584e-05, 'epoch': 2.15}
{'loss': 1.084, 'learning_rate': 4.72934472934473e-05, 'epoch': 2.23}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 1.0048, 'learning_rate': 4.700854700854701e-05, 'epoch': 2.31}
{'loss': 1.0436, 'learning_rate': 4.672364672364672e-05, 'epoch': 2.38}
{'loss': 1.1604, 'learning_rate': 4.643874643874644e-05, 'epoch': 2.46}
{'loss': 1.0482, 'learning_rate': 4.615384615384616e-05, 'epoch': 2.54}
{'loss': 1.0724, 'learning_rate': 4.586894586894587e-05, 'epoch': 2.61}
{'loss': 1.1059, 'learning_rate': 4.558404558404559e-05, 'epoch': 2.69}
{'loss': 1.1369, 'learning_rate': 4.52991452991453e-05, 'epoch': 2.77}
{'loss': 1.0975, 'learning_rate': 4.501424501424502e-05, 'epoch': 2.84}
{'loss': 1.0709, 'learning_rate': 4.472934472934473e-05, 'epoch': 2.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 1.0188, 'learning_rate': 4.4444444444444447e-05, 'epoch': 3.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Ballast Type\checkpoint-390
Configuration saved in models/ViT_Ballast Type\checkpoint-390\config.json
Model weights saved in models/ViT_Ballast Type\checkpoint-390\pytorch_model.bin
Image processor saved in models/ViT_Ballast Type\checkpoint-390\preprocessor_config.json
{'eval_loss': 1.0679458379745483, 'eval_accuracy': 0.6376741950985103, 'eval_f1': 0.028119371252089118, 'eval_precision': 0.05114729173265758, 'eval_recall': 0.029058735341865805, 'eval_runtime': 16.2147, 'eval_samples_per_second': 128.34, 'eval_steps_per_second': 8.079, 'epoch': 3.0}
{'loss': 1.0288, 'learning_rate': 4.415954415954416e-05, 'epoch': 3.08}
{'loss': 1.0274, 'learning_rate': 4.3874643874643876e-05, 'epoch': 3.15}
{'loss': 1.1033, 'learning_rate': 4.358974358974359e-05, 'epoch': 3.23}
{'loss': 0.9999, 'learning_rate': 4.3304843304843306e-05, 'epoch': 3.31}
{'loss': 1.0461, 'learning_rate': 4.301994301994302e-05, 'epoch': 3.38}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 1.0642, 'learning_rate': 4.2735042735042735e-05, 'epoch': 3.46}
{'loss': 0.9846, 'learning_rate': 4.2450142450142457e-05, 'epoch': 3.54}
{'loss': 0.9901, 'learning_rate': 4.216524216524217e-05, 'epoch': 3.61}
{'loss': 1.0149, 'learning_rate': 4.1880341880341886e-05, 'epoch': 3.69}
{'loss': 0.9877, 'learning_rate': 4.15954415954416e-05, 'epoch': 3.77}
{'loss': 1.0388, 'learning_rate': 4.131054131054131e-05, 'epoch': 3.84}
{'loss': 1.1039, 'learning_rate': 4.1025641025641023e-05, 'epoch': 3.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 1.0583, 'learning_rate': 4.074074074074074e-05, 'epoch': 4.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Ballast Type\checkpoint-520
Configuration saved in models/ViT_Ballast Type\checkpoint-520\config.json
Model weights saved in models/ViT_Ballast Type\checkpoint-520\pytorch_model.bin
Image processor saved in models/ViT_Ballast Type\checkpoint-520\preprocessor_config.json
{'eval_loss': 1.0645204782485962, 'eval_accuracy': 0.6554541086016338, 'eval_f1': 0.024584315336753943, 'eval_precision': 0.0650682801921954, 'eval_recall': 0.026863008272700415, 'eval_runtime': 16.2171, 'eval_samples_per_second': 128.322, 'eval_steps_per_second': 8.078, 'epoch': 4.0}
{'loss': 1.019, 'learning_rate': 4.045584045584046e-05, 'epoch': 4.08}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.955, 'learning_rate': 4.0170940170940174e-05, 'epoch': 4.15}
{'loss': 0.9901, 'learning_rate': 3.988603988603989e-05, 'epoch': 4.23}
{'loss': 0.9779, 'learning_rate': 3.9601139601139604e-05, 'epoch': 4.31}
{'loss': 1.0169, 'learning_rate': 3.931623931623932e-05, 'epoch': 4.38}
{'loss': 0.9127, 'learning_rate': 3.903133903133903e-05, 'epoch': 4.46}
{'loss': 0.9444, 'learning_rate': 3.874643874643875e-05, 'epoch': 4.54}
{'loss': 0.9575, 'learning_rate': 3.846153846153846e-05, 'epoch': 4.61}
{'loss': 1.0836, 'learning_rate': 3.817663817663818e-05, 'epoch': 4.69}
{'loss': 1.059, 'learning_rate': 3.789173789173789e-05, 'epoch': 4.77}
{'loss': 0.9125, 'learning_rate': 3.760683760683761e-05, 'epoch': 4.84}
{'loss': 1.0467, 'learning_rate': 3.732193732193732e-05, 'epoch': 4.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 1.0983, 'learning_rate': 3.7037037037037037e-05, 'epoch': 5.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Ballast Type\checkpoint-650
Configuration saved in models/ViT_Ballast Type\checkpoint-650\config.json
Model weights saved in models/ViT_Ballast Type\checkpoint-650\pytorch_model.bin
Image processor saved in models/ViT_Ballast Type\checkpoint-650\preprocessor_config.json
{'eval_loss': 1.061731219291687, 'eval_accuracy': 0.6352715040845748, 'eval_f1': 0.028684363767474328, 'eval_precision': 0.040800221341048486, 'eval_recall': 0.029205928828545866, 'eval_runtime': 16.2702, 'eval_samples_per_second': 127.902, 'eval_steps_per_second': 8.052, 'epoch': 5.0}
{'loss': 0.9569, 'learning_rate': 3.675213675213676e-05, 'epoch': 5.08}
{'loss': 0.8866, 'learning_rate': 3.646723646723647e-05, 'epoch': 5.15}
{'loss': 0.8978, 'learning_rate': 3.618233618233619e-05, 'epoch': 5.23}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.916, 'learning_rate': 3.58974358974359e-05, 'epoch': 5.31}
{'loss': 1.0011, 'learning_rate': 3.561253561253561e-05, 'epoch': 5.38}
{'loss': 0.9227, 'learning_rate': 3.5327635327635325e-05, 'epoch': 5.46}
{'loss': 1.0336, 'learning_rate': 3.504273504273504e-05, 'epoch': 5.54}
{'loss': 0.9649, 'learning_rate': 3.475783475783476e-05, 'epoch': 5.61}
{'loss': 0.979, 'learning_rate': 3.4472934472934476e-05, 'epoch': 5.69}
{'loss': 0.9508, 'learning_rate': 3.418803418803419e-05, 'epoch': 5.77}
{'loss': 0.9463, 'learning_rate': 3.3903133903133905e-05, 'epoch': 5.84}
{'loss': 0.931, 'learning_rate': 3.361823361823362e-05, 'epoch': 5.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 0.9446, 'learning_rate': 3.3333333333333335e-05, 'epoch': 6.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Ballast Type\checkpoint-780
Configuration saved in models/ViT_Ballast Type\checkpoint-780\config.json
Model weights saved in models/ViT_Ballast Type\checkpoint-780\pytorch_model.bin
Image processor saved in models/ViT_Ballast Type\checkpoint-780\preprocessor_config.json
{'eval_loss': 1.0987049341201782, 'eval_accuracy': 0.6093224411340702, 'eval_f1': 0.02863480292318366, 'eval_precision': 0.05008236821086262, 'eval_recall': 0.029709892745992887, 'eval_runtime': 16.5525, 'eval_samples_per_second': 125.721, 'eval_steps_per_second': 7.914, 'epoch': 6.0}
{'loss': 0.9247, 'learning_rate': 3.304843304843305e-05, 'epoch': 6.08}
{'loss': 0.9066, 'learning_rate': 3.2763532763532764e-05, 'epoch': 6.15}
{'loss': 0.8671, 'learning_rate': 3.247863247863248e-05, 'epoch': 6.23}
{'loss': 0.8084, 'learning_rate': 3.2193732193732194e-05, 'epoch': 6.31}
{'loss': 0.928, 'learning_rate': 3.190883190883191e-05, 'epoch': 6.38}
{'loss': 0.8764, 'learning_rate': 3.162393162393162e-05, 'epoch': 6.46}
{'loss': 0.8774, 'learning_rate': 3.133903133903134e-05, 'epoch': 6.54}
{'loss': 0.8797, 'learning_rate': 3.105413105413106e-05, 'epoch': 6.61}
{'loss': 0.921, 'learning_rate': 3.0769230769230774e-05, 'epoch': 6.69}
{'loss': 0.8225, 'learning_rate': 3.0484330484330486e-05, 'epoch': 6.77}
{'loss': 0.9348, 'learning_rate': 3.01994301994302e-05, 'epoch': 6.84}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.996, 'learning_rate': 2.9914529914529915e-05, 'epoch': 6.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 0.9003, 'learning_rate': 2.962962962962963e-05, 'epoch': 7.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Ballast Type\checkpoint-910
Configuration saved in models/ViT_Ballast Type\checkpoint-910\config.json
Model weights saved in models/ViT_Ballast Type\checkpoint-910\pytorch_model.bin
Image processor saved in models/ViT_Ballast Type\checkpoint-910\preprocessor_config.json
{'eval_loss': 1.0923269987106323, 'eval_accuracy': 0.6429601153291686, 'eval_f1': 0.025110104602217204, 'eval_precision': 0.04066327115988118, 'eval_recall': 0.0268669684153812, 'eval_runtime': 16.4767, 'eval_samples_per_second': 126.299, 'eval_steps_per_second': 7.951, 'epoch': 7.0}
{'loss': 0.8465, 'learning_rate': 2.9344729344729345e-05, 'epoch': 7.08}
{'loss': 0.8458, 'learning_rate': 2.9059829059829063e-05, 'epoch': 7.15}
{'loss': 0.7959, 'learning_rate': 2.8774928774928778e-05, 'epoch': 7.23}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.9208, 'learning_rate': 2.8490028490028492e-05, 'epoch': 7.31}
{'loss': 0.8718, 'learning_rate': 2.8205128205128207e-05, 'epoch': 7.38}
{'loss': 0.9049, 'learning_rate': 2.7920227920227922e-05, 'epoch': 7.46}
{'loss': 0.8352, 'learning_rate': 2.7635327635327633e-05, 'epoch': 7.54}
{'loss': 0.8338, 'learning_rate': 2.7350427350427355e-05, 'epoch': 7.61}
{'loss': 0.7971, 'learning_rate': 2.706552706552707e-05, 'epoch': 7.69}
{'loss': 0.8909, 'learning_rate': 2.6780626780626784e-05, 'epoch': 7.77}
{'loss': 0.8289, 'learning_rate': 2.64957264957265e-05, 'epoch': 7.84}
{'loss': 0.8146, 'learning_rate': 2.621082621082621e-05, 'epoch': 7.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 0.8544, 'learning_rate': 2.5925925925925925e-05, 'epoch': 8.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Ballast Type\checkpoint-1040
Configuration saved in models/ViT_Ballast Type\checkpoint-1040\config.json
Model weights saved in models/ViT_Ballast Type\checkpoint-1040\pytorch_model.bin
Image processor saved in models/ViT_Ballast Type\checkpoint-1040\preprocessor_config.json
{'eval_loss': 1.0934051275253296, 'eval_accuracy': 0.6304661220567035, 'eval_f1': 0.031111995522242954, 'eval_precision': 0.04856835699407761, 'eval_recall': 0.03065328500480986, 'eval_runtime': 16.5397, 'eval_samples_per_second': 125.818, 'eval_steps_per_second': 7.92, 'epoch': 8.0}
{'loss': 0.8034, 'learning_rate': 2.564102564102564e-05, 'epoch': 8.08}
{'loss': 0.8409, 'learning_rate': 2.535612535612536e-05, 'epoch': 8.15}
{'loss': 0.8075, 'learning_rate': 2.5071225071225073e-05, 'epoch': 8.23}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.7607, 'learning_rate': 2.4786324786324787e-05, 'epoch': 8.31}
{'loss': 0.7651, 'learning_rate': 2.4501424501424502e-05, 'epoch': 8.38}
{'loss': 0.7881, 'learning_rate': 2.4216524216524217e-05, 'epoch': 8.46}
{'loss': 0.7854, 'learning_rate': 2.3931623931623935e-05, 'epoch': 8.54}
{'loss': 0.7241, 'learning_rate': 2.364672364672365e-05, 'epoch': 8.61}
{'loss': 0.7838, 'learning_rate': 2.336182336182336e-05, 'epoch': 8.69}
{'loss': 0.8462, 'learning_rate': 2.307692307692308e-05, 'epoch': 8.77}
{'loss': 0.817, 'learning_rate': 2.2792022792022794e-05, 'epoch': 8.84}
{'loss': 0.7455, 'learning_rate': 2.250712250712251e-05, 'epoch': 8.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 0.7629, 'learning_rate': 2.2222222222222223e-05, 'epoch': 9.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Ballast Type\checkpoint-1170
Configuration saved in models/ViT_Ballast Type\checkpoint-1170\config.json
Model weights saved in models/ViT_Ballast Type\checkpoint-1170\pytorch_model.bin
Image processor saved in models/ViT_Ballast Type\checkpoint-1170\preprocessor_config.json
{'eval_loss': 1.0970001220703125, 'eval_accuracy': 0.6295050456511293, 'eval_f1': 0.03153675549956877, 'eval_precision': 0.047923505513627195, 'eval_recall': 0.030938287384655084, 'eval_runtime': 16.5555, 'eval_samples_per_second': 125.698, 'eval_steps_per_second': 7.913, 'epoch': 9.0}
{'loss': 0.7094, 'learning_rate': 2.1937321937321938e-05, 'epoch': 9.08}
{'loss': 0.6703, 'learning_rate': 2.1652421652421653e-05, 'epoch': 9.15}
{'loss': 0.7258, 'learning_rate': 2.1367521367521368e-05, 'epoch': 9.23}
{'loss': 0.8245, 'learning_rate': 2.1082621082621086e-05, 'epoch': 9.31}
{'loss': 0.7429, 'learning_rate': 2.07977207977208e-05, 'epoch': 9.38}
{'loss': 0.7252, 'learning_rate': 2.0512820512820512e-05, 'epoch': 9.46}
{'loss': 0.7694, 'learning_rate': 2.022792022792023e-05, 'epoch': 9.54}
{'loss': 0.7918, 'learning_rate': 1.9943019943019945e-05, 'epoch': 9.61}
{'loss': 0.6957, 'learning_rate': 1.965811965811966e-05, 'epoch': 9.69}
{'loss': 0.7873, 'learning_rate': 1.9373219373219374e-05, 'epoch': 9.77}
{'loss': 0.7602, 'learning_rate': 1.908831908831909e-05, 'epoch': 9.84}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.7548, 'learning_rate': 1.8803418803418804e-05, 'epoch': 9.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 0.8106, 'learning_rate': 1.8518518518518518e-05, 'epoch': 10.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Ballast Type\checkpoint-1300
Configuration saved in models/ViT_Ballast Type\checkpoint-1300\config.json
{'eval_loss': 1.093361496925354, 'eval_accuracy': 0.6333493512734263, 'eval_f1': 0.04445896062110183, 'eval_precision': 0.07020817726653096, 'eval_recall': 0.039530723357488695, 'eval_runtime': 16.3828, 'eval_samples_per_second': 127.023, 'eval_steps_per_second': 7.996, 'epoch': 10.0}
Model weights saved in models/ViT_Ballast Type\checkpoint-1300\pytorch_model.bin
Image processor saved in models/ViT_Ballast Type\checkpoint-1300\preprocessor_config.json
{'loss': 0.7983, 'learning_rate': 1.8233618233618236e-05, 'epoch': 10.08}
{'loss': 0.6873, 'learning_rate': 1.794871794871795e-05, 'epoch': 10.15}
{'loss': 0.6279, 'learning_rate': 1.7663817663817662e-05, 'epoch': 10.23}
{'loss': 0.6886, 'learning_rate': 1.737891737891738e-05, 'epoch': 10.31}
{'loss': 0.75, 'learning_rate': 1.7094017094017095e-05, 'epoch': 10.38}
{'loss': 0.6394, 'learning_rate': 1.680911680911681e-05, 'epoch': 10.46}
{'loss': 0.6985, 'learning_rate': 1.6524216524216525e-05, 'epoch': 10.54}
{'loss': 0.6645, 'learning_rate': 1.623931623931624e-05, 'epoch': 10.61}
{'loss': 0.7629, 'learning_rate': 1.5954415954415954e-05, 'epoch': 10.69}
{'loss': 0.6918, 'learning_rate': 1.566951566951567e-05, 'epoch': 10.77}
{'loss': 0.6625, 'learning_rate': 1.5384615384615387e-05, 'epoch': 10.84}
{'loss': 0.7063, 'learning_rate': 1.50997150997151e-05, 'epoch': 10.92}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 0.6976, 'learning_rate': 1.4814814814814815e-05, 'epoch': 11.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Ballast Type\checkpoint-1430
Configuration saved in models/ViT_Ballast Type\checkpoint-1430\config.json
Model weights saved in models/ViT_Ballast Type\checkpoint-1430\pytorch_model.bin
Image processor saved in models/ViT_Ballast Type\checkpoint-1430\preprocessor_config.json
{'eval_loss': 1.1442564725875854, 'eval_accuracy': 0.6290245074483422, 'eval_f1': 0.04561968902732716, 'eval_precision': 0.07076120000164994, 'eval_recall': 0.04072583231862578, 'eval_runtime': 16.5596, 'eval_samples_per_second': 125.668, 'eval_steps_per_second': 7.911, 'epoch': 11.0}
{'loss': 0.8309, 'learning_rate': 1.4529914529914531e-05, 'epoch': 11.08}
{'loss': 0.6986, 'learning_rate': 1.4245014245014246e-05, 'epoch': 11.15}
{'loss': 0.657, 'learning_rate': 1.3960113960113961e-05, 'epoch': 11.23}
{'loss': 0.6095, 'learning_rate': 1.3675213675213677e-05, 'epoch': 11.31}
{'loss': 0.5791, 'learning_rate': 1.3390313390313392e-05, 'epoch': 11.38}
{'loss': 0.6662, 'learning_rate': 1.3105413105413105e-05, 'epoch': 11.46}
{'loss': 0.7619, 'learning_rate': 1.282051282051282e-05, 'epoch': 11.54}
{'loss': 0.6232, 'learning_rate': 1.2535612535612536e-05, 'epoch': 11.61}
{'loss': 0.6472, 'learning_rate': 1.2250712250712251e-05, 'epoch': 11.69}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.6102, 'learning_rate': 1.1965811965811967e-05, 'epoch': 11.77}
{'loss': 0.6568, 'learning_rate': 1.168091168091168e-05, 'epoch': 11.84}
{'loss': 0.7364, 'learning_rate': 1.1396011396011397e-05, 'epoch': 11.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 0.6871, 'learning_rate': 1.1111111111111112e-05, 'epoch': 12.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Ballast Type\checkpoint-1560
Configuration saved in models/ViT_Ballast Type\checkpoint-1560\config.json
Model weights saved in models/ViT_Ballast Type\checkpoint-1560\pytorch_model.bin
Image processor saved in models/ViT_Ballast Type\checkpoint-1560\preprocessor_config.json
{'eval_loss': 1.1720362901687622, 'eval_accuracy': 0.6131667467563672, 'eval_f1': 0.043554137025880416, 'eval_precision': 0.061817215043021505, 'eval_recall': 0.03912179090842873, 'eval_runtime': 16.6014, 'eval_samples_per_second': 125.351, 'eval_steps_per_second': 7.891, 'epoch': 12.0}
{'loss': 0.6488, 'learning_rate': 1.0826210826210826e-05, 'epoch': 12.08}
{'loss': 0.6037, 'learning_rate': 1.0541310541310543e-05, 'epoch': 12.15}
{'loss': 0.6385, 'learning_rate': 1.0256410256410256e-05, 'epoch': 12.23}
{'loss': 0.6154, 'learning_rate': 9.971509971509972e-06, 'epoch': 12.31}
{'loss': 0.707, 'learning_rate': 9.686609686609687e-06, 'epoch': 12.38}
{'loss': 0.6379, 'learning_rate': 9.401709401709402e-06, 'epoch': 12.46}
{'loss': 0.685, 'learning_rate': 9.116809116809118e-06, 'epoch': 12.54}
{'loss': 0.6557, 'learning_rate': 8.831908831908831e-06, 'epoch': 12.61}
{'loss': 0.7028, 'learning_rate': 8.547008547008548e-06, 'epoch': 12.69}
{'loss': 0.6709, 'learning_rate': 8.262108262108262e-06, 'epoch': 12.77}
{'loss': 0.6163, 'learning_rate': 7.977207977207977e-06, 'epoch': 12.84}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.6075, 'learning_rate': 7.692307692307694e-06, 'epoch': 12.92}
{'loss': 0.5957, 'learning_rate': 7.4074074074074075e-06, 'epoch': 13.0}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Ballast Type\checkpoint-1690
Configuration saved in models/ViT_Ballast Type\checkpoint-1690\config.json
Model weights saved in models/ViT_Ballast Type\checkpoint-1690\pytorch_model.bin
Image processor saved in models/ViT_Ballast Type\checkpoint-1690\preprocessor_config.json
{'eval_loss': 1.1857186555862427, 'eval_accuracy': 0.5992311388755406, 'eval_f1': 0.043839084873603267, 'eval_precision': 0.06405776233314778, 'eval_recall': 0.03933337247498869, 'eval_runtime': 16.5575, 'eval_samples_per_second': 125.683, 'eval_steps_per_second': 7.912, 'epoch': 13.0}
{'loss': 0.5967, 'learning_rate': 7.122507122507123e-06, 'epoch': 13.08}
{'loss': 0.6986, 'learning_rate': 6.837606837606839e-06, 'epoch': 13.15}
{'loss': 0.6177, 'learning_rate': 6.5527065527065525e-06, 'epoch': 13.23}
{'loss': 0.6863, 'learning_rate': 6.267806267806268e-06, 'epoch': 13.31}
{'loss': 0.6566, 'learning_rate': 5.982905982905984e-06, 'epoch': 13.38}
{'loss': 0.5966, 'learning_rate': 5.6980056980056985e-06, 'epoch': 13.46}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.5886, 'learning_rate': 5.413105413105413e-06, 'epoch': 13.54}
{'loss': 0.582, 'learning_rate': 5.128205128205128e-06, 'epoch': 13.61}
{'loss': 0.6247, 'learning_rate': 4.8433048433048435e-06, 'epoch': 13.69}
{'loss': 0.5862, 'learning_rate': 4.558404558404559e-06, 'epoch': 13.77}
{'loss': 0.5996, 'learning_rate': 4.273504273504274e-06, 'epoch': 13.84}
{'loss': 0.5939, 'learning_rate': 3.988603988603989e-06, 'epoch': 13.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 0.5841, 'learning_rate': 3.7037037037037037e-06, 'epoch': 14.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Ballast Type\checkpoint-1820
Configuration saved in models/ViT_Ballast Type\checkpoint-1820\config.json
Model weights saved in models/ViT_Ballast Type\checkpoint-1820\pytorch_model.bin
Image processor saved in models/ViT_Ballast Type\checkpoint-1820\preprocessor_config.json
{'eval_loss': 1.1992911100387573, 'eval_accuracy': 0.6367131186929361, 'eval_f1': 0.035194939133085405, 'eval_precision': 0.045009121075805916, 'eval_recall': 0.03360126903000671, 'eval_runtime': 16.4461, 'eval_samples_per_second': 126.535, 'eval_steps_per_second': 7.965, 'epoch': 14.0}
{'loss': 0.6042, 'learning_rate': 3.4188034188034193e-06, 'epoch': 14.08}
{'loss': 0.5977, 'learning_rate': 3.133903133903134e-06, 'epoch': 14.15}
{'loss': 0.6307, 'learning_rate': 2.8490028490028492e-06, 'epoch': 14.23}
{'loss': 0.6, 'learning_rate': 2.564102564102564e-06, 'epoch': 14.31}
{'loss': 0.5787, 'learning_rate': 2.2792022792022796e-06, 'epoch': 14.38}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.6067, 'learning_rate': 1.9943019943019943e-06, 'epoch': 14.46}
{'loss': 0.5968, 'learning_rate': 1.7094017094017097e-06, 'epoch': 14.54}
{'loss': 0.5956, 'learning_rate': 1.4245014245014246e-06, 'epoch': 14.61}
{'loss': 0.599, 'learning_rate': 1.1396011396011398e-06, 'epoch': 14.69}
{'loss': 0.6236, 'learning_rate': 8.547008547008548e-07, 'epoch': 14.77}
{'loss': 0.5586, 'learning_rate': 5.698005698005699e-07, 'epoch': 14.84}
{'loss': 0.6105, 'learning_rate': 2.8490028490028494e-07, 'epoch': 14.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 0.5881, 'learning_rate': 0.0, 'epoch': 15.0}
{'eval_loss': 1.1776914596557617, 'eval_accuracy': 0.6256607400288323, 'eval_f1': 0.052768981508568125, 'eval_precision': 0.06709322744864389, 'eval_recall': 0.048300375845500766, 'eval_runtime': 16.5044, 'eval_samples_per_second': 126.088, 'eval_steps_per_second': 7.937, 'epoch': 15.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Ballast Type\checkpoint-1950
Configuration saved in models/ViT_Ballast Type\checkpoint-1950\config.json
Model weights saved in models/ViT_Ballast Type\checkpoint-1950\pytorch_model.bin
Image processor saved in models/ViT_Ballast Type\checkpoint-1950\preprocessor_config.json
Training completed. Do not forget to share your model on huggingface.co/models =)
Loading best model from models/ViT_Ballast Type\checkpoint-1950 (score: 0.052768981508568125).
Setting `WANDB_LOG_MODEL` from true to `end` instead
Saving model checkpoint to C:\Users\chris\AppData\Local\Temp\tmpj0qyc2az
Configuration saved in C:\Users\chris\AppData\Local\Temp\tmpj0qyc2az\config.json
Model weights saved in C:\Users\chris\AppData\Local\Temp\tmpj0qyc2az\pytorch_model.bin
Image processor saved in C:\Users\chris\AppData\Local\Temp\tmpj0qyc2az\preprocessor_config.json
Logging model artifacts. ...
{'train_runtime': 2239.7427, 'train_samples_per_second': 55.741, 'train_steps_per_second': 0.871, 'train_loss': 0.9195040499858367, 'epoch': 15.0}