[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37]
[0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 36]
[36]
['Fractional  Sloop', 'Gunter', 'Cat (Marconi)', 'Masthead Sloop', 'Fractional (9/10) Sloop', 'Cat (rotating spar)', 'Wing (multi element)', 'Masthead  Yawl', 'NaN', 'Cutter', 'Staysail Ketch', 'Frac. Sloop (Rotating Spar)', 'Masthead  Ketch', 'Cat (unstayed)', 'Gaff head Cat', 'Lateen', 'Solent', 'Gaff Head Cutter', 'Gaffhead Sloop', 'Fractionally Rigged Ketch', 'Cat Ketch (unstayed)', 'Fractional (7/8) Sloop', 'Gaff-Yawl', 'Frac. Sloop (Free standing)', '2 Mst. Schooner', 'Cat Ketch', 'Gaff Topsail Cutter', 'Junk Rig', 'Gaff head  Ketch', 'Sprit/Lug', 'Gunter-Yawl', 'Standing Lug', 'B&R', 'B&R Fractional', 'Cutter/Ketch', 'Sloop or Yawl', 'Brigantine', 'Fractional Yawl']
loading configuration file config.json from cache at C:\Users\chris/.cache\huggingface\hub\models--microsoft--resnet-18\snapshots\2f536bd335677c6b111b3d103af458ef57a6145e\config.json
Model config ResNetConfig {
  "_name_or_path": "microsoft/resnet-18",
  "architectures": [
    "ResNetForImageClassification"
  ],
  "depths": [
    2,
    2,
    2,
    2
  ],
  "downsample_in_first_stage": false,
  "embedding_size": 64,
  "hidden_act": "relu",
  "hidden_sizes": [
    64,
    128,
    256,
    512
  ],
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31",
    "32": "LABEL_32",
    "33": "LABEL_33",
    "34": "LABEL_34",
    "35": "LABEL_35",
    "36": "LABEL_36",
    "37": "LABEL_37"
  },
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_32": 32,
    "LABEL_33": 33,
    "LABEL_34": 34,
    "LABEL_35": 35,
    "LABEL_36": 36,
    "LABEL_37": 37,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_type": "basic",
  "model_type": "resnet",
  "num_channels": 3,
  "out_features": null,
  "stage_names": [
    "stem",
    "stage1",
    "stage2",
    "stage3",
    "stage4"
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.26.1"
}
loading weights file pytorch_model.bin from cache at C:\Users\chris/.cache\huggingface\hub\models--microsoft--resnet-18\snapshots\2f536bd335677c6b111b3d103af458ef57a6145e\pytorch_model.bin
All model checkpoint weights were used when initializing ResNetForImageClassification.
Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-18 and are newly initialized because the shapes did not match:
- classifier.1.weight: found shape torch.Size([1000, 512]) in the checkpoint and torch.Size([38, 512]) in the model instantiated
- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([38]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
Setting `WANDB_LOG_MODEL` from true to `end` instead
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 8323
  Num Epochs = 15
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 4
  Total optimization steps = 1950
  Number of trainable parameters = 11196006
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
{'loss': 3.5388, 'learning_rate': 2.564102564102564e-06, 'epoch': 0.08}
{'loss': 3.5169, 'learning_rate': 5.128205128205128e-06, 'epoch': 0.15}
{'loss': 3.491, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.23}
{'loss': 3.4195, 'learning_rate': 1.0256410256410256e-05, 'epoch': 0.31}
{'loss': 3.3114, 'learning_rate': 1.282051282051282e-05, 'epoch': 0.38}
{'loss': 3.2111, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.46}
{'loss': 3.0618, 'learning_rate': 1.794871794871795e-05, 'epoch': 0.54}
{'loss': 2.981, 'learning_rate': 2.0512820512820512e-05, 'epoch': 0.61}
{'loss': 2.7351, 'learning_rate': 2.307692307692308e-05, 'epoch': 0.69}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 2.6243, 'learning_rate': 2.564102564102564e-05, 'epoch': 0.77}
{'loss': 2.4869, 'learning_rate': 2.8205128205128207e-05, 'epoch': 0.84}
{'loss': 2.2797, 'learning_rate': 3.0769230769230774e-05, 'epoch': 0.92}
{'loss': 2.0596, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'eval_loss': 2.031872272491455, 'eval_accuracy': 0.5329168668909178, 'eval_f1': 0.5329168668909178, 'eval_precision': 0.5329168668909178, 'eval_recall': 0.5329168668909178, 'eval_runtime': 11.653, 'eval_samples_per_second': 178.581, 'eval_steps_per_second': 11.242, 'epoch': 1.0}
Saving model checkpoint to models/model_Rigging Type\checkpoint-130
Configuration saved in models/model_Rigging Type\checkpoint-130\config.json
Model weights saved in models/model_Rigging Type\checkpoint-130\pytorch_model.bin
Image processor saved in models/model_Rigging Type\checkpoint-130\preprocessor_config.json
{'loss': 1.9895, 'learning_rate': 3.58974358974359e-05, 'epoch': 1.08}
{'loss': 1.8865, 'learning_rate': 3.846153846153846e-05, 'epoch': 1.15}
{'loss': 1.7809, 'learning_rate': 4.1025641025641023e-05, 'epoch': 1.23}
{'loss': 1.6206, 'learning_rate': 4.358974358974359e-05, 'epoch': 1.31}
{'loss': 1.5912, 'learning_rate': 4.615384615384616e-05, 'epoch': 1.38}
{'loss': 1.6201, 'learning_rate': 4.871794871794872e-05, 'epoch': 1.46}
{'loss': 1.4464, 'learning_rate': 4.985754985754986e-05, 'epoch': 1.54}
{'loss': 1.3428, 'learning_rate': 4.9572649572649575e-05, 'epoch': 1.61}
{'loss': 1.4222, 'learning_rate': 4.928774928774929e-05, 'epoch': 1.69}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 1.3399, 'learning_rate': 4.9002849002849004e-05, 'epoch': 1.77}
{'loss': 1.3251, 'learning_rate': 4.871794871794872e-05, 'epoch': 1.84}
{'loss': 1.4298, 'learning_rate': 4.8433048433048433e-05, 'epoch': 1.92}
{'loss': 1.3546, 'learning_rate': 4.814814814814815e-05, 'epoch': 2.0}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'eval_loss': 1.3718066215515137, 'eval_accuracy': 0.5761653051417588, 'eval_f1': 0.5761653051417588, 'eval_precision': 0.5761653051417588, 'eval_recall': 0.5761653051417588, 'eval_runtime': 11.538, 'eval_samples_per_second': 180.361, 'eval_steps_per_second': 11.354, 'epoch': 2.0}
Saving model checkpoint to models/model_Rigging Type\checkpoint-260
Configuration saved in models/model_Rigging Type\checkpoint-260\config.json
Model weights saved in models/model_Rigging Type\checkpoint-260\pytorch_model.bin
Image processor saved in models/model_Rigging Type\checkpoint-260\preprocessor_config.json
{'loss': 1.4097, 'learning_rate': 4.786324786324787e-05, 'epoch': 2.08}
{'loss': 1.2238, 'learning_rate': 4.7578347578347584e-05, 'epoch': 2.15}
{'loss': 1.2074, 'learning_rate': 4.72934472934473e-05, 'epoch': 2.23}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 1.2937, 'learning_rate': 4.700854700854701e-05, 'epoch': 2.31}
{'loss': 1.2673, 'learning_rate': 4.672364672364672e-05, 'epoch': 2.38}
{'loss': 1.2841, 'learning_rate': 4.643874643874644e-05, 'epoch': 2.46}
{'loss': 1.1443, 'learning_rate': 4.615384615384616e-05, 'epoch': 2.54}
{'loss': 1.2709, 'learning_rate': 4.586894586894587e-05, 'epoch': 2.61}
{'loss': 1.16, 'learning_rate': 4.558404558404559e-05, 'epoch': 2.69}
{'loss': 1.2516, 'learning_rate': 4.52991452991453e-05, 'epoch': 2.77}
{'loss': 1.3002, 'learning_rate': 4.501424501424502e-05, 'epoch': 2.84}
{'loss': 1.2587, 'learning_rate': 4.472934472934473e-05, 'epoch': 2.92}
{'loss': 1.1843, 'learning_rate': 4.4444444444444447e-05, 'epoch': 3.0}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'eval_loss': 1.2502614259719849, 'eval_accuracy': 0.5982700624699664, 'eval_f1': 0.5982700624699664, 'eval_precision': 0.5982700624699664, 'eval_recall': 0.5982700624699664, 'eval_runtime': 11.617, 'eval_samples_per_second': 179.134, 'eval_steps_per_second': 11.277, 'epoch': 3.0}
Saving model checkpoint to models/model_Rigging Type\checkpoint-390
Configuration saved in models/model_Rigging Type\checkpoint-390\config.json
Model weights saved in models/model_Rigging Type\checkpoint-390\pytorch_model.bin
Image processor saved in models/model_Rigging Type\checkpoint-390\preprocessor_config.json
{'loss': 1.0528, 'learning_rate': 4.415954415954416e-05, 'epoch': 3.08}
{'loss': 1.1051, 'learning_rate': 4.3874643874643876e-05, 'epoch': 3.15}
{'loss': 1.134, 'learning_rate': 4.358974358974359e-05, 'epoch': 3.23}
{'loss': 1.0991, 'learning_rate': 4.3304843304843306e-05, 'epoch': 3.31}
{'loss': 1.1489, 'learning_rate': 4.301994301994302e-05, 'epoch': 3.38}
{'loss': 1.1369, 'learning_rate': 4.2735042735042735e-05, 'epoch': 3.46}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 1.1581, 'learning_rate': 4.2450142450142457e-05, 'epoch': 3.54}
{'loss': 1.1592, 'learning_rate': 4.216524216524217e-05, 'epoch': 3.61}
{'loss': 1.1163, 'learning_rate': 4.1880341880341886e-05, 'epoch': 3.69}
{'loss': 1.1604, 'learning_rate': 4.15954415954416e-05, 'epoch': 3.77}
{'loss': 1.2048, 'learning_rate': 4.131054131054131e-05, 'epoch': 3.84}
{'loss': 1.1097, 'learning_rate': 4.1025641025641023e-05, 'epoch': 3.92}
{'loss': 1.1867, 'learning_rate': 4.074074074074074e-05, 'epoch': 4.0}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'eval_loss': 1.2153077125549316, 'eval_accuracy': 0.6179721287842384, 'eval_f1': 0.6179721287842384, 'eval_precision': 0.6179721287842384, 'eval_recall': 0.6179721287842384, 'eval_runtime': 11.609, 'eval_samples_per_second': 179.257, 'eval_steps_per_second': 11.284, 'epoch': 4.0}
Saving model checkpoint to models/model_Rigging Type\checkpoint-520
Configuration saved in models/model_Rigging Type\checkpoint-520\config.json
Model weights saved in models/model_Rigging Type\checkpoint-520\pytorch_model.bin
Image processor saved in models/model_Rigging Type\checkpoint-520\preprocessor_config.json
{'loss': 1.1562, 'learning_rate': 4.045584045584046e-05, 'epoch': 4.08}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 1.0351, 'learning_rate': 4.0170940170940174e-05, 'epoch': 4.15}
{'loss': 1.1246, 'learning_rate': 3.988603988603989e-05, 'epoch': 4.23}
{'loss': 1.1649, 'learning_rate': 3.9601139601139604e-05, 'epoch': 4.31}
{'loss': 1.1522, 'learning_rate': 3.931623931623932e-05, 'epoch': 4.38}
{'loss': 1.0143, 'learning_rate': 3.903133903133903e-05, 'epoch': 4.46}
{'loss': 1.0682, 'learning_rate': 3.874643874643875e-05, 'epoch': 4.54}
{'loss': 1.1045, 'learning_rate': 3.846153846153846e-05, 'epoch': 4.61}
{'loss': 1.0137, 'learning_rate': 3.817663817663818e-05, 'epoch': 4.69}
{'loss': 0.9934, 'learning_rate': 3.789173789173789e-05, 'epoch': 4.77}
{'loss': 1.0644, 'learning_rate': 3.760683760683761e-05, 'epoch': 4.84}
{'loss': 1.0335, 'learning_rate': 3.732193732193732e-05, 'epoch': 4.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 0.9483, 'learning_rate': 3.7037037037037037e-05, 'epoch': 5.0}
{'eval_loss': 1.1687977313995361, 'eval_accuracy': 0.6395963479096588, 'eval_f1': 0.6395963479096588, 'eval_precision': 0.6395963479096588, 'eval_recall': 0.6395963479096588, 'eval_runtime': 11.505, 'eval_samples_per_second': 180.878, 'eval_steps_per_second': 11.386, 'epoch': 5.0}
Saving model checkpoint to models/model_Rigging Type\checkpoint-650
Configuration saved in models/model_Rigging Type\checkpoint-650\config.json
Model weights saved in models/model_Rigging Type\checkpoint-650\pytorch_model.bin
Image processor saved in models/model_Rigging Type\checkpoint-650\preprocessor_config.json
{'loss': 1.1005, 'learning_rate': 3.675213675213676e-05, 'epoch': 5.08}
{'loss': 0.9271, 'learning_rate': 3.646723646723647e-05, 'epoch': 5.15}
{'loss': 0.9598, 'learning_rate': 3.618233618233619e-05, 'epoch': 5.23}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 1.0042, 'learning_rate': 3.58974358974359e-05, 'epoch': 5.31}
{'loss': 1.0088, 'learning_rate': 3.561253561253561e-05, 'epoch': 5.38}
{'loss': 1.0342, 'learning_rate': 3.5327635327635325e-05, 'epoch': 5.46}
{'loss': 1.0592, 'learning_rate': 3.504273504273504e-05, 'epoch': 5.54}
{'loss': 1.0474, 'learning_rate': 3.475783475783476e-05, 'epoch': 5.61}
{'loss': 1.0287, 'learning_rate': 3.4472934472934476e-05, 'epoch': 5.69}
{'loss': 1.0543, 'learning_rate': 3.418803418803419e-05, 'epoch': 5.77}
{'loss': 0.9915, 'learning_rate': 3.3903133903133905e-05, 'epoch': 5.84}
{'loss': 1.0043, 'learning_rate': 3.361823361823362e-05, 'epoch': 5.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 0.9814, 'learning_rate': 3.3333333333333335e-05, 'epoch': 6.0}
{'eval_loss': 1.1272509098052979, 'eval_accuracy': 0.6477654973570399, 'eval_f1': 0.6477654973570399, 'eval_precision': 0.6477654973570399, 'eval_recall': 0.6477654973570399, 'eval_runtime': 11.486, 'eval_samples_per_second': 181.177, 'eval_steps_per_second': 11.405, 'epoch': 6.0}
Saving model checkpoint to models/model_Rigging Type\checkpoint-780
Configuration saved in models/model_Rigging Type\checkpoint-780\config.json
Model weights saved in models/model_Rigging Type\checkpoint-780\pytorch_model.bin
Image processor saved in models/model_Rigging Type\checkpoint-780\preprocessor_config.json
{'loss': 1.0631, 'learning_rate': 3.304843304843305e-05, 'epoch': 6.08}
{'loss': 0.993, 'learning_rate': 3.2763532763532764e-05, 'epoch': 6.15}
{'loss': 0.963, 'learning_rate': 3.247863247863248e-05, 'epoch': 6.23}
{'loss': 0.9618, 'learning_rate': 3.2193732193732194e-05, 'epoch': 6.31}
{'loss': 0.9237, 'learning_rate': 3.190883190883191e-05, 'epoch': 6.38}
{'loss': 0.916, 'learning_rate': 3.162393162393162e-05, 'epoch': 6.46}
{'loss': 1.0052, 'learning_rate': 3.133903133903134e-05, 'epoch': 6.54}
{'loss': 0.9711, 'learning_rate': 3.105413105413106e-05, 'epoch': 6.61}
{'loss': 1.0217, 'learning_rate': 3.0769230769230774e-05, 'epoch': 6.69}
{'loss': 0.9479, 'learning_rate': 3.0484330484330486e-05, 'epoch': 6.77}
{'loss': 0.9987, 'learning_rate': 3.01994301994302e-05, 'epoch': 6.84}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.9333, 'learning_rate': 2.9914529914529915e-05, 'epoch': 6.92}
{'loss': 1.0626, 'learning_rate': 2.962962962962963e-05, 'epoch': 7.0}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'eval_loss': 1.1438510417938232, 'eval_accuracy': 0.6391158097068717, 'eval_f1': 0.6391158097068717, 'eval_precision': 0.6391158097068717, 'eval_recall': 0.6391158097068717, 'eval_runtime': 11.684, 'eval_samples_per_second': 178.107, 'eval_steps_per_second': 11.212, 'epoch': 7.0}
Saving model checkpoint to models/model_Rigging Type\checkpoint-910
Configuration saved in models/model_Rigging Type\checkpoint-910\config.json
Model weights saved in models/model_Rigging Type\checkpoint-910\pytorch_model.bin
Image processor saved in models/model_Rigging Type\checkpoint-910\preprocessor_config.json
{'loss': 0.8684, 'learning_rate': 2.9344729344729345e-05, 'epoch': 7.08}
{'loss': 0.9274, 'learning_rate': 2.9059829059829063e-05, 'epoch': 7.15}
{'loss': 0.932, 'learning_rate': 2.8774928774928778e-05, 'epoch': 7.23}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.9776, 'learning_rate': 2.8490028490028492e-05, 'epoch': 7.31}
{'loss': 0.9197, 'learning_rate': 2.8205128205128207e-05, 'epoch': 7.38}
{'loss': 0.9861, 'learning_rate': 2.7920227920227922e-05, 'epoch': 7.46}
{'loss': 0.8987, 'learning_rate': 2.7635327635327633e-05, 'epoch': 7.54}
{'loss': 0.8757, 'learning_rate': 2.7350427350427355e-05, 'epoch': 7.61}
{'loss': 0.9582, 'learning_rate': 2.706552706552707e-05, 'epoch': 7.69}
{'loss': 0.9503, 'learning_rate': 2.6780626780626784e-05, 'epoch': 7.77}
{'loss': 0.9416, 'learning_rate': 2.64957264957265e-05, 'epoch': 7.84}
{'loss': 0.9669, 'learning_rate': 2.621082621082621e-05, 'epoch': 7.92}
{'loss': 0.9416, 'learning_rate': 2.5925925925925925e-05, 'epoch': 8.0}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'eval_loss': 1.1303478479385376, 'eval_accuracy': 0.6549735703988467, 'eval_f1': 0.6549735703988467, 'eval_precision': 0.6549735703988467, 'eval_recall': 0.6549735703988467, 'eval_runtime': 11.674, 'eval_samples_per_second': 178.259, 'eval_steps_per_second': 11.222, 'epoch': 8.0}
Saving model checkpoint to models/model_Rigging Type\checkpoint-1040
Configuration saved in models/model_Rigging Type\checkpoint-1040\config.json
Model weights saved in models/model_Rigging Type\checkpoint-1040\pytorch_model.bin
Image processor saved in models/model_Rigging Type\checkpoint-1040\preprocessor_config.json
{'loss': 1.0187, 'learning_rate': 2.564102564102564e-05, 'epoch': 8.08}
{'loss': 0.8862, 'learning_rate': 2.535612535612536e-05, 'epoch': 8.15}
{'loss': 0.9231, 'learning_rate': 2.5071225071225073e-05, 'epoch': 8.23}
{'loss': 0.8753, 'learning_rate': 2.4786324786324787e-05, 'epoch': 8.31}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.896, 'learning_rate': 2.4501424501424502e-05, 'epoch': 8.38}
{'loss': 0.8516, 'learning_rate': 2.4216524216524217e-05, 'epoch': 8.46}
{'loss': 0.8781, 'learning_rate': 2.3931623931623935e-05, 'epoch': 8.54}
{'loss': 0.8634, 'learning_rate': 2.364672364672365e-05, 'epoch': 8.61}
{'loss': 0.9749, 'learning_rate': 2.336182336182336e-05, 'epoch': 8.69}
{'loss': 0.9579, 'learning_rate': 2.307692307692308e-05, 'epoch': 8.77}
{'loss': 0.8884, 'learning_rate': 2.2792022792022794e-05, 'epoch': 8.84}
{'loss': 0.9181, 'learning_rate': 2.250712250712251e-05, 'epoch': 8.92}
{'loss': 0.7929, 'learning_rate': 2.2222222222222223e-05, 'epoch': 9.0}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'eval_loss': 1.098500370979309, 'eval_accuracy': 0.652090341182124, 'eval_f1': 0.652090341182124, 'eval_precision': 0.652090341182124, 'eval_recall': 0.652090341182124, 'eval_runtime': 11.54, 'eval_samples_per_second': 180.329, 'eval_steps_per_second': 11.352, 'epoch': 9.0}
Saving model checkpoint to models/model_Rigging Type\checkpoint-1170
Configuration saved in models/model_Rigging Type\checkpoint-1170\config.json
Model weights saved in models/model_Rigging Type\checkpoint-1170\pytorch_model.bin
Image processor saved in models/model_Rigging Type\checkpoint-1170\preprocessor_config.json
{'loss': 0.9429, 'learning_rate': 2.1937321937321938e-05, 'epoch': 9.08}
{'loss': 0.8451, 'learning_rate': 2.1652421652421653e-05, 'epoch': 9.15}
{'loss': 0.8384, 'learning_rate': 2.1367521367521368e-05, 'epoch': 9.23}
{'loss': 0.8967, 'learning_rate': 2.1082621082621086e-05, 'epoch': 9.31}
{'loss': 0.8313, 'learning_rate': 2.07977207977208e-05, 'epoch': 9.38}
{'loss': 0.8033, 'learning_rate': 2.0512820512820512e-05, 'epoch': 9.46}
{'loss': 0.8838, 'learning_rate': 2.022792022792023e-05, 'epoch': 9.54}
{'loss': 0.7952, 'learning_rate': 1.9943019943019945e-05, 'epoch': 9.61}
{'loss': 0.8552, 'learning_rate': 1.965811965811966e-05, 'epoch': 9.69}
{'loss': 0.9539, 'learning_rate': 1.9373219373219374e-05, 'epoch': 9.77}
{'loss': 0.799, 'learning_rate': 1.908831908831909e-05, 'epoch': 9.84}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 1.0241, 'learning_rate': 1.8803418803418804e-05, 'epoch': 9.92}
{'loss': 0.9931, 'learning_rate': 1.8518518518518518e-05, 'epoch': 10.0}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
Saving model checkpoint to models/model_Rigging Type\checkpoint-1300
Configuration saved in models/model_Rigging Type\checkpoint-1300\config.json
Model weights saved in models/model_Rigging Type\checkpoint-1300\pytorch_model.bin
Image processor saved in models/model_Rigging Type\checkpoint-1300\preprocessor_config.json
{'eval_loss': 1.1052685976028442, 'eval_accuracy': 0.6559346468044209, 'eval_f1': 0.6559346468044209, 'eval_precision': 0.6559346468044209, 'eval_recall': 0.6559346468044209, 'eval_runtime': 11.597, 'eval_samples_per_second': 179.443, 'eval_steps_per_second': 11.296, 'epoch': 10.0}
{'loss': 0.8847, 'learning_rate': 1.8233618233618236e-05, 'epoch': 10.08}
{'loss': 0.7911, 'learning_rate': 1.794871794871795e-05, 'epoch': 10.15}
{'loss': 0.7941, 'learning_rate': 1.7663817663817662e-05, 'epoch': 10.23}
{'loss': 0.8614, 'learning_rate': 1.737891737891738e-05, 'epoch': 10.31}
{'loss': 0.8793, 'learning_rate': 1.7094017094017095e-05, 'epoch': 10.38}
{'loss': 0.8633, 'learning_rate': 1.680911680911681e-05, 'epoch': 10.46}
{'loss': 0.8018, 'learning_rate': 1.6524216524216525e-05, 'epoch': 10.54}
{'loss': 0.8704, 'learning_rate': 1.623931623931624e-05, 'epoch': 10.61}
{'loss': 0.9136, 'learning_rate': 1.5954415954415954e-05, 'epoch': 10.69}
{'loss': 0.8274, 'learning_rate': 1.566951566951567e-05, 'epoch': 10.77}
{'loss': 0.8432, 'learning_rate': 1.5384615384615387e-05, 'epoch': 10.84}
{'loss': 0.866, 'learning_rate': 1.50997150997151e-05, 'epoch': 10.92}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.81, 'learning_rate': 1.4814814814814815e-05, 'epoch': 11.0}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'eval_loss': 1.0903651714324951, 'eval_accuracy': 0.6544930321960596, 'eval_f1': 0.6544930321960596, 'eval_precision': 0.6544930321960596, 'eval_recall': 0.6544930321960596, 'eval_runtime': 11.582, 'eval_samples_per_second': 179.675, 'eval_steps_per_second': 11.311, 'epoch': 11.0}
Saving model checkpoint to models/model_Rigging Type\checkpoint-1430
Configuration saved in models/model_Rigging Type\checkpoint-1430\config.json
Model weights saved in models/model_Rigging Type\checkpoint-1430\pytorch_model.bin
Image processor saved in models/model_Rigging Type\checkpoint-1430\preprocessor_config.json
{'loss': 0.8363, 'learning_rate': 1.4529914529914531e-05, 'epoch': 11.08}
{'loss': 0.8437, 'learning_rate': 1.4245014245014246e-05, 'epoch': 11.15}
{'loss': 0.8183, 'learning_rate': 1.3960113960113961e-05, 'epoch': 11.23}
{'loss': 0.8969, 'learning_rate': 1.3675213675213677e-05, 'epoch': 11.31}
{'loss': 0.8097, 'learning_rate': 1.3390313390313392e-05, 'epoch': 11.38}
{'loss': 0.8541, 'learning_rate': 1.3105413105413105e-05, 'epoch': 11.46}
{'loss': 0.8794, 'learning_rate': 1.282051282051282e-05, 'epoch': 11.54}
{'loss': 0.8419, 'learning_rate': 1.2535612535612536e-05, 'epoch': 11.61}
{'loss': 0.8278, 'learning_rate': 1.2250712250712251e-05, 'epoch': 11.69}
{'loss': 0.8725, 'learning_rate': 1.1965811965811967e-05, 'epoch': 11.77}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.8165, 'learning_rate': 1.168091168091168e-05, 'epoch': 11.84}
{'loss': 0.734, 'learning_rate': 1.1396011396011397e-05, 'epoch': 11.92}
{'loss': 0.8234, 'learning_rate': 1.1111111111111112e-05, 'epoch': 12.0}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'eval_loss': 1.1359834671020508, 'eval_accuracy': 0.640076886112446, 'eval_f1': 0.640076886112446, 'eval_precision': 0.640076886112446, 'eval_recall': 0.640076886112446, 'eval_runtime': 11.823, 'eval_samples_per_second': 176.013, 'eval_steps_per_second': 11.08, 'epoch': 12.0}
Saving model checkpoint to models/model_Rigging Type\checkpoint-1560
Configuration saved in models/model_Rigging Type\checkpoint-1560\config.json
Model weights saved in models/model_Rigging Type\checkpoint-1560\pytorch_model.bin
Image processor saved in models/model_Rigging Type\checkpoint-1560\preprocessor_config.json
{'loss': 0.7623, 'learning_rate': 1.0826210826210826e-05, 'epoch': 12.08}
{'loss': 0.8357, 'learning_rate': 1.0541310541310543e-05, 'epoch': 12.15}
{'loss': 0.8398, 'learning_rate': 1.0256410256410256e-05, 'epoch': 12.23}
{'loss': 0.8394, 'learning_rate': 9.971509971509972e-06, 'epoch': 12.31}
{'loss': 0.7434, 'learning_rate': 9.686609686609687e-06, 'epoch': 12.38}
{'loss': 0.8269, 'learning_rate': 9.401709401709402e-06, 'epoch': 12.46}
{'loss': 0.8085, 'learning_rate': 9.116809116809118e-06, 'epoch': 12.54}
{'loss': 0.7604, 'learning_rate': 8.831908831908831e-06, 'epoch': 12.61}
{'loss': 0.8504, 'learning_rate': 8.547008547008548e-06, 'epoch': 12.69}
{'loss': 0.8269, 'learning_rate': 8.262108262108262e-06, 'epoch': 12.77}
{'loss': 0.8749, 'learning_rate': 7.977207977207977e-06, 'epoch': 12.84}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.8014, 'learning_rate': 7.692307692307694e-06, 'epoch': 12.92}
{'loss': 0.871, 'learning_rate': 7.4074074074074075e-06, 'epoch': 13.0}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
Saving model checkpoint to models/model_Rigging Type\checkpoint-1690
Configuration saved in models/model_Rigging Type\checkpoint-1690\config.json
Model weights saved in models/model_Rigging Type\checkpoint-1690\pytorch_model.bin
Image processor saved in models/model_Rigging Type\checkpoint-1690\preprocessor_config.json
{'eval_loss': 1.10276198387146, 'eval_accuracy': 0.6554541086016338, 'eval_f1': 0.6554541086016338, 'eval_precision': 0.6554541086016338, 'eval_recall': 0.6554541086016338, 'eval_runtime': 11.597, 'eval_samples_per_second': 179.443, 'eval_steps_per_second': 11.296, 'epoch': 13.0}
{'loss': 0.807, 'learning_rate': 7.122507122507123e-06, 'epoch': 13.08}
{'loss': 0.7538, 'learning_rate': 6.837606837606839e-06, 'epoch': 13.15}
{'loss': 0.8622, 'learning_rate': 6.5527065527065525e-06, 'epoch': 13.23}
{'loss': 0.7963, 'learning_rate': 6.267806267806268e-06, 'epoch': 13.31}
{'loss': 0.8108, 'learning_rate': 5.982905982905984e-06, 'epoch': 13.38}
{'loss': 0.7614, 'learning_rate': 5.6980056980056985e-06, 'epoch': 13.46}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.8831, 'learning_rate': 5.413105413105413e-06, 'epoch': 13.54}
{'loss': 0.7899, 'learning_rate': 5.128205128205128e-06, 'epoch': 13.61}
{'loss': 0.769, 'learning_rate': 4.8433048433048435e-06, 'epoch': 13.69}
{'loss': 0.8741, 'learning_rate': 4.558404558404559e-06, 'epoch': 13.77}
{'loss': 0.7446, 'learning_rate': 4.273504273504274e-06, 'epoch': 13.84}
{'loss': 0.7971, 'learning_rate': 3.988603988603989e-06, 'epoch': 13.92}
{'loss': 0.8376, 'learning_rate': 3.7037037037037037e-06, 'epoch': 14.0}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'eval_loss': 1.1142157316207886, 'eval_accuracy': 0.6487265737626141, 'eval_f1': 0.6487265737626141, 'eval_precision': 0.6487265737626141, 'eval_recall': 0.6487265737626141, 'eval_runtime': 11.627, 'eval_samples_per_second': 178.98, 'eval_steps_per_second': 11.267, 'epoch': 14.0}
Saving model checkpoint to models/model_Rigging Type\checkpoint-1820
Configuration saved in models/model_Rigging Type\checkpoint-1820\config.json
Model weights saved in models/model_Rigging Type\checkpoint-1820\pytorch_model.bin
Image processor saved in models/model_Rigging Type\checkpoint-1820\preprocessor_config.json
{'loss': 0.9282, 'learning_rate': 3.4188034188034193e-06, 'epoch': 14.08}
{'loss': 0.7899, 'learning_rate': 3.133903133903134e-06, 'epoch': 14.15}
{'loss': 0.8899, 'learning_rate': 2.8490028490028492e-06, 'epoch': 14.23}
{'loss': 0.8306, 'learning_rate': 2.564102564102564e-06, 'epoch': 14.31}
{'loss': 0.7033, 'learning_rate': 2.2792022792022796e-06, 'epoch': 14.38}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.752, 'learning_rate': 1.9943019943019943e-06, 'epoch': 14.46}
{'loss': 0.7508, 'learning_rate': 1.7094017094017097e-06, 'epoch': 14.54}
{'loss': 0.8044, 'learning_rate': 1.4245014245014246e-06, 'epoch': 14.61}
{'loss': 0.8503, 'learning_rate': 1.1396011396011398e-06, 'epoch': 14.69}
{'loss': 0.828, 'learning_rate': 8.547008547008548e-07, 'epoch': 14.77}
{'loss': 0.8348, 'learning_rate': 5.698005698005699e-07, 'epoch': 14.84}
{'loss': 0.7573, 'learning_rate': 2.8490028490028494e-07, 'epoch': 14.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 0.7847, 'learning_rate': 0.0, 'epoch': 15.0}
{'eval_loss': 1.0982977151870728, 'eval_accuracy': 0.6559346468044209, 'eval_f1': 0.6559346468044209, 'eval_precision': 0.6559346468044209, 'eval_recall': 0.6559346468044209, 'eval_runtime': 11.825, 'eval_samples_per_second': 175.983, 'eval_steps_per_second': 11.078, 'epoch': 15.0}
Saving model checkpoint to models/model_Rigging Type\checkpoint-1950
Configuration saved in models/model_Rigging Type\checkpoint-1950\config.json
Model weights saved in models/model_Rigging Type\checkpoint-1950\pytorch_model.bin
Image processor saved in models/model_Rigging Type\checkpoint-1950\preprocessor_config.json
Training completed. Do not forget to share your model on huggingface.co/models =)
Loading best model from models/model_Rigging Type\checkpoint-1300 (score: 0.6559346468044209).
Setting `WANDB_LOG_MODEL` from true to `end` instead
Saving model checkpoint to C:\Users\chris\AppData\Local\Temp\tmp5e7urknf
Configuration saved in C:\Users\chris\AppData\Local\Temp\tmp5e7urknf\config.json
{'train_runtime': 987.3317, 'train_samples_per_second': 126.447, 'train_steps_per_second': 1.975, 'train_loss': 1.1207897944328113, 'epoch': 15.0}
Model weights saved in C:\Users\chris\AppData\Local\Temp\tmp5e7urknf\pytorch_model.bin
Image processor saved in C:\Users\chris\AppData\Local\Temp\tmp5e7urknf\preprocessor_config.json
Logging model artifacts. ...