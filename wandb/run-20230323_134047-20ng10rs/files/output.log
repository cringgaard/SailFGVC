Loading cached split indices for dataset at C:\Users\chris\.cache\huggingface\datasets\cringgaard___boats_dataset\default\0.0.0\cf118af5708518fea28486aed25e2f1632c5b8d5e716255a840c5c012a2b161b\cache-60009d52c7c5e571.arrow and C:\Users\chris\.cache\huggingface\datasets\cringgaard___boats_dataset\default\0.0.0\cf118af5708518fea28486aed25e2f1632c5b8d5e716255a840c5c012a2b161b\cache-5ebfa60464b5cdf9.arrow
Loading cached processed dataset at C:\Users\chris\.cache\huggingface\datasets\cringgaard___boats_dataset\default\0.0.0\cf118af5708518fea28486aed25e2f1632c5b8d5e716255a840c5c012a2b161b\cache-94ed6d8196f9a191.arrow
Loading cached processed dataset at C:\Users\chris\.cache\huggingface\datasets\cringgaard___boats_dataset\default\0.0.0\cf118af5708518fea28486aed25e2f1632c5b8d5e716255a840c5c012a2b161b\cache-b1bc5046c8473b52.arrow
loading configuration file config.json from cache at C:\Users\chris/.cache\huggingface\hub\models--google--vit-base-patch16-224\snapshots\2ddc9d4e473d7ba52128f0df4723e478fa14fb80\config.json
Model config ViTConfig {
  "_name_or_path": "google/vit-base-patch16-224",
  "architectures": [
    "ViTForImageClassification"
  ],
  "attention_probs_dropout_prob": 0.0,
  "encoder_stride": 16,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31",
    "32": "LABEL_32",
    "33": "LABEL_33",
    "34": "LABEL_34",
    "35": "LABEL_35",
    "36": "LABEL_36",
    "37": "LABEL_37"
  },
  "image_size": 224,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_32": 32,
    "LABEL_33": 33,
    "LABEL_34": 34,
    "LABEL_35": 35,
    "LABEL_36": 36,
    "LABEL_37": 37,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "model_type": "vit",
  "num_attention_heads": 12,
  "num_channels": 3,
  "num_hidden_layers": 12,
  "patch_size": 16,
  "qkv_bias": true,
  "transformers_version": "4.26.1"
}
loading weights file pytorch_model.bin from cache at C:\Users\chris/.cache\huggingface\hub\models--google--vit-base-patch16-224\snapshots\2ddc9d4e473d7ba52128f0df4723e478fa14fb80\pytorch_model.bin
All model checkpoint weights were used when initializing ViTForImageClassification.
Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:
- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([38, 768]) in the model instantiated
- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([38]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
Setting `WANDB_LOG_MODEL` from true to `end` instead
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 8323
  Num Epochs = 15
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 4
  Total optimization steps = 1950
  Number of trainable parameters = 85827878
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37]
[0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 36]
[36]
['Fractional  Sloop', 'Gunter', 'Cat (Marconi)', 'Masthead Sloop', 'Fractional (9/10) Sloop', 'Cat (rotating spar)', 'Wing (multi element)', 'Masthead  Yawl', 'NaN', 'Cutter', 'Staysail Ketch', 'Frac. Sloop (Rotating Spar)', 'Masthead  Ketch', 'Cat (unstayed)', 'Gaff head Cat', 'Lateen', 'Solent', 'Gaff Head Cutter', 'Gaffhead Sloop', 'Fractionally Rigged Ketch', 'Cat Ketch (unstayed)', 'Fractional (7/8) Sloop', 'Gaff-Yawl', 'Frac. Sloop (Free standing)', '2 Mst. Schooner', 'Cat Ketch', 'Gaff Topsail Cutter', 'Junk Rig', 'Gaff head  Ketch', 'Sprit/Lug', 'Gunter-Yawl', 'Standing Lug', 'B&R', 'B&R Fractional', 'Cutter/Ketch', 'Sloop or Yawl', 'Brigantine', 'Fractional Yawl']
{'loss': 3.7955, 'learning_rate': 2.564102564102564e-06, 'epoch': 0.08}
{'loss': 3.5819, 'learning_rate': 5.128205128205128e-06, 'epoch': 0.15}
{'loss': 3.1277, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.23}
{'loss': 2.5556, 'learning_rate': 1.0256410256410256e-05, 'epoch': 0.31}
{'loss': 1.9998, 'learning_rate': 1.282051282051282e-05, 'epoch': 0.38}
{'loss': 1.713, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.46}
{'loss': 1.5887, 'learning_rate': 1.794871794871795e-05, 'epoch': 0.54}
{'loss': 1.6207, 'learning_rate': 2.0512820512820512e-05, 'epoch': 0.61}
{'loss': 1.4785, 'learning_rate': 2.307692307692308e-05, 'epoch': 0.69}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 1.6056, 'learning_rate': 2.564102564102564e-05, 'epoch': 0.77}
{'loss': 1.4878, 'learning_rate': 2.8205128205128207e-05, 'epoch': 0.84}
{'loss': 1.5087, 'learning_rate': 3.0769230769230774e-05, 'epoch': 0.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 1.3929, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Rigging Type\checkpoint-130
Configuration saved in models/ViT_Rigging Type\checkpoint-130\config.json
{'eval_loss': 1.4594727754592896, 'eval_accuracy': 0.5593464680442095, 'eval_f1': 0.039774611723933234, 'eval_precision': 0.04401090758323893, 'eval_recall': 0.045037811597607544, 'eval_runtime': 16.226, 'eval_samples_per_second': 128.251, 'eval_steps_per_second': 8.073, 'epoch': 1.0}
Model weights saved in models/ViT_Rigging Type\checkpoint-130\pytorch_model.bin
Image processor saved in models/ViT_Rigging Type\checkpoint-130\preprocessor_config.json
{'loss': 1.3714, 'learning_rate': 3.58974358974359e-05, 'epoch': 1.08}
{'loss': 1.4285, 'learning_rate': 3.846153846153846e-05, 'epoch': 1.15}
{'loss': 1.3913, 'learning_rate': 4.1025641025641023e-05, 'epoch': 1.23}
{'loss': 1.2814, 'learning_rate': 4.358974358974359e-05, 'epoch': 1.31}
{'loss': 1.2537, 'learning_rate': 4.615384615384616e-05, 'epoch': 1.38}
{'loss': 1.3792, 'learning_rate': 4.871794871794872e-05, 'epoch': 1.46}
{'loss': 1.1827, 'learning_rate': 4.985754985754986e-05, 'epoch': 1.54}
{'loss': 1.1849, 'learning_rate': 4.9572649572649575e-05, 'epoch': 1.61}
{'loss': 1.2618, 'learning_rate': 4.928774928774929e-05, 'epoch': 1.69}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 1.2, 'learning_rate': 4.9002849002849004e-05, 'epoch': 1.77}
{'loss': 1.155, 'learning_rate': 4.871794871794872e-05, 'epoch': 1.84}
{'loss': 1.2307, 'learning_rate': 4.8433048433048433e-05, 'epoch': 1.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 1.1492, 'learning_rate': 4.814814814814815e-05, 'epoch': 2.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Rigging Type\checkpoint-260
Configuration saved in models/ViT_Rigging Type\checkpoint-260\config.json
Model weights saved in models/ViT_Rigging Type\checkpoint-260\pytorch_model.bin
Image processor saved in models/ViT_Rigging Type\checkpoint-260\preprocessor_config.json
{'eval_loss': 1.2101713418960571, 'eval_accuracy': 0.6198942815953868, 'eval_f1': 0.08117506492483105, 'eval_precision': 0.1280455240588131, 'eval_recall': 0.07503225515249713, 'eval_runtime': 16.193, 'eval_samples_per_second': 128.512, 'eval_steps_per_second': 8.09, 'epoch': 2.0}
{'loss': 1.1995, 'learning_rate': 4.786324786324787e-05, 'epoch': 2.08}
{'loss': 1.0806, 'learning_rate': 4.7578347578347584e-05, 'epoch': 2.15}
{'loss': 1.0645, 'learning_rate': 4.72934472934473e-05, 'epoch': 2.23}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 1.1224, 'learning_rate': 4.700854700854701e-05, 'epoch': 2.31}
{'loss': 1.0787, 'learning_rate': 4.672364672364672e-05, 'epoch': 2.38}
{'loss': 1.1371, 'learning_rate': 4.643874643874644e-05, 'epoch': 2.46}
{'loss': 1.0075, 'learning_rate': 4.615384615384616e-05, 'epoch': 2.54}
{'loss': 1.0859, 'learning_rate': 4.586894586894587e-05, 'epoch': 2.61}
{'loss': 0.9731, 'learning_rate': 4.558404558404559e-05, 'epoch': 2.69}
{'loss': 1.0721, 'learning_rate': 4.52991452991453e-05, 'epoch': 2.77}
{'loss': 1.1317, 'learning_rate': 4.501424501424502e-05, 'epoch': 2.84}
{'loss': 1.0909, 'learning_rate': 4.472934472934473e-05, 'epoch': 2.92}
{'loss': 0.9932, 'learning_rate': 4.4444444444444447e-05, 'epoch': 3.0}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Rigging Type\checkpoint-390
Configuration saved in models/ViT_Rigging Type\checkpoint-390\config.json
Model weights saved in models/ViT_Rigging Type\checkpoint-390\pytorch_model.bin
Image processor saved in models/ViT_Rigging Type\checkpoint-390\preprocessor_config.json
{'eval_loss': 1.093622088432312, 'eval_accuracy': 0.6568957232099952, 'eval_f1': 0.1482581617641229, 'eval_precision': 0.1865270005610004, 'eval_recall': 0.1364519384156851, 'eval_runtime': 16.479, 'eval_samples_per_second': 126.282, 'eval_steps_per_second': 7.95, 'epoch': 3.0}
{'loss': 0.861, 'learning_rate': 4.415954415954416e-05, 'epoch': 3.08}
{'loss': 0.9361, 'learning_rate': 4.3874643874643876e-05, 'epoch': 3.15}
{'loss': 0.9773, 'learning_rate': 4.358974358974359e-05, 'epoch': 3.23}
{'loss': 0.9844, 'learning_rate': 4.3304843304843306e-05, 'epoch': 3.31}
{'loss': 0.9809, 'learning_rate': 4.301994301994302e-05, 'epoch': 3.38}
{'loss': 0.9623, 'learning_rate': 4.2735042735042735e-05, 'epoch': 3.46}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.9712, 'learning_rate': 4.2450142450142457e-05, 'epoch': 3.54}
{'loss': 1.0163, 'learning_rate': 4.216524216524217e-05, 'epoch': 3.61}
{'loss': 0.9719, 'learning_rate': 4.1880341880341886e-05, 'epoch': 3.69}
{'loss': 0.9445, 'learning_rate': 4.15954415954416e-05, 'epoch': 3.77}
{'loss': 1.0236, 'learning_rate': 4.131054131054131e-05, 'epoch': 3.84}
{'loss': 0.9763, 'learning_rate': 4.1025641025641023e-05, 'epoch': 3.92}
{'loss': 0.9878, 'learning_rate': 4.074074074074074e-05, 'epoch': 4.0}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Rigging Type\checkpoint-520
Configuration saved in models/ViT_Rigging Type\checkpoint-520\config.json
Model weights saved in models/ViT_Rigging Type\checkpoint-520\pytorch_model.bin
Image processor saved in models/ViT_Rigging Type\checkpoint-520\preprocessor_config.json
{'eval_loss': 1.061134696006775, 'eval_accuracy': 0.6636232580490149, 'eval_f1': 0.13421978597995704, 'eval_precision': 0.1762172977534197, 'eval_recall': 0.1237423026083039, 'eval_runtime': 16.6294, 'eval_samples_per_second': 125.14, 'eval_steps_per_second': 7.878, 'epoch': 4.0}
{'loss': 0.9655, 'learning_rate': 4.045584045584046e-05, 'epoch': 4.08}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.7906, 'learning_rate': 4.0170940170940174e-05, 'epoch': 4.15}
{'loss': 0.9713, 'learning_rate': 3.988603988603989e-05, 'epoch': 4.23}
{'loss': 0.9406, 'learning_rate': 3.9601139601139604e-05, 'epoch': 4.31}
{'loss': 0.9141, 'learning_rate': 3.931623931623932e-05, 'epoch': 4.38}
{'loss': 0.816, 'learning_rate': 3.903133903133903e-05, 'epoch': 4.46}
{'loss': 0.8729, 'learning_rate': 3.874643874643875e-05, 'epoch': 4.54}
{'loss': 0.858, 'learning_rate': 3.846153846153846e-05, 'epoch': 4.61}
{'loss': 0.8247, 'learning_rate': 3.817663817663818e-05, 'epoch': 4.69}
{'loss': 0.8132, 'learning_rate': 3.789173789173789e-05, 'epoch': 4.77}
{'loss': 0.839, 'learning_rate': 3.760683760683761e-05, 'epoch': 4.84}
{'loss': 0.806, 'learning_rate': 3.732193732193732e-05, 'epoch': 4.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 0.7242, 'learning_rate': 3.7037037037037037e-05, 'epoch': 5.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Rigging Type\checkpoint-650
Configuration saved in models/ViT_Rigging Type\checkpoint-650\config.json
{'eval_loss': 1.0403414964675903, 'eval_accuracy': 0.667948101874099, 'eval_f1': 0.16151269137328336, 'eval_precision': 0.2619438363962106, 'eval_recall': 0.14858205913123504, 'eval_runtime': 16.2642, 'eval_samples_per_second': 127.95, 'eval_steps_per_second': 8.055, 'epoch': 5.0}
Model weights saved in models/ViT_Rigging Type\checkpoint-650\pytorch_model.bin
Image processor saved in models/ViT_Rigging Type\checkpoint-650\preprocessor_config.json
{'loss': 0.8295, 'learning_rate': 3.675213675213676e-05, 'epoch': 5.08}
{'loss': 0.6909, 'learning_rate': 3.646723646723647e-05, 'epoch': 5.15}
{'loss': 0.7764, 'learning_rate': 3.618233618233619e-05, 'epoch': 5.23}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.7384, 'learning_rate': 3.58974358974359e-05, 'epoch': 5.31}
{'loss': 0.737, 'learning_rate': 3.561253561253561e-05, 'epoch': 5.38}
{'loss': 0.7985, 'learning_rate': 3.5327635327635325e-05, 'epoch': 5.46}
{'loss': 0.8184, 'learning_rate': 3.504273504273504e-05, 'epoch': 5.54}
{'loss': 0.8508, 'learning_rate': 3.475783475783476e-05, 'epoch': 5.61}
{'loss': 0.7811, 'learning_rate': 3.4472934472934476e-05, 'epoch': 5.69}
{'loss': 0.8345, 'learning_rate': 3.418803418803419e-05, 'epoch': 5.77}
{'loss': 0.7625, 'learning_rate': 3.3903133903133905e-05, 'epoch': 5.84}
{'loss': 0.7936, 'learning_rate': 3.361823361823362e-05, 'epoch': 5.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 0.7632, 'learning_rate': 3.3333333333333335e-05, 'epoch': 6.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Rigging Type\checkpoint-780
Configuration saved in models/ViT_Rigging Type\checkpoint-780\config.json
Model weights saved in models/ViT_Rigging Type\checkpoint-780\pytorch_model.bin
Image processor saved in models/ViT_Rigging Type\checkpoint-780\preprocessor_config.json
{'eval_loss': 1.009352684020996, 'eval_accuracy': 0.6713118692936089, 'eval_f1': 0.1850627558682508, 'eval_precision': 0.2640161062805436, 'eval_recall': 0.1667568749414471, 'eval_runtime': 16.281, 'eval_samples_per_second': 127.818, 'eval_steps_per_second': 8.046, 'epoch': 6.0}
{'loss': 0.7695, 'learning_rate': 3.304843304843305e-05, 'epoch': 6.08}
{'loss': 0.7223, 'learning_rate': 3.2763532763532764e-05, 'epoch': 6.15}
{'loss': 0.676, 'learning_rate': 3.247863247863248e-05, 'epoch': 6.23}
{'loss': 0.7572, 'learning_rate': 3.2193732193732194e-05, 'epoch': 6.31}
{'loss': 0.6846, 'learning_rate': 3.190883190883191e-05, 'epoch': 6.38}
{'loss': 0.665, 'learning_rate': 3.162393162393162e-05, 'epoch': 6.46}
{'loss': 0.7057, 'learning_rate': 3.133903133903134e-05, 'epoch': 6.54}
{'loss': 0.7441, 'learning_rate': 3.105413105413106e-05, 'epoch': 6.61}
{'loss': 0.7526, 'learning_rate': 3.0769230769230774e-05, 'epoch': 6.69}
{'loss': 0.6618, 'learning_rate': 3.0484330484330486e-05, 'epoch': 6.77}
{'loss': 0.7217, 'learning_rate': 3.01994301994302e-05, 'epoch': 6.84}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.7026, 'learning_rate': 2.9914529914529915e-05, 'epoch': 6.92}
{'loss': 0.7722, 'learning_rate': 2.962962962962963e-05, 'epoch': 7.0}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Rigging Type\checkpoint-910
Configuration saved in models/ViT_Rigging Type\checkpoint-910\config.json
Model weights saved in models/ViT_Rigging Type\checkpoint-910\pytorch_model.bin
Image processor saved in models/ViT_Rigging Type\checkpoint-910\preprocessor_config.json
{'eval_loss': 1.0171763896942139, 'eval_accuracy': 0.6703507928880346, 'eval_f1': 0.19695544025038658, 'eval_precision': 0.2744546077179035, 'eval_recall': 0.1747854497427083, 'eval_runtime': 16.5626, 'eval_samples_per_second': 125.645, 'eval_steps_per_second': 7.909, 'epoch': 7.0}
{'loss': 0.6405, 'learning_rate': 2.9344729344729345e-05, 'epoch': 7.08}
{'loss': 0.6511, 'learning_rate': 2.9059829059829063e-05, 'epoch': 7.15}
{'loss': 0.6643, 'learning_rate': 2.8774928774928778e-05, 'epoch': 7.23}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.6841, 'learning_rate': 2.8490028490028492e-05, 'epoch': 7.31}
{'loss': 0.6643, 'learning_rate': 2.8205128205128207e-05, 'epoch': 7.38}
{'loss': 0.7227, 'learning_rate': 2.7920227920227922e-05, 'epoch': 7.46}
{'loss': 0.6227, 'learning_rate': 2.7635327635327633e-05, 'epoch': 7.54}
{'loss': 0.6397, 'learning_rate': 2.7350427350427355e-05, 'epoch': 7.61}
{'loss': 0.7132, 'learning_rate': 2.706552706552707e-05, 'epoch': 7.69}
{'loss': 0.6441, 'learning_rate': 2.6780626780626784e-05, 'epoch': 7.77}
{'loss': 0.6848, 'learning_rate': 2.64957264957265e-05, 'epoch': 7.84}
{'loss': 0.6544, 'learning_rate': 2.621082621082621e-05, 'epoch': 7.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 0.6574, 'learning_rate': 2.5925925925925925e-05, 'epoch': 8.0}
{'eval_loss': 1.0194205045700073, 'eval_accuracy': 0.6847669389716482, 'eval_f1': 0.2061717525807183, 'eval_precision': 0.25264385092533964, 'eval_recall': 0.187566190517443, 'eval_runtime': 16.1664, 'eval_samples_per_second': 128.724, 'eval_steps_per_second': 8.103, 'epoch': 8.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Rigging Type\checkpoint-1040
Configuration saved in models/ViT_Rigging Type\checkpoint-1040\config.json
Model weights saved in models/ViT_Rigging Type\checkpoint-1040\pytorch_model.bin
Image processor saved in models/ViT_Rigging Type\checkpoint-1040\preprocessor_config.json
{'loss': 0.6553, 'learning_rate': 2.564102564102564e-05, 'epoch': 8.08}
{'loss': 0.5719, 'learning_rate': 2.535612535612536e-05, 'epoch': 8.15}
{'loss': 0.6599, 'learning_rate': 2.5071225071225073e-05, 'epoch': 8.23}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.6225, 'learning_rate': 2.4786324786324787e-05, 'epoch': 8.31}
{'loss': 0.6139, 'learning_rate': 2.4501424501424502e-05, 'epoch': 8.38}
{'loss': 0.6033, 'learning_rate': 2.4216524216524217e-05, 'epoch': 8.46}
{'loss': 0.6213, 'learning_rate': 2.3931623931623935e-05, 'epoch': 8.54}
{'loss': 0.5807, 'learning_rate': 2.364672364672365e-05, 'epoch': 8.61}
{'loss': 0.6791, 'learning_rate': 2.336182336182336e-05, 'epoch': 8.69}
{'loss': 0.683, 'learning_rate': 2.307692307692308e-05, 'epoch': 8.77}
{'loss': 0.5703, 'learning_rate': 2.2792022792022794e-05, 'epoch': 8.84}
{'loss': 0.6248, 'learning_rate': 2.250712250712251e-05, 'epoch': 8.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 0.5212, 'learning_rate': 2.2222222222222223e-05, 'epoch': 9.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Rigging Type\checkpoint-1170
Configuration saved in models/ViT_Rigging Type\checkpoint-1170\config.json
Model weights saved in models/ViT_Rigging Type\checkpoint-1170\pytorch_model.bin
Image processor saved in models/ViT_Rigging Type\checkpoint-1170\preprocessor_config.json
{'eval_loss': 1.017215609550476, 'eval_accuracy': 0.6905333974050937, 'eval_f1': 0.1924191200629454, 'eval_precision': 0.256453623905414, 'eval_recall': 0.16817701443123526, 'eval_runtime': 16.206, 'eval_samples_per_second': 128.409, 'eval_steps_per_second': 8.083, 'epoch': 9.0}
{'loss': 0.6038, 'learning_rate': 2.1937321937321938e-05, 'epoch': 9.08}
{'loss': 0.5408, 'learning_rate': 2.1652421652421653e-05, 'epoch': 9.15}
{'loss': 0.5581, 'learning_rate': 2.1367521367521368e-05, 'epoch': 9.23}
{'loss': 0.5927, 'learning_rate': 2.1082621082621086e-05, 'epoch': 9.31}
{'loss': 0.5159, 'learning_rate': 2.07977207977208e-05, 'epoch': 9.38}
{'loss': 0.5118, 'learning_rate': 2.0512820512820512e-05, 'epoch': 9.46}
{'loss': 0.5554, 'learning_rate': 2.022792022792023e-05, 'epoch': 9.54}
{'loss': 0.5573, 'learning_rate': 1.9943019943019945e-05, 'epoch': 9.61}
{'loss': 0.594, 'learning_rate': 1.965811965811966e-05, 'epoch': 9.69}
{'loss': 0.5786, 'learning_rate': 1.9373219373219374e-05, 'epoch': 9.77}
{'loss': 0.5581, 'learning_rate': 1.908831908831909e-05, 'epoch': 9.84}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.6816, 'learning_rate': 1.8803418803418804e-05, 'epoch': 9.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 0.6651, 'learning_rate': 1.8518518518518518e-05, 'epoch': 10.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Rigging Type\checkpoint-1300
Configuration saved in models/ViT_Rigging Type\checkpoint-1300\config.json
Model weights saved in models/ViT_Rigging Type\checkpoint-1300\pytorch_model.bin
Image processor saved in models/ViT_Rigging Type\checkpoint-1300\preprocessor_config.json
{'eval_loss': 1.017815351486206, 'eval_accuracy': 0.6847669389716482, 'eval_f1': 0.21348634542071263, 'eval_precision': 0.2461088559721355, 'eval_recall': 0.20861332352834466, 'eval_runtime': 16.5377, 'eval_samples_per_second': 125.834, 'eval_steps_per_second': 7.921, 'epoch': 10.0}
{'loss': 0.5529, 'learning_rate': 1.8233618233618236e-05, 'epoch': 10.08}
{'loss': 0.5309, 'learning_rate': 1.794871794871795e-05, 'epoch': 10.15}
{'loss': 0.4939, 'learning_rate': 1.7663817663817662e-05, 'epoch': 10.23}
{'loss': 0.5379, 'learning_rate': 1.737891737891738e-05, 'epoch': 10.31}
{'loss': 0.5641, 'learning_rate': 1.7094017094017095e-05, 'epoch': 10.38}
{'loss': 0.5565, 'learning_rate': 1.680911680911681e-05, 'epoch': 10.46}
{'loss': 0.5087, 'learning_rate': 1.6524216524216525e-05, 'epoch': 10.54}
{'loss': 0.5618, 'learning_rate': 1.623931623931624e-05, 'epoch': 10.61}
{'loss': 0.5579, 'learning_rate': 1.5954415954415954e-05, 'epoch': 10.69}
{'loss': 0.5591, 'learning_rate': 1.566951566951567e-05, 'epoch': 10.77}
{'loss': 0.4939, 'learning_rate': 1.5384615384615387e-05, 'epoch': 10.84}
{'loss': 0.5181, 'learning_rate': 1.50997150997151e-05, 'epoch': 10.92}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 0.4661, 'learning_rate': 1.4814814814814815e-05, 'epoch': 11.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Rigging Type\checkpoint-1430
Configuration saved in models/ViT_Rigging Type\checkpoint-1430\config.json
{'eval_loss': 0.9819446206092834, 'eval_accuracy': 0.695819317635752, 'eval_f1': 0.22028052153434374, 'eval_precision': 0.2862622624305365, 'eval_recall': 0.20021185557297042, 'eval_runtime': 16.7403, 'eval_samples_per_second': 124.311, 'eval_steps_per_second': 7.825, 'epoch': 11.0}
Model weights saved in models/ViT_Rigging Type\checkpoint-1430\pytorch_model.bin
Image processor saved in models/ViT_Rigging Type\checkpoint-1430\preprocessor_config.json
{'loss': 0.4716, 'learning_rate': 1.4529914529914531e-05, 'epoch': 11.08}
{'loss': 0.5144, 'learning_rate': 1.4245014245014246e-05, 'epoch': 11.15}
{'loss': 0.5121, 'learning_rate': 1.3960113960113961e-05, 'epoch': 11.23}
{'loss': 0.6037, 'learning_rate': 1.3675213675213677e-05, 'epoch': 11.31}
{'loss': 0.5174, 'learning_rate': 1.3390313390313392e-05, 'epoch': 11.38}
{'loss': 0.5047, 'learning_rate': 1.3105413105413105e-05, 'epoch': 11.46}
{'loss': 0.4906, 'learning_rate': 1.282051282051282e-05, 'epoch': 11.54}
{'loss': 0.517, 'learning_rate': 1.2535612535612536e-05, 'epoch': 11.61}
{'loss': 0.4596, 'learning_rate': 1.2250712250712251e-05, 'epoch': 11.69}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.5556, 'learning_rate': 1.1965811965811967e-05, 'epoch': 11.77}
{'loss': 0.5345, 'learning_rate': 1.168091168091168e-05, 'epoch': 11.84}
{'loss': 0.4358, 'learning_rate': 1.1396011396011397e-05, 'epoch': 11.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 0.5358, 'learning_rate': 1.1111111111111112e-05, 'epoch': 12.0}
{'eval_loss': 1.041707158088684, 'eval_accuracy': 0.6794810187409899, 'eval_f1': 0.19728528721647492, 'eval_precision': 0.2594194733521644, 'eval_recall': 0.1762721322706157, 'eval_runtime': 16.5687, 'eval_samples_per_second': 125.598, 'eval_steps_per_second': 7.906, 'epoch': 12.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Rigging Type\checkpoint-1560
Configuration saved in models/ViT_Rigging Type\checkpoint-1560\config.json
Model weights saved in models/ViT_Rigging Type\checkpoint-1560\pytorch_model.bin
Image processor saved in models/ViT_Rigging Type\checkpoint-1560\preprocessor_config.json
{'loss': 0.4499, 'learning_rate': 1.0826210826210826e-05, 'epoch': 12.08}
{'loss': 0.4936, 'learning_rate': 1.0541310541310543e-05, 'epoch': 12.15}
{'loss': 0.4858, 'learning_rate': 1.0256410256410256e-05, 'epoch': 12.23}
{'loss': 0.4962, 'learning_rate': 9.971509971509972e-06, 'epoch': 12.31}
{'loss': 0.4679, 'learning_rate': 9.686609686609687e-06, 'epoch': 12.38}
{'loss': 0.4626, 'learning_rate': 9.401709401709402e-06, 'epoch': 12.46}
{'loss': 0.4909, 'learning_rate': 9.116809116809118e-06, 'epoch': 12.54}
{'loss': 0.4425, 'learning_rate': 8.831908831908831e-06, 'epoch': 12.61}
{'loss': 0.5345, 'learning_rate': 8.547008547008548e-06, 'epoch': 12.69}
{'loss': 0.4734, 'learning_rate': 8.262108262108262e-06, 'epoch': 12.77}
{'loss': 0.5496, 'learning_rate': 7.977207977207977e-06, 'epoch': 12.84}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.4476, 'learning_rate': 7.692307692307694e-06, 'epoch': 12.92}
{'loss': 0.4613, 'learning_rate': 7.4074074074074075e-06, 'epoch': 13.0}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Rigging Type\checkpoint-1690
Configuration saved in models/ViT_Rigging Type\checkpoint-1690\config.json
Model weights saved in models/ViT_Rigging Type\checkpoint-1690\pytorch_model.bin
Image processor saved in models/ViT_Rigging Type\checkpoint-1690\preprocessor_config.json
{'eval_loss': 0.9957237243652344, 'eval_accuracy': 0.7078327727054301, 'eval_f1': 0.23341680328307565, 'eval_precision': 0.30235613236047937, 'eval_recall': 0.2115961172569109, 'eval_runtime': 16.5283, 'eval_samples_per_second': 125.905, 'eval_steps_per_second': 7.926, 'epoch': 13.0}
{'loss': 0.4419, 'learning_rate': 7.122507122507123e-06, 'epoch': 13.08}
{'loss': 0.453, 'learning_rate': 6.837606837606839e-06, 'epoch': 13.15}
{'loss': 0.4745, 'learning_rate': 6.5527065527065525e-06, 'epoch': 13.23}
{'loss': 0.4434, 'learning_rate': 6.267806267806268e-06, 'epoch': 13.31}
{'loss': 0.4735, 'learning_rate': 5.982905982905984e-06, 'epoch': 13.38}
{'loss': 0.4444, 'learning_rate': 5.6980056980056985e-06, 'epoch': 13.46}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.4954, 'learning_rate': 5.413105413105413e-06, 'epoch': 13.54}
{'loss': 0.4655, 'learning_rate': 5.128205128205128e-06, 'epoch': 13.61}
{'loss': 0.4757, 'learning_rate': 4.8433048433048435e-06, 'epoch': 13.69}
{'loss': 0.4534, 'learning_rate': 4.558404558404559e-06, 'epoch': 13.77}
{'loss': 0.4394, 'learning_rate': 4.273504273504274e-06, 'epoch': 13.84}
{'loss': 0.4735, 'learning_rate': 3.988603988603989e-06, 'epoch': 13.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 0.4846, 'learning_rate': 3.7037037037037037e-06, 'epoch': 14.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Rigging Type\checkpoint-1820
Configuration saved in models/ViT_Rigging Type\checkpoint-1820\config.json
{'eval_loss': 1.0425660610198975, 'eval_accuracy': 0.6890917827967323, 'eval_f1': 0.2325981225644445, 'eval_precision': 0.3191056672233251, 'eval_recall': 0.204798629663093, 'eval_runtime': 16.8727, 'eval_samples_per_second': 123.335, 'eval_steps_per_second': 7.764, 'epoch': 14.0}
Model weights saved in models/ViT_Rigging Type\checkpoint-1820\pytorch_model.bin
Image processor saved in models/ViT_Rigging Type\checkpoint-1820\preprocessor_config.json
{'loss': 0.459, 'learning_rate': 3.4188034188034193e-06, 'epoch': 14.08}
{'loss': 0.4342, 'learning_rate': 3.133903133903134e-06, 'epoch': 14.15}
{'loss': 0.4855, 'learning_rate': 2.8490028490028492e-06, 'epoch': 14.23}
{'loss': 0.5152, 'learning_rate': 2.564102564102564e-06, 'epoch': 14.31}
{'loss': 0.4117, 'learning_rate': 2.2792022792022796e-06, 'epoch': 14.38}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\PIL\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
{'loss': 0.3994, 'learning_rate': 1.9943019943019943e-06, 'epoch': 14.46}
{'loss': 0.442, 'learning_rate': 1.7094017094017097e-06, 'epoch': 14.54}
{'loss': 0.4592, 'learning_rate': 1.4245014245014246e-06, 'epoch': 14.61}
{'loss': 0.5083, 'learning_rate': 1.1396011396011398e-06, 'epoch': 14.69}
{'loss': 0.4454, 'learning_rate': 8.547008547008548e-07, 'epoch': 14.77}
{'loss': 0.4455, 'learning_rate': 5.698005698005699e-07, 'epoch': 14.84}
{'loss': 0.4281, 'learning_rate': 2.8490028490028494e-07, 'epoch': 14.92}
***** Running Evaluation *****
  Num examples = 2081
  Batch size = 16
{'loss': 0.4085, 'learning_rate': 0.0, 'epoch': 15.0}
{'eval_loss': 0.9976723790168762, 'eval_accuracy': 0.699663623258049, 'eval_f1': 0.2271372701106199, 'eval_precision': 0.3098228692672205, 'eval_recall': 0.1987879672605256, 'eval_runtime': 16.482, 'eval_samples_per_second': 126.259, 'eval_steps_per_second': 7.948, 'epoch': 15.0}
c:\Users\chris\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Saving model checkpoint to models/ViT_Rigging Type\checkpoint-1950
Configuration saved in models/ViT_Rigging Type\checkpoint-1950\config.json
Model weights saved in models/ViT_Rigging Type\checkpoint-1950\pytorch_model.bin
Image processor saved in models/ViT_Rigging Type\checkpoint-1950\preprocessor_config.json
Training completed. Do not forget to share your model on huggingface.co/models =)
Loading best model from models/ViT_Rigging Type\checkpoint-1690 (score: 0.23341680328307565).
Setting `WANDB_LOG_MODEL` from true to `end` instead
Saving model checkpoint to C:\Users\chris\AppData\Local\Temp\tmp67t_pn9_
Configuration saved in C:\Users\chris\AppData\Local\Temp\tmp67t_pn9_\config.json
Model weights saved in C:\Users\chris\AppData\Local\Temp\tmp67t_pn9_\pytorch_model.bin
Image processor saved in C:\Users\chris\AppData\Local\Temp\tmp67t_pn9_\preprocessor_config.json
Logging model artifacts. ...
{'train_runtime': 2257.6752, 'train_samples_per_second': 55.298, 'train_steps_per_second': 0.864, 'train_loss': 0.8053199263108082, 'epoch': 15.0}