{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import urllib.request as img_request\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json \n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from os import makedirs\n",
    "from os import listdir\n",
    "from os import getcwd\n",
    "import os\n",
    "from data.classes import *\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from Labeller import Labeller\n",
    "import PIL\n",
    "import torchvision\n",
    "import glob\n",
    "import torch\n",
    "import io\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining important directories as variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = getcwd()\n",
    "data_dir = \"data/\"\n",
    "img_dir = \"E:/data/images/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining regex tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (os.path.exists(data_dir)):\n",
    "    print(\"Making dir\" , data_dir)\n",
    "    makedirs(data_dir)\n",
    "\n",
    "if not(os.path.exists(img_dir)):\n",
    "    print(\"Making dir\" , img_dir)\n",
    "    makedirs(img_dir)\n",
    "\n",
    "if not(os.path.exists(img_dir+\"/boat24\")):\n",
    "    print(\"Making dir\" , img_dir+\"/boat24\")\n",
    "    makedirs(img_dir+\"/boat24\")\n",
    "\n",
    "if not(os.path.exists(img_dir+\"/image_search\")):\n",
    "    print(\"Making dir\" , img_dir+\"/image_search\")\n",
    "    makedirs(img_dir+\"/image_search\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sailboatdata.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pages_tag = re.compile(r'<a href=.*page=(\\d*).*/li>') # For getting the amount of pages on the domain\n",
    "\n",
    "name_tag = re.compile(r'<a href=\\\"https://sailboatdata\\.com/sailboat/.*\\\">(.*)</a>') # For making a list of all the boats on sailboatdata\n",
    "\n",
    "specs_tag = re.compile(r'<div class=\\\" col-\\w\\w-\\d*  col-\\w\\w-6 sailboatdata-label \\\">\\s*(.*):\\s</div>\\s<.*\\s*(.*)') # For scraping data from specific boat url\n",
    "\n",
    "image_tag = re.compile(r'(?:photo|drawing)\\\".*src=\\\"(http.*)\\\"/>')\n",
    "\n",
    "photo_draw_tag = re.compile(r'sailboat/(\\w*)')\n",
    "\n",
    "name_description_tag = re.compile(r'title\" content=\"(.+)\"[^½]+?description\" content=\"(.+)\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs = []\n",
    "categories = []\n",
    "boat_data = pd.DataFrame()\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_url = 'https://sailboatdata.com/sailboat?page={}&paginate=25'\n",
    "\n",
    "print(\"Connecting to\" , general_url.format(1))\n",
    "init_request = requests.get(general_url.format(1))\n",
    "print(init_request.status_code)\n",
    "pages = re.findall(n_pages_tag , init_request.text)\n",
    "n_pages = pages[-1]\n",
    "\n",
    "boat_types = []\n",
    "for i in tqdm(range (1,int(n_pages)+1)):\n",
    "  # print(\"Connecting to\" , general_url.format(i))\n",
    "  r = requests.get(general_url.format(i))\n",
    "  # print(r.status_code)\n",
    "  boat_types += re.findall(name_tag,r.text)\n",
    "  time.sleep(0.5) #To not throw too many requests at the website\n",
    "\n",
    "print(len(boat_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for boat in boat_types:\n",
    "    # GETTNG REQUEST\n",
    "    print(\"Scraping\",boat,\"...\")\n",
    "    r = requests.get(\"https://sailboatdata.com/sailboat/{}?units=metric\".format(boat.replace(\" \",\"-\").replace(\"(\",\"\").replace(\")\",\"\").replace(\".\",\"\")))\n",
    "    raw_text = r.text\n",
    "\n",
    "    # SCRAPING IMAGES\n",
    "    images = re.findall(image_tag,raw_text)\n",
    "    print(\"         -------scraping\")\n",
    "    for image in images:\n",
    "        img_path = str(counter).zfill(5)+\".jpg\"\n",
    "        if not os.path.isfile(img_path):\n",
    "            counter += 1\n",
    "            print(image)\n",
    "            try:\n",
    "                img_request.urlretrieve(image,os.path.join(img_dir,'sailboatdata')+img_path)\n",
    "            except:\n",
    "                print(\"Strange url\" , image.replace(\" \",\"%20\"))\n",
    "                try:\n",
    "                    img_request.urlretrieve(image,img_dir+img_path)\n",
    "                except:\n",
    "                    has_image = False\n",
    "\n",
    "            # SCRAPING SPECS\n",
    "            categories_specs = re.findall(specs_tag,raw_text)\n",
    "            specs = [el[1] for el in categories_specs]\n",
    "            categories = [el[0] for el in categories_specs]\n",
    "            new_row = {categories[i]: specs[i] for i in range(len(categories))}\n",
    "            new_row['name'] = boat\n",
    "            new_row['img_path'] = img_path\n",
    "            boat_data = pd.concat([boat_data, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Done\\n\")\n",
    "    if counter%100 == 1:\n",
    "        print(\"saving\")\n",
    "        boat_data.to_csv(data_dir+\"boat_data.csv\" , index=False)\n",
    "boat_data.to_csv(data_dir+\"boat_data.csv\" , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "data_raw = pd.read_csv(data_dir+\"boat_data.csv\")\n",
    "data_raw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## boat24.com"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pages_tag = re.compile(r'\\?page=(\\d{3,})') # For getting the amount of pages on the domain\n",
    "\n",
    "# name_and_url_tag = re.compile(r'__title\\\"><a href=\\\"(https://www\\.boat24\\.com.+)\\\"\\stitle=\\\"(.+?)\\\"\\s>')\n",
    "\n",
    "# image_tag = re.compile(r'srcset=\\\"(https://[^\\s]+?\\.jpg) \\d[^2]')\n",
    "\n",
    "# name_tag = re.compile(r'\\\"blurb__title\\\"><a.+?title=\\\"(.+?)\\\"\\s*')\n",
    "# image_tag=re.compile(r'<div class=\\\"blurb__image-area\\\".+?srcset=\\\"(https://static\\.b24\\.co/fotos.+?\\.jpg)|(https://www\\.boat24\\.com/img/icons/boat-types/mynd/type-1-230\\.png)')\n",
    "# url_tag = re.compile(r'<a href=\\\"(https://www\\.boat24\\.com/en/sailingboats/.+)')\n",
    "url_name_image_tag = re.compile(r'data-link=\\\"(https://www\\.boat24\\.com/en/sailingboats/.{1,100}?\\d\\/)\\\"[^¤]*?title=\\\"(.{1,40})\\\"><div class=\\\"blurb__image-area\\\".+?srcset=\\\"(https://static\\.b24\\.co/fotos.+?\\.jpg)')\n",
    "image_tag = re.compile(r'src=\\\"(https://static\\.b24\\.co/fotos.+?\\.jpg)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(url , tag):\n",
    "    r = requests.get(url)\n",
    "    retval = re.findall(tag , r.text)\n",
    "    return list(zip(*retval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sailboatdata_data = pd.read_csv(os.path.join(data_dir, \"boat_data_clean.csv\"))\n",
    "# try:\n",
    "#     boat24_data_raw = pd.read_csv(os.path.join(data_dir, \"boat24_data_raw.csv\"))\n",
    "# except:\n",
    "#     boat24_data_raw = pd.DataFrame(columns = [\"url\",\"name\",\"img_url , html\"])\n",
    "# class Labeller:\n",
    "#     def __init__(self , boat_names = Name_Classes.values(), embedding_size = 2**8):\n",
    "#         self.boat_names = boat_names\n",
    "#         self.onehot_map = {}\n",
    "#         self.embedding_size = embedding_size\n",
    "#         self.str2label = {v: k for k, v in Name_Classes.items()}\n",
    "#         for boat in boat_names:\n",
    "#             self.onehot_encode(boat)\n",
    "#         self.label2embedding = {k: self.onehot_encode(v) for k, v in sorted(Name_Classes.items())}\n",
    "\n",
    "#     def onehot_encode(self, boat_name):\n",
    "#         retval = np.zeros(self.embedding_size)\n",
    "#         for char in boat_name.upper():\n",
    "#             try:\n",
    "#                 retval += self.onehot_map[char]\n",
    "#             except:\n",
    "#                 embedding = np.zeros(2**8)\n",
    "#                 embedding[len(self.onehot_map.keys())] = 1\n",
    "#                 self.onehot_map[char] = embedding\n",
    "#                 retval += self.onehot_map[char]\n",
    "#         return retval\n",
    "    \n",
    "#     def add_label(self , boat_name , distance_threshold = 2):\n",
    "#         try:\n",
    "#             return True , self.str2label[boat_name.upper()] , 0\n",
    "#         except:\n",
    "#             embedding = self.onehot_encode(boat_name)\n",
    "#             distances = np.linalg.norm(embedding - np.array(list(self.label2embedding.values())), axis=1)\n",
    "#             return_label = np.argmin(distances)\n",
    "#             if distances[return_label] < distance_threshold:\n",
    "#                 # print(boat_name+\" not found adding label\" , Name_Classes[int(return_label)]+\" with the distance\" , distances[return_label])\n",
    "#                 return True , return_label , distances[return_label]\n",
    "#             else:\n",
    "#                 # print(boat_name+\" not found closest was\" , Name_Classes[int(return_label)]+\" with the distance\" , distances[return_label])\n",
    "#                 return False , return_label , distances[return_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_test , names_test , images_test = scrape(\"https://www.boat24.com/en/sailingboats/?page=1\" , url_name_image_tag)\n",
    "print(urls_test)\n",
    "print(names_test)\n",
    "print(images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boat24_data_raw = pd.read_csv('data/boat24_data_raw.csv')\n",
    "sailboatdata_data = pd.read_csv('data/boat_data_clean.csv')\n",
    "boat_names = [Name_Classes[x] for x in sailboatdata_data['name'].unique()]\n",
    "\n",
    "try:\n",
    "    already_scraped = pd.read_csv(os.path.join(data_dir, \"already_scraped.csv\") , header=None)\n",
    "except:\n",
    "    already_scraped = pd.DataFrame(['dummy_url'])\n",
    "try:\n",
    "    boat24_data_clean = pd.read_csv(os.path.join(data_dir, \"boat24_data_clean.csv\"))\n",
    "except:\n",
    "    boat24_data_clean = pd.DataFrame(columns = sailboatdata_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_request = requests.get(\"https://www.boat24.com/en/sailingboats/?page=0\")\n",
    "n_pages = re.findall(n_pages_tag , init_request.text)\n",
    "print(int(n_pages[0]))\n",
    "\n",
    "urls = []\n",
    "names = []\n",
    "images = []\n",
    "html = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(0,int(n_pages[0])+1,20)):\n",
    "    url_list , name_list , image_list = scrape(\"https://www.boat24.com/en/sailingboats/?page={}\".format(i) , url_name_image_tag)\n",
    "    urls += url_list\n",
    "    names += name_list\n",
    "    images += image_list\n",
    "    html += [requests.get(url).text for url in url_list]\n",
    "\n",
    "    time.sleep(0.1) #To not throw too many requests at the website\n",
    "boat24_data_raw = pd.concat([boat24_data_raw , pd.DataFrame({\"url\":urls , \"name\":names , \"img_url\":images , \"html\":html})])\n",
    "boat24_data_raw.drop_duplicates(subset=['url'] , keep='first')\n",
    "boat24_data_raw['name'] = boat24_data_raw['name'].str.upper()\n",
    "boat24_data_raw.to_csv(data_dir+\"boat24_data_raw.csv\" , index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could use UrlListCrawler with:\n",
    "# from icrawler.builtin import UrlListCrawler to download using more threads\n",
    "# see https://icrawler.readthedocs.io/en/latest/builtin.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeller = Labeller(boat_names)\n",
    "counter = os.listdir(os.path.join(os.getcwd(),img_dir,\"boat24\")).__len__()\n",
    "save = False\n",
    "\n",
    "for i in tqdm(range(len(boat24_data_raw))):\n",
    "    found , name , distance = labeller.add_label([BeautifulSoup(boat24_data_raw['name'][i].lower() , 'html.parser').get_text()])\n",
    "    if found and not (boat24_data_raw['url'][i] == already_scraped[0]).any():\n",
    "        images = re.findall(image_tag , boat24_data_raw['html'][i])\n",
    "        print('Setting' , BeautifulSoup(boat24_data_raw['name'][i] , 'html.parser').get_text() , 'to' , Name_Classes[name] , 'with distance' , distance)\n",
    "        for image in images:\n",
    "            file_name = \"boat24_\"+str(counter).zfill(6)+\".jpg\"\n",
    "            img_subdir = os.path.join(img_dir,\"boat24\", Name_Classes[name])\n",
    "            if not os.path.exists(img_subdir):\n",
    "                os.makedirs(img_subdir)\n",
    "            try:\n",
    "                img_request.urlretrieve(image,os.path.join(img_subdir,file_name))\n",
    "                row = sailboatdata_data[sailboatdata_data['name'] == name].iloc[0].to_frame().T\n",
    "                row['img_path'] = file_name\n",
    "                row['img_url'] = image\n",
    "                row['original_label'] = BeautifulSoup(boat24_data_raw['name'][i] , 'html.parser').get_text()\n",
    "                boat24_data_clean = pd.concat([boat24_data_clean , row])\n",
    "                counter += 1\n",
    "            except:\n",
    "                print(\"Error downloading image\",image)\n",
    "            if counter % 10 == 0:\n",
    "                save = True\n",
    "    already_scraped = pd.concat([already_scraped , pd.DataFrame([boat24_data_raw['url'][i]])])\n",
    "    if save:\n",
    "        boat24_data_clean.to_csv(os.path.join(data_dir,\"boat24_data_clean.csv\") , index=False)\n",
    "        already_scraped.to_csv(os.path.join(data_dir,\"already_scraped.csv\") , index=False , header=False)\n",
    "        save = False\n",
    "boat24_data_clean.to_csv(os.path.join(data_dir,\"boat24_data_clean.csv\") , index=False)\n",
    "already_scraped.to_csv(os.path.join(data_dir,\"already_scraped.csv\") , index=False , header=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boat_data_clean = pd.read_csv(data_dir+\"boat_data_clean.csv\")\n",
    "\n",
    "from icrawler.builtin import BingImageCrawler\n",
    "boat_data_clean = pd.read_csv(data_dir+\"boat_data_clean.csv\")\n",
    "boat_names = [Name_Classes[x] for x in boat_data_clean['name'].unique()]\n",
    "already_found = os.walk(os.path.join(os.getcwd(),img_dir,\"image_search\")).__next__()[1]\n",
    "for boat in boat_names:\n",
    "    if boat not in already_found:\n",
    "        path = os.path.join(img_dir,\"image_search\",boat)\n",
    "        if not(os.path.exists(path)):\n",
    "            print(\"Making dir\" , path)\n",
    "            makedirs(path)\n",
    "        google_Crawler = BingImageCrawler(storage = {'root_dir': path} , downloader_threads = 16)\n",
    "        google_Crawler.crawl(keyword = boat + \" sailboat\", max_num = 16 , filters = {\"type\":\"photo\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "sailboatdata_data = pd.read_csv(data_dir+\"boat_data_clean.csv\")\n",
    "label2id = {Name_Classes[x]:x for x in Name_Classes}\n",
    "image_search_data = pd.DataFrame(columns = sailboatdata_data.columns)\n",
    "for dir in tqdm(os.walk(os.path.join(os.getcwd(),img_dir,\"image_search\")).__next__()[1]):\n",
    "    rows = []\n",
    "    for file in os.listdir(os.path.join(os.getcwd(),img_dir,\"image_search\",dir)):\n",
    "        if file.endswith(\".jpg\"):\n",
    "            file_name = \"image_search\" + str(counter).zfill(6) + \".jpg\"\n",
    "            counter += 1\n",
    "            # os.rename(os.path.join(os.getcwd(),img_dir,\"image_search\",dir,file) , os.path.join(os.getcwd(),img_dir,\"image_search\",dir,file_name))\n",
    "            try:\n",
    "                row = sailboatdata_data[sailboatdata_data['name'] == label2id[dir]].iloc[0].to_frame().T\n",
    "            except:\n",
    "                row = sailboatdata_data[sailboatdata_data['name'] == label2id[dir+\".\"]].iloc[0].to_frame().T\n",
    "            row['img_path'] = file_name\n",
    "            rows += [row]\n",
    "    try:\n",
    "        image_search_data = pd.concat([image_search_data , pd.concat(rows)])\n",
    "    except:\n",
    "        print(\"Error with dir\",dir)\n",
    "image_search_data.to_csv(os.path.join(data_dir,\"image_search_data.csv\") , index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial HTML cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_cleaner_tag = re.compile(r'([\\d.]+)(\\s*m|\\s*kg)(<.*>)*')\n",
    "\n",
    "def clean_row(row):\n",
    "  cleaned_row = []\n",
    "  for i in range(len(row)):\n",
    "    if i != 32:\n",
    "      try:\n",
    "        cleaned_row.append(float(re.sub(units_cleaner_tag , r'\\g<1>' , row[i].replace(',','') , )))\n",
    "      except:\n",
    "        cleaned_row.append(row[i])\n",
    "    else:\n",
    "      cleaned_row.append(row[i])\n",
    "  return cleaned_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = []\n",
    "for i , row in data_raw.iterrows():\n",
    "  cleaned_data.append(clean_row(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = pd.DataFrame(cleaned_data , columns = data_raw.columns)\n",
    "data_clean.to_csv(data_dir+\"boat_data_clean.csv\" , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = pd.read_csv(data_dir+\"boat_data_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_clean_tag = re.compile(r'[^\\w\\s/&]')\n",
    "for column in data_clean.columns:\n",
    "    if (data_clean[column].dtype == \"object\" or data_clean[column].dtype == \"category\") and not column == \"Download Boat Record\" and not column == \"url\" and not column == \"img_path\":\n",
    "        data_clean[column] = data_clean[column].str.strip().str.rstrip('.').str.lower()\n",
    "        data_clean[column] = data_clean[column].str.replace(row_clean_tag , '')\n",
    "        # data_clean[column] = data_clean[column].astype('category')\n",
    "        # pd.DataFrame(data_clean[column].cat.categories).to_csv(data_dir+\"labels/\"+column.replace(\" \",\"_\")+\".txt\" , index = False , header = False)\n",
    "        # data_clean[column] = data_clean[column].cat.codes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average hash computation.\n",
    "# See:\n",
    "# https://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html\n",
    "def average_hasher(image):\n",
    "  # Read image.\n",
    "  img = PIL.Image.open(image)\n",
    "\n",
    "  # Simplify image by reducing its size and colors.\n",
    "  pixels = img.convert(\"L\").resize((8, 8), Image.ANTIALIAS).getdata()\n",
    "\n",
    "  # Get the average pixel value.\n",
    "  sum = 0.0\n",
    "  for p in pixels: sum += p\n",
    "  mean = sum / 64\n",
    "\n",
    "  # Generate the hash by comparing each pixel's value to the average.\n",
    "  bits = 0\n",
    "  for p in pixels:\n",
    "    bits <<= 1\n",
    "    if p > mean: bits |= 1\n",
    "\n",
    "  return \"%016x\" % bits, img.width, img.height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_images(dir , data , existing_hashes = []):\n",
    "    image_hashes = []\n",
    "    found_duplicates = 0\n",
    "    for i , row in tqdm(data.iterrows()):\n",
    "        try:\n",
    "            img_hash = average_hasher(os.path.join(img_dir , dir , data['img_path'][i] ))[0]\n",
    "        except:\n",
    "            try:\n",
    "                img_hash = average_hasher(os.path.join(img_dir , dir , Name_Classes[data['name'][i]] ,data['img_path'][i] ))[0]\n",
    "            except:\n",
    "                print(\"Error with \",os.path.join(img_dir , dir , Name_Classes[data['name'][i]] ,data['img_path'][i]))\n",
    "                img_hash = None\n",
    "        if img_hash not in existing_hashes:\n",
    "            image_hashes.append(img_hash)\n",
    "        else:\n",
    "            if found_duplicates <= 10:\n",
    "                print(\"Found duplicate\",img_hash)\n",
    "            elif found_duplicates < 10:\n",
    "                print(\"Found over 10 duplicates. Will not print more...\")\n",
    "            found_duplicates += 1\n",
    "            image_hashes.append(None)\n",
    "    print(\"Found\",found_duplicates,\"duplicates\")\n",
    "    return image_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_26372\\1406540995.py:9: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  pixels = img.convert(\"L\").resize((8, 8), Image.ANTIALIAS).getdata()\n",
      "120it [00:00, 586.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\6-METER (INTERNATIONAL)\\00040.jpg\n",
      "Error with  E:/data/images/sailboatdata\\ALC 35 MKII (LE COMTE)\\00147.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "410it [00:00, 522.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\ANDREWS 43\\00332.jpg\n",
      "Error with  E:/data/images/sailboatdata\\ARCHAMBAULT 40\\00402.jpg\n",
      "Error with  E:/data/images/sailboatdata\\ARCHAMBAULT 40\\00403.jpg\n",
      "Error with  E:/data/images/sailboatdata\\ARCONA 410\\00417.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "511it [00:01, 461.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\ASTRÉA 42\\00453.jpg\n",
      "Error with  E:/data/images/sailboatdata\\ATLANTA 31 (COLVIC)\\00471.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "664it [00:01, 442.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\BALT 26\\00595.jpg\n",
      "Error with  E:/data/images/sailboatdata\\BALT 26\\00596.jpg\n",
      "Error with  E:/data/images/sailboatdata\\BALT 27\\00597.jpg\n",
      "Error with  E:/data/images/sailboatdata\\BALT 27\\00598.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "889it [00:01, 537.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\BAYCRUISER 23\\00793.jpg\n",
      "Error with  E:/data/images/sailboatdata\\BAYCRUISER 23\\00794.jpg\n",
      "Error with  E:/data/images/sailboatdata\\BAYCRUISER 26\\00795.jpg\n",
      "Error with  E:/data/images/sailboatdata\\BEETLE 14\\00815.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1009it [00:01, 564.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\BIANCA 28\\00925.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1357it [00:02, 524.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\CAL 24\\01265.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1663it [00:03, 529.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\CATALINA 380\\01566.jpg\n",
      "Error with  E:/data/images/sailboatdata\\CHALLENGER 32\\01661.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1910it [00:03, 564.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\CNB 66\\01826.jpg\n",
      "Error with  E:/data/images/sailboatdata\\COLUMBIA 36\\01882.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2157it [00:04, 589.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\CONTEST 30 MK I\\02098.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2217it [00:04, 533.14it/s]c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "2396it [00:04, 542.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\CYCLADES 51.5 (BENETEAU)\\02342.jpg\n",
      "Error with  E:/data/images/sailboatdata\\CYCLADES 51.5 (BENETEAU)\\02343.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2620it [00:05, 519.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\DISCOVERY 32-2\\02548.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3031it [00:05, 589.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\ELAN GT5\\02917.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3329it [00:06, 554.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\F3\\03243.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3777it [00:07, 545.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\FLYING PHANTOM ELITE\\03665.jpg\n",
      "Error with  E:/data/images/sailboatdata\\FLYING PHANTOM ESSENTIAL\\03666.jpg\n",
      "Error with  E:/data/images/sailboatdata\\FLYING PHANTOM ESSENTIAL\\03667.jpg\n",
      "Error with  E:/data/images/sailboatdata\\FORELLE\\03691.jpg\n",
      "Error with  E:/data/images/sailboatdata\\FORELLE\\03692.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3885it [00:07, 510.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\FUN 23\\03806.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4120it [00:07, 525.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\GULF 29\\04041.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4469it [00:08, 451.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\HELMSMAN 47\\04380.jpg\n",
      "Error with  E:/data/images/sailboatdata\\HIGH TENSION 36\\04415.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4825it [00:09, 550.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\HYLAS 46\\04747.jpg\n",
      "Error with  E:/data/images/sailboatdata\\HYLAS 46\\04748.jpg\n",
      "Error with  E:/data/images/sailboatdata\\HYLAS 48\\04750.jpg\n",
      "Error with  E:/data/images/sailboatdata\\HYLAS 48\\04751.jpg\n",
      "Error with  E:/data/images/sailboatdata\\HYLAS 63\\04762.jpg\n",
      "Error with  E:/data/images/sailboatdata\\HYLAS 63\\04763.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5131it [00:09, 552.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\ITALIA 13.98\\05035.jpg\n",
      "Error with  E:/data/images/sailboatdata\\JAVELIN 14 (FOX)\\05060.jpg\n",
      "Error with  E:/data/images/sailboatdata\\JEANNEAU YACHTS 58\\05080.jpg\n",
      "Error with  E:/data/images/sailboatdata\\JEANNEAU YACHTS 58\\05081.jpg\n",
      "Error with  E:/data/images/sailboatdata\\JOHNSON WEEKENDER 18\\05104.jpg\n",
      "Error with  E:/data/images/sailboatdata\\JOUËT 1300\\05123.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5352it [00:10, 434.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\LADY SLIPPER\\05319.jpg\n",
      "Error with  E:/data/images/sailboatdata\\LANAVERRE 630 H\\05371.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5463it [00:10, 492.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\LARSEN 232\\05407.jpg\n",
      "Error with  E:/data/images/sailboatdata\\LEOPARD 47\\05475.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5850it [00:11, 530.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\MARINER 19 CB\\05802.jpg\n",
      "Error with  E:/data/images/sailboatdata\\MARINER 19 CB\\05803.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6213it [00:12, 565.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\MONHEGAN 48\\06098.jpg\n",
      "Error with  E:/data/images/sailboatdata\\MONHEGAN 48\\06099.jpg\n",
      "Error with  E:/data/images/sailboatdata\\MOORINGS 4700\\06188.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6498it [00:12, 412.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\NAVER 29\\06465.jpg\n",
      "Error with  E:/data/images/sailboatdata\\NEPTUN 210\\06483.jpg\n",
      "Error with  E:/data/images/sailboatdata\\NEPTUN 210\\06484.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6799it [00:13, 468.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\NORTH WIND 435\\06735.jpg\n",
      "Error with  E:/data/images/sailboatdata\\NORTHEAST 38-1\\06739.jpg\n",
      "Error with  E:/data/images/sailboatdata\\NOVA 40\\06767.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7019it [00:13, 507.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\OLYMPIC ADVENTURE 47\\06959.jpg\n",
      "Error with  E:/data/images/sailboatdata\\ORCA 43\\07005.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7362it [00:14, 489.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\PEDRICK 43 (CHEOY LEE)\\07313.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7669it [00:14, 585.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\PUMA 38-K\\07566.jpg\n",
      "Error with  E:/data/images/sailboatdata\\PUMA 38-S\\07567.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7909it [00:15, 550.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\RUSTLER 24\\07838.jpg\n",
      "Error with  E:/data/images/sailboatdata\\RUSTLER 37\\07844.jpg\n",
      "Error with  E:/data/images/sailboatdata\\RUSTLER 37\\07845.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8320it [00:16, 561.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\SEADOG (SKIMMAR)\\08224.jpg\n",
      "Error with  E:/data/images/sailboatdata\\SEAWIND (ALLIED)\\08316.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8499it [00:16, 470.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\SIGMA 36 (THOMAS)\\08448.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8732it [00:16, 527.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\SOUTHERLY 49\\08668.jpg\n",
      "Error with  E:/data/images/sailboatdata\\SOUTHERLY 49\\08669.jpg\n",
      "Error with  E:/data/images/sailboatdata\\SPENCER 1330\\08744.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9087it [00:17, 568.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\SUN ODYSSEY 319 (JEANNEAU)\\08978.jpg\n",
      "Error with  E:/data/images/sailboatdata\\SUN ODYSSEY 449 (JEANNEAU)\\09041.jpg\n",
      "Error with  E:/data/images/sailboatdata\\SUN ODYSSEY 449 (JEANNEAU)\\09042.jpg\n",
      "Error with  E:/data/images/sailboatdata\\SUN ODYSSEY 519 (JEANNEAU)\\09064.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9308it [00:18, 510.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\SWAN 41\\09222.jpg\n",
      "Error with  E:/data/images/sailboatdata\\SWAN 41\\09223.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9577it [00:18, 491.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\TEMPEST (INTERNATIONAL)\\09502.jpg\n",
      "Error with  E:/data/images/sailboatdata\\TINA (CARTER)\\09558.jpg\n",
      "Error with  E:/data/images/sailboatdata\\TOMCAT 6.2\\09587.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9736it [00:18, 492.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/sailboatdata\\TRINTELLA 38\\09667.jpg\n",
      "Error with  E:/data/images/sailboatdata\\TRIPP 40 MH\\09700.jpg\n",
      "Error with  E:/data/images/sailboatdata\\TRISBAL 36\\09704.jpg\n",
      "Error with  E:/data/images/sailboatdata\\TUR 84\\09738.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10495it [00:20, 510.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique hashes: 10130\n"
     ]
    }
   ],
   "source": [
    "sailboatdata_data = pd.read_csv(data_dir+\"boat_data_clean.csv\")\n",
    "sailboatdata_data['image_hash'] = hash_images(\"sailboatdata\" , sailboatdata_data)\n",
    "# Remove duplicate hashes.\n",
    "sailboatdata_data = sailboatdata_data.drop_duplicates(subset=['image_hash'])\n",
    "print(\"Number of unique hashes:\",len(sailboatdata_data))\n",
    "sailboatdata_data.to_csv(data_dir+\"sailboatdata_data_hashed.csv\" , index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_26372\\1406540995.py:9: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  pixels = img.convert(\"L\").resize((8, 8), Image.ANTIALIAS).getdata()\n",
      "9969it [01:29, 117.61it/s]"
     ]
    }
   ],
   "source": [
    "boat24_data = pd.read_csv(data_dir+\"boat24_data_clean.csv\")\n",
    "boat24_data['image_hash'] = hash_images(\"boat24\" , boat24_data)\n",
    "# Remove duplicate hashes.\n",
    "boat24_data = boat24_data.drop_duplicates(subset=['image_hash'])\n",
    "print(\"Number of unique hashes:\",len(boat24_data))\n",
    "boat24_data.to_csv(data_dir+\"boat24_data_hashed.csv\" , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_26372\\1406540995.py:9: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  pixels = img.convert(\"L\").resize((8, 8), Image.ANTIALIAS).getdata()\n",
      "24it [00:00, 86.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found duplicate fff7f3e3e7c00000\n",
      "Found duplicate 8f8f8f8f8f0cf000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:00, 85.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found duplicate ff40000070f2fc78\n",
      "Found duplicate 000048f8fcfcffff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [00:00, 135.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found duplicate ffffffffef401800\n",
      "Found duplicate 0f3f7feff73f0c00\n",
      "Found duplicate ff40000070f2fc78\n",
      "Found duplicate 30100e393f1f0600\n",
      "Found duplicate ffffffce0020f800\n",
      "Found duplicate 03001b1b38ff3e00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1583it [00:11, 204.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/image_search\\ALC 35 MKII (LE COMTE)\\image_search001548.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12535it [01:58, 86.30it/s] c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "19289it [03:29, 184.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 26\\image_search019248.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 26\\image_search019249.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 26\\image_search019250.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 30\\image_search019251.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 30\\image_search019252.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 30\\image_search019253.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 30\\image_search019254.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 30\\image_search019255.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 30\\image_search019256.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 30\\image_search019257.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 30\\image_search019258.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 30\\image_search019259.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 30\\image_search019260.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 30\\image_search019261.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 30\\image_search019262.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 30\\image_search019263.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 30\\image_search019264.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 30\\image_search019265.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 30\\image_search019266.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 32\\image_search019267.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 32\\image_search019268.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 32\\image_search019269.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 32\\image_search019270.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 32\\image_search019271.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 32\\image_search019272.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 32\\image_search019273.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 32\\image_search019274.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 32\\image_search019275.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 32\\image_search019276.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 32\\image_search019277.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 32\\image_search019278.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 32\\image_search019279.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 32\\image_search019280.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 32\\image_search019281.jpg\n",
      "Error with  E:/data/images/image_search\\CLIPPER MARINE 32\\image_search019282.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31232it [05:37, 152.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/image_search\\ELAN 362\\image_search031195.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 362\\image_search031196.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 362\\image_search031197.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 362\\image_search031198.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 362\\image_search031199.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 362\\image_search031200.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 362\\image_search031201.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 362\\image_search031202.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 362\\image_search031203.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 362\\image_search031204.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 362\\image_search031205.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 362\\image_search031206.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 362\\image_search031207.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 362\\image_search031208.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 37\\image_search031209.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 37\\image_search031210.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 37\\image_search031211.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 37\\image_search031212.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 37\\image_search031213.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 37\\image_search031214.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 37\\image_search031215.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 37\\image_search031216.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 37\\image_search031217.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 37\\image_search031218.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 37\\image_search031219.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 37\\image_search031220.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 37\\image_search031221.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 37\\image_search031222.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 37\\image_search031223.jpg\n",
      "Error with  E:/data/images/image_search\\ELAN 37\\image_search031224.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87824it [15:50, 85.11it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/image_search\\SEA EAGLE II (ENDERLEIN)\\image_search087819.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "94166it [16:54, 95.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  E:/data/images/image_search\\SPENCER 35 MKII\\image_search094142.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113282it [20:20, 92.80it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique hashes: 82141\n"
     ]
    }
   ],
   "source": [
    "image_search_data = pd.read_csv(data_dir+\"image_search_data.csv\")\n",
    "image_search_data['image_hash'] = hash_images(\"image_search\" , image_search_data , sailboatdata_data['image_hash'].tolist())\n",
    "# Remove duplicate hashes.\n",
    "image_search_data = image_search_data.drop_duplicates(subset=['image_hash'])\n",
    "print(\"Number of unique hashes:\",len(image_search_data))\n",
    "image_search_data.to_csv(data_dir+\"image_search_data_hashed.csv\" , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train , data_test = train_test_split(data_clean , test_size = 0.2 , random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"data/train/\"\n",
    "test_dir = \"data/test/\"\n",
    "\n",
    "if not(os.path.exists(train_dir)):\n",
    "    print(\"Making dir\" , train_dir)\n",
    "    makedirs(train_dir)\n",
    "\n",
    "if not(os.path.exists(test_dir)):\n",
    "    print(\"Making dir\" , test_dir)\n",
    "    makedirs(test_dir)\n",
    "\n",
    "\n",
    "for image in data_train['img_path']:\n",
    "    try:\n",
    "        img = Image.open(img_dir+image)\n",
    "        img = img.convert('RGB')\n",
    "        img.save(train_dir+image)\n",
    "    except:\n",
    "        print(\"Image not found\" , image)\n",
    "        data_train = data_train[data_train['img_path'] != image]\n",
    "\n",
    "for image in data_test['img_path']:\n",
    "    try:\n",
    "        img = Image.open(img_dir+image)\n",
    "        img = img.convert('RGB')\n",
    "        img.save(test_dir+image)\n",
    "    except:\n",
    "        print(\"Image not found\" , image)\n",
    "        data_test = data_test[data_test['img_path'] != image]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.to_csv(data_dir+\"boat_data_train.csv\" , index = False)\n",
    "data_test.to_csv(data_dir+\"boat_data_test.csv\" , index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = pd.DataFrame(cleaned_data , columns = data_raw.columns)\n",
    "\n",
    "col_list = ['Hull Type' , 'Rigging Type' , 'Construction' , 'Ballast Type' , 'Designer' , 'name']\n",
    "\n",
    "for column in data_clean.columns:\n",
    "    if column in col_list:\n",
    "        path = data_dir+\"/labels/\"+column.replace(\" \",\"_\")\n",
    "        data_clean[column] = data_clean[column].astype('category')\n",
    "        classes = dict( zip(data_clean[column] , data_clean[column].cat.codes))\n",
    "        with open(path+\".json\", \"w\") as outfile:\n",
    "            json.dump(classes, outfile , indent = 4)\n",
    "        data_clean[column] = data_clean[column].cat.codes\n",
    "\n",
    "data_clean.to_csv(data_dir+\"boat_data_clean.csv\" , index = False)\n",
    "data_train , data_test = train_test_split(data_clean , test_size = 0.2 , random_state = 42)\n",
    "data_train.to_csv(data_dir+\"boat_data_train.csv\" , index = False)\n",
    "data_test.to_csv(data_dir+\"boat_data_test.csv\" , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"images/00002.jpg\"\n",
    "# df = data_train\n",
    "# idx = df.index[(df['img_path'] == file_path.split(\"/\")[1])].values[0]\n",
    "\n",
    "# print(\n",
    "# Hull_Type_Classes[df['Hull Type'][idx]],[df['Hull Type'][idx]],\"\\n\"+\n",
    "# Rigging_Type_Classes[df['Rigging Type'][idx]],[df['Rigging Type'][idx]],\"\\n\"+\n",
    "# Construction_Classes[df['Construction'][idx]],[df['Construction'][idx]],\"\\n\"+\n",
    "# Ballast_Type_Classes[df['Ballast Type'][idx]],[df['Ballast Type'][idx]],\"\\n\"+\n",
    "# Designer_Classes[df['Designer'][idx]],[df['Designer'][idx]],\"\\n\"+\n",
    "# Name_Classes[df['name'][idx]],[df['name'][idx]],\"\\n\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Hull Type' , 'Rigging Type' , 'Construction' , 'Ballast Type' , 'Designer' , 'name']\n",
    "for label in labels:\n",
    "    sailboatdata_data = pd.read_csv(\"data/boat_data_clean.csv\")\n",
    "    sailboatdata_data = sailboatdata_data.dropna(subset=[label])\n",
    "    # drop labels with less than 2 entries\n",
    "    \n",
    "    sailboatdata_data = sailboatdata_data.groupby(label).filter(lambda x: len(x) > 1)\n",
    "    sailboatdata_data = sailboatdata_data.drop_duplicates(subset=[label])\n",
    "    print(label,sailboatdata_data.__len__())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "edf259275ad4a72d4dd5b452264ad5fb2b635233dff2a31edc6ebc740e55e21b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
