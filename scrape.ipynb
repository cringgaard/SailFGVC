{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import urllib.request as img_request\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json \n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from os import makedirs\n",
    "from os import listdir\n",
    "from os import getcwd\n",
    "import os\n",
    "from data.classes import *\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from Labeller import Labeller\n",
    "import PIL\n",
    "import torchvision\n",
    "import glob\n",
    "import torch\n",
    "import io\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining important directories as variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = getcwd()\n",
    "data_dir = \"data/\"\n",
    "img_dir = \"E:/data/images/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining regex tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (os.path.exists(data_dir)):\n",
    "    print(\"Making dir\" , data_dir)\n",
    "    makedirs(data_dir)\n",
    "\n",
    "if not(os.path.exists(img_dir)):\n",
    "    print(\"Making dir\" , img_dir)\n",
    "    makedirs(img_dir)\n",
    "\n",
    "if not(os.path.exists(img_dir+\"/boat24\")):\n",
    "    print(\"Making dir\" , img_dir+\"/boat24\")\n",
    "    makedirs(img_dir+\"/boat24\")\n",
    "\n",
    "if not(os.path.exists(img_dir+\"/image_search\")):\n",
    "    print(\"Making dir\" , img_dir+\"/image_search\")\n",
    "    makedirs(img_dir+\"/image_search\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sailboatdata.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pages_tag = re.compile(r'<a href=.*page=(\\d*).*/li>') # For getting the amount of pages on the domain\n",
    "\n",
    "name_tag = re.compile(r'<a href=\\\"https://sailboatdata\\.com/sailboat/.*\\\">(.*)</a>') # For making a list of all the boats on sailboatdata\n",
    "\n",
    "specs_tag = re.compile(r'<div class=\\\" col-\\w\\w-\\d*  col-\\w\\w-6 sailboatdata-label \\\">\\s*(.*):\\s</div>\\s<.*\\s*(.*)') # For scraping data from specific boat url\n",
    "\n",
    "image_tag = re.compile(r'(?:photo|drawing)\\\".*src=\\\"(http.*)\\\"/>')\n",
    "\n",
    "photo_draw_tag = re.compile(r'sailboat/(\\w*)')\n",
    "\n",
    "name_description_tag = re.compile(r'title\" content=\"(.+)\"[^½]+?description\" content=\"(.+)\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs = []\n",
    "categories = []\n",
    "boat_data = pd.DataFrame()\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_url = 'https://sailboatdata.com/sailboat?page={}&paginate=25'\n",
    "\n",
    "print(\"Connecting to\" , general_url.format(1))\n",
    "init_request = requests.get(general_url.format(1))\n",
    "print(init_request.status_code)\n",
    "pages = re.findall(n_pages_tag , init_request.text)\n",
    "n_pages = pages[-1]\n",
    "\n",
    "boat_types = []\n",
    "for i in tqdm(range (1,int(n_pages)+1)):\n",
    "  # print(\"Connecting to\" , general_url.format(i))\n",
    "  r = requests.get(general_url.format(i))\n",
    "  # print(r.status_code)\n",
    "  boat_types += re.findall(name_tag,r.text)\n",
    "  time.sleep(0.5) #To not throw too many requests at the website\n",
    "\n",
    "print(len(boat_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for boat in boat_types:\n",
    "    # GETTNG REQUEST\n",
    "    print(\"Scraping\",boat,\"...\")\n",
    "    r = requests.get(\"https://sailboatdata.com/sailboat/{}?units=metric\".format(boat.replace(\" \",\"-\").replace(\"(\",\"\").replace(\")\",\"\").replace(\".\",\"\")))\n",
    "    raw_text = r.text\n",
    "\n",
    "    # SCRAPING IMAGES\n",
    "    images = re.findall(image_tag,raw_text)\n",
    "    print(\"         -------scraping\")\n",
    "    for image in images:\n",
    "        img_path = str(counter).zfill(5)+\".jpg\"\n",
    "        if not os.path.isfile(img_path):\n",
    "            counter += 1\n",
    "            print(image)\n",
    "            try:\n",
    "                img_request.urlretrieve(image,os.path.join(img_dir,'sailboatdata')+img_path)\n",
    "            except:\n",
    "                print(\"Strange url\" , image.replace(\" \",\"%20\"))\n",
    "                try:\n",
    "                    img_request.urlretrieve(image,img_dir+img_path)\n",
    "                except:\n",
    "                    has_image = False\n",
    "\n",
    "            # SCRAPING SPECS\n",
    "            categories_specs = re.findall(specs_tag,raw_text)\n",
    "            specs = [el[1] for el in categories_specs]\n",
    "            categories = [el[0] for el in categories_specs]\n",
    "            new_row = {categories[i]: specs[i] for i in range(len(categories))}\n",
    "            new_row['name'] = boat\n",
    "            new_row['img_path'] = img_path\n",
    "            boat_data = pd.concat([boat_data, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Done\\n\")\n",
    "    if counter%100 == 1:\n",
    "        print(\"saving\")\n",
    "        boat_data.to_csv(data_dir+\"boat_data.csv\" , index=False)\n",
    "boat_data.to_csv(data_dir+\"boat_data.csv\" , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "data_raw = pd.read_csv(data_dir+\"boat_data.csv\")\n",
    "data_raw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## boat24.com"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pages_tag = re.compile(r'\\?page=(\\d{3,})') # For getting the amount of pages on the domain\n",
    "\n",
    "# name_and_url_tag = re.compile(r'__title\\\"><a href=\\\"(https://www\\.boat24\\.com.+)\\\"\\stitle=\\\"(.+?)\\\"\\s>')\n",
    "\n",
    "# image_tag = re.compile(r'srcset=\\\"(https://[^\\s]+?\\.jpg) \\d[^2]')\n",
    "\n",
    "# name_tag = re.compile(r'\\\"blurb__title\\\"><a.+?title=\\\"(.+?)\\\"\\s*')\n",
    "# image_tag=re.compile(r'<div class=\\\"blurb__image-area\\\".+?srcset=\\\"(https://static\\.b24\\.co/fotos.+?\\.jpg)|(https://www\\.boat24\\.com/img/icons/boat-types/mynd/type-1-230\\.png)')\n",
    "# url_tag = re.compile(r'<a href=\\\"(https://www\\.boat24\\.com/en/sailingboats/.+)')\n",
    "url_name_image_tag = re.compile(r'data-link=\\\"(https://www\\.boat24\\.com/en/sailingboats/.{1,100}?\\d\\/)\\\"[^¤]*?title=\\\"(.{1,40})\\\"><div class=\\\"blurb__image-area\\\".+?srcset=\\\"(https://static\\.b24\\.co/fotos.+?\\.jpg)')\n",
    "image_tag = re.compile(r'src=\\\"(https://static\\.b24\\.co/fotos.+?\\.jpg)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(url , tag):\n",
    "    r = requests.get(url)\n",
    "    retval = re.findall(tag , r.text)\n",
    "    return list(zip(*retval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sailboatdata_data = pd.read_csv(os.path.join(data_dir, \"boat_data_clean.csv\"))\n",
    "# try:\n",
    "#     boat24_data_raw = pd.read_csv(os.path.join(data_dir, \"boat24_data_raw.csv\"))\n",
    "# except:\n",
    "#     boat24_data_raw = pd.DataFrame(columns = [\"url\",\"name\",\"img_url , html\"])\n",
    "# class Labeller:\n",
    "#     def __init__(self , boat_names = Name_Classes.values(), embedding_size = 2**8):\n",
    "#         self.boat_names = boat_names\n",
    "#         self.onehot_map = {}\n",
    "#         self.embedding_size = embedding_size\n",
    "#         self.str2label = {v: k for k, v in Name_Classes.items()}\n",
    "#         for boat in boat_names:\n",
    "#             self.onehot_encode(boat)\n",
    "#         self.label2embedding = {k: self.onehot_encode(v) for k, v in sorted(Name_Classes.items())}\n",
    "\n",
    "#     def onehot_encode(self, boat_name):\n",
    "#         retval = np.zeros(self.embedding_size)\n",
    "#         for char in boat_name.upper():\n",
    "#             try:\n",
    "#                 retval += self.onehot_map[char]\n",
    "#             except:\n",
    "#                 embedding = np.zeros(2**8)\n",
    "#                 embedding[len(self.onehot_map.keys())] = 1\n",
    "#                 self.onehot_map[char] = embedding\n",
    "#                 retval += self.onehot_map[char]\n",
    "#         return retval\n",
    "    \n",
    "#     def add_label(self , boat_name , distance_threshold = 2):\n",
    "#         try:\n",
    "#             return True , self.str2label[boat_name.upper()] , 0\n",
    "#         except:\n",
    "#             embedding = self.onehot_encode(boat_name)\n",
    "#             distances = np.linalg.norm(embedding - np.array(list(self.label2embedding.values())), axis=1)\n",
    "#             return_label = np.argmin(distances)\n",
    "#             if distances[return_label] < distance_threshold:\n",
    "#                 # print(boat_name+\" not found adding label\" , Name_Classes[int(return_label)]+\" with the distance\" , distances[return_label])\n",
    "#                 return True , return_label , distances[return_label]\n",
    "#             else:\n",
    "#                 # print(boat_name+\" not found closest was\" , Name_Classes[int(return_label)]+\" with the distance\" , distances[return_label])\n",
    "#                 return False , return_label , distances[return_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_test , names_test , images_test = scrape(\"https://www.boat24.com/en/sailingboats/?page=1\" , url_name_image_tag)\n",
    "print(urls_test)\n",
    "print(names_test)\n",
    "print(images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boat24_data_raw = pd.read_csv('data/boat24_data_raw.csv')\n",
    "sailboatdata_data = pd.read_csv('data/boat_data_clean.csv')\n",
    "boat_names = [Name_Classes[x] for x in sailboatdata_data['name'].unique()]\n",
    "\n",
    "try:\n",
    "    already_scraped = pd.read_csv(os.path.join(data_dir, \"already_scraped.csv\") , header=None)\n",
    "except:\n",
    "    already_scraped = pd.DataFrame(['dummy_url'])\n",
    "try:\n",
    "    boat24_data_clean = pd.read_csv(os.path.join(data_dir, \"boat24_data_clean.csv\"))\n",
    "except:\n",
    "    boat24_data_clean = pd.DataFrame(columns = sailboatdata_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_request = requests.get(\"https://www.boat24.com/en/sailingboats/?page=0\")\n",
    "n_pages = re.findall(n_pages_tag , init_request.text)\n",
    "print(int(n_pages[0]))\n",
    "\n",
    "urls = []\n",
    "names = []\n",
    "images = []\n",
    "html = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(0,int(n_pages[0])+1,20)):\n",
    "    url_list , name_list , image_list = scrape(\"https://www.boat24.com/en/sailingboats/?page={}\".format(i) , url_name_image_tag)\n",
    "    urls += url_list\n",
    "    names += name_list\n",
    "    images += image_list\n",
    "    html += [requests.get(url).text for url in url_list]\n",
    "\n",
    "    time.sleep(0.1) #To not throw too many requests at the website\n",
    "boat24_data_raw = pd.concat([boat24_data_raw , pd.DataFrame({\"url\":urls , \"name\":names , \"img_url\":images , \"html\":html})])\n",
    "boat24_data_raw.drop_duplicates(subset=['url'] , keep='first')\n",
    "boat24_data_raw['name'] = boat24_data_raw['name'].str.upper()\n",
    "boat24_data_raw.to_csv(data_dir+\"boat24_data_raw.csv\" , index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could use UrlListCrawler with:\n",
    "# from icrawler.builtin import UrlListCrawler to download using more threads\n",
    "# see https://icrawler.readthedocs.io/en/latest/builtin.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boat24_data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boat24_data_raw = pd.read_csv(os.path.join(data_dir, \"boat24_data_raw.csv\"))\n",
    "sailboatdata_data = pd.read_csv(os.path.join(data_dir, \"boat_data_clean.csv\"))\n",
    "\n",
    "boat_names = [Name_Classes[x] for x in sailboatdata_data['name'].unique()]\n",
    "\n",
    "labeller = Labeller(boat_names)\n",
    "counter = 10\n",
    "for path,subdirs,files in os.walk(os.path.join(os.getcwd(),img_dir,\"boat24\")):\n",
    "    for file in files:\n",
    "        counter += 1\n",
    "save = False\n",
    "print(\"Starting counter at\" , counter)\n",
    "\n",
    "for i in tqdm(range(len(boat24_data_raw))):\n",
    "    found , name , distance = labeller.add_label([BeautifulSoup(boat24_data_raw['name'][i].lower() , 'html.parser').get_text()])\n",
    "    if found and not (boat24_data_raw['url'][i] == already_scraped[0]).any():\n",
    "        images = re.findall(image_tag , boat24_data_raw['html'][i])\n",
    "        print('Setting' , BeautifulSoup(boat24_data_raw['name'][i] , 'html.parser').get_text() , 'to' , Name_Classes[name] , 'with distance' , distance)\n",
    "        for image in images:\n",
    "            file_name = \"boat24_\"+str(counter).zfill(6)+\".jpg\"\n",
    "            img_subdir = os.path.join(img_dir,\"boat24\", Name_Classes[name])\n",
    "            if not os.path.exists(img_subdir):\n",
    "                os.makedirs(img_subdir)\n",
    "            try:\n",
    "                img_request.urlretrieve(image,os.path.join(img_subdir,file_name))\n",
    "                row = sailboatdata_data[sailboatdata_data['name'] == name].iloc[0].to_frame().T\n",
    "                row['img_path'] = file_name\n",
    "                row['img_url'] = image\n",
    "                row['image_hash'] = None\n",
    "                row['original_label'] = BeautifulSoup(boat24_data_raw['name'][i] , 'html.parser').get_text()\n",
    "                boat24_data_clean = pd.concat([boat24_data_clean , row])\n",
    "                counter += 1\n",
    "            except:\n",
    "                print(\"Error downloading image\",image)\n",
    "            if counter % 10 == 0:\n",
    "                save = True\n",
    "    already_scraped = pd.concat([already_scraped , pd.DataFrame([boat24_data_raw['url'][i]])])\n",
    "    if save:\n",
    "        boat24_data_clean.to_csv(os.path.join(data_dir,\"boat24_data_clean.csv\") , index=False)\n",
    "        already_scraped.to_csv(os.path.join(data_dir,\"already_scraped.csv\") , index=False , header=False)\n",
    "        save = False\n",
    "boat24_data_clean.to_csv(os.path.join(data_dir,\"boat24_data_clean.csv\") , index=False)\n",
    "already_scraped.to_csv(os.path.join(data_dir,\"already_scraped.csv\") , index=False , header=False)\n",
    "\n",
    "print(\"Checking if boat24_data_clean only contains uniques : \" , boat24_data_clean.value_counts('img_path').sum() == len(boat24_data_clean))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boat_data_clean = pd.read_csv(data_dir+\"sailboatdata_data_hashed_cleaned.csv\")\n",
    "from icrawler.builtin import BingImageCrawler\n",
    "\n",
    "boat_names = [Name_Classes[x] for x in boat_data_clean['name'].unique()]\n",
    "already_found = os.walk(os.path.join(os.getcwd(),img_dir,\"image_search_50\")).__next__()[1]\n",
    "for boat in boat_names:\n",
    "    if boat not in already_found:\n",
    "        path = os.path.join(img_dir,\"image_search_50\",boat)\n",
    "        if not(os.path.exists(path)):\n",
    "            print(\"Making dir\" , path)\n",
    "            makedirs(path)\n",
    "        bing_Crawler = BingImageCrawler(storage = {'root_dir': path} , downloader_threads = 16)\n",
    "        bing_Crawler.crawl(keyword = boat + \" sailboat\", max_num = 50 , filters = {\"type\":\"photo\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = 0\n",
    "sailboatdata_data = pd.read_csv(data_dir+\"boat_data_clean.csv\")\n",
    "label2id = {Name_Classes[x]:x for x in Name_Classes}\n",
    "image_search_data = pd.DataFrame(columns = sailboatdata_data.columns)\n",
    "for dir in tqdm(os.walk(os.path.join(os.getcwd(),img_dir,\"image_search\")).__next__()[1]):\n",
    "    rows = []\n",
    "    for file in os.listdir(os.path.join(os.getcwd(),img_dir,\"image_search\",dir)):\n",
    "        if file.endswith(\".jpg\"):\n",
    "            # file_name = \"image_search\" + str(counter).zfill(6) + \".jpg\"\n",
    "            # counter += 1\n",
    "            # os.rename(os.path.join(os.getcwd(),img_dir,\"image_search\",dir,file) , os.path.join(os.getcwd(),img_dir,\"image_search\",dir,file_name))\n",
    "            try:\n",
    "                row = sailboatdata_data[sailboatdata_data['name'] == label2id[dir]].iloc[0].to_frame().T\n",
    "            except:\n",
    "                row = sailboatdata_data[sailboatdata_data['name'] == label2id[dir+\".\"]].iloc[0].to_frame().T\n",
    "            row['img_path'] = dir+\"_\"+file\n",
    "            rows += [row]\n",
    "    try:\n",
    "        image_search_data = pd.concat([image_search_data , pd.concat(rows)])\n",
    "    except:\n",
    "        print(\"Error with dir\",dir)\n",
    "image_search_data.to_csv(os.path.join(data_dir,\"image_search_data_50.csv\") , index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial HTML cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_cleaner_tag = re.compile(r'([\\d.]+)(\\s*m|\\s*kg)(<.*>)*')\n",
    "\n",
    "def clean_row(row):\n",
    "  cleaned_row = []\n",
    "  for i in range(len(row)):\n",
    "    if i != 32:\n",
    "      try:\n",
    "        cleaned_row.append(float(re.sub(units_cleaner_tag , r'\\g<1>' , row[i].replace(',','') , )))\n",
    "      except:\n",
    "        cleaned_row.append(row[i])\n",
    "    else:\n",
    "      cleaned_row.append(row[i])\n",
    "  return cleaned_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = []\n",
    "for i , row in data_raw.iterrows():\n",
    "  cleaned_data.append(clean_row(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = pd.DataFrame(cleaned_data , columns = data_raw.columns)\n",
    "data_clean.to_csv(data_dir+\"boat_data_clean.csv\" , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = pd.read_csv(data_dir+\"boat_data_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_clean_tag = re.compile(r'[^\\w\\s/&]')\n",
    "for column in data_clean.columns:\n",
    "    if (data_clean[column].dtype == \"object\" or data_clean[column].dtype == \"category\") and not column == \"Download Boat Record\" and not column == \"url\" and not column == \"img_path\":\n",
    "        data_clean[column] = data_clean[column].str.strip().str.rstrip('.').str.lower()\n",
    "        data_clean[column] = data_clean[column].str.replace(row_clean_tag , '')\n",
    "        # data_clean[column] = data_clean[column].astype('category')\n",
    "        # pd.DataFrame(data_clean[column].cat.categories).to_csv(data_dir+\"labels/\"+column.replace(\" \",\"_\")+\".txt\" , index = False , header = False)\n",
    "        # data_clean[column] = data_clean[column].cat.codes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average hash computation.\n",
    "# See:\n",
    "# https://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html\n",
    "def average_hasher(image):\n",
    "  # Read image.\n",
    "  img = PIL.Image.open(image)\n",
    "\n",
    "  # Simplify image by reducing its size and colors.\n",
    "  pixels = img.convert(\"L\").resize((8, 8), Image.ANTIALIAS).getdata()\n",
    "\n",
    "  # Get the average pixel value.\n",
    "  sum = 0.0\n",
    "  for p in pixels: sum += p\n",
    "  mean = sum / 64\n",
    "\n",
    "  # Generate the hash by comparing each pixel's value to the average.\n",
    "  bits = 0\n",
    "  for p in pixels:\n",
    "    bits <<= 1\n",
    "    if p > mean: bits |= 1\n",
    "\n",
    "  return \"%016x\" % bits, img.width, img.height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_images(dir , data , existing_hashes = []):\n",
    "    image_hashes = []\n",
    "    found_duplicates = 0\n",
    "    for i , row in tqdm(data.iterrows()):\n",
    "        try:\n",
    "            img_hash = average_hasher(os.path.join(img_dir , dir , data['img_path'][i] ))[0]\n",
    "        except:\n",
    "            try:\n",
    "                img_hash = average_hasher(os.path.join(img_dir , dir , Name_Classes[data['name'][i]] ,data['img_path'][i] ))[0]\n",
    "            except:\n",
    "                print(\"Error with \",os.path.join(img_dir , dir , Name_Classes[data['name'][i]] ,data['img_path'][i]))\n",
    "                img_hash = None\n",
    "        if img_hash not in existing_hashes:\n",
    "            image_hashes.append(img_hash)\n",
    "        else:\n",
    "            if found_duplicates <= 10:\n",
    "                print(\"Found duplicate\",img_hash)\n",
    "            elif found_duplicates < 10:\n",
    "                print(\"Found over 10 duplicates. Will not print more...\")\n",
    "            found_duplicates += 1\n",
    "            image_hashes.append(None)\n",
    "    print(\"Found\",found_duplicates,\"duplicates\")\n",
    "    return image_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sailboatdata_data = pd.read_csv(data_dir+\"boat_data_clean.csv\")\n",
    "sailboatdata_data['image_hash'] = hash_images(\"sailboatdata\" , sailboatdata_data)\n",
    "# Remove duplicate hashes.\n",
    "sailboatdata_data = sailboatdata_data.dropna(subset=['image_hash'])\n",
    "print(\"Number of unique hashes:\",len(sailboatdata_data))\n",
    "sailboatdata_data.to_csv(data_dir+\"sailboatdata_data_hashed.csv\" , index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boat24_data = pd.read_csv(data_dir+\"boat24_data_clean.csv\")\n",
    "boat24_data['image_hash'] = hash_images(\"boat24\" , boat24_data)\n",
    "# Remove duplicate hashes.\n",
    "boat24_data = boat24_data.dropna(subset=['image_hash'])\n",
    "print(\"Number of unique hashes:\",len(boat24_data))\n",
    "boat24_data.to_csv(data_dir+\"boat24_data_hashed.csv\" , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_search_data = pd.read_csv(data_dir+\"image_search_data.csv\")\n",
    "image_search_data['image_hash'] = hash_images(\"image_search\" , image_search_data , sailboatdata_data['image_hash'].tolist())\n",
    "# Remove duplicate hashes.\n",
    "image_search_data = image_search_data.dropna(subset=['image_hash'])\n",
    "print(\"Number of unique hashes:\",len(image_search_data))\n",
    "image_search_data.to_csv(data_dir+\"image_search_data_hashed.csv\" , index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Grouping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_construction(data):\n",
    "    labels = []\n",
    "    for i in range(len(data)):\n",
    "        label = Construction_Classes[data['Construction'][i]].lower().strip().replace(\".\",\" \").replace(\"/\",\" \").replace(\",\",\" \")\n",
    "        if label not in labels:\n",
    "            labels.append(label)\n",
    "    label_map = {\n",
    "                    \"Fibreglass\" : [],\n",
    "                    \"GRP\" : [],\n",
    "                    \"Carbon\" : [],\n",
    "                    \"Wood\" : [],\n",
    "                    \"Metal\" : [],\n",
    "                    \"Plastic\" : [],\n",
    "                    \"NaN\" : [],\n",
    "                \n",
    "                }\n",
    "    banned_words = [\"or\" , \"nan\" , \"none\" , \"unknown\" , \"other\" , \"various\" , \"various\" , \"unknown\" , \"n/a\" , \"not applicable\"]\n",
    "    fibreglass_contains = [\"fibreglass\" , \"fiberglass\" , \"glass\" , \"glassfiber\" , \"glass fibre\" , \"glassfiber\" , \"fg\"]\n",
    "    grp_contains = [\"grp\"]\n",
    "    carbon_contains = [\"carbon\" , \"carbonfiber\" , \"carbon fibre\" , \"carbonfiber\"]\n",
    "    wood_contains = [\"wood\" , \"mahogany\" , \"teak\" , \"oak\" , \"cedar\" , \"pine\" , \"fir\" , \"spruce\" , \"balsa\" , \"plywood\" , \"ply wood\"]\n",
    "    metal_contains = [\"metal\" , \"steel\" , \"aluminium\" , \"aluminum\" , \"iron\" , \"bronze\" , \"copper\" , \"lead\" , \"brass\" , \"stainless steel\" , \"stainlesssteel\" , \"stainlesssteel\" , \"alu\"]\n",
    "    plastic_contains = [\"plastic\" , \"pvc\" , \"polyester\" , \"polyethylene\" , \"polypropylene\" , \"polyurethane\" , \"polyvinyl chloride\" , \"polyvinylchloride\" , \"polyvinylchloride\"]\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] not in label_map:\n",
    "            # First check if label contains any banned words.\n",
    "            if any(word in labels[i] for word in banned_words):\n",
    "                label_map[\"NaN\"].append(labels[i])\n",
    "            # Then check if label contains any carbon words.\n",
    "            elif any(word in labels[i] for word in carbon_contains):\n",
    "                label_map[\"Carbon\"].append(labels[i])\n",
    "            # Then check if label contains any grp words.\n",
    "            elif any(word in labels[i] for word in grp_contains):\n",
    "                label_map[\"GRP\"].append(labels[i])\n",
    "            # Then check if label contains any fibreglass words.\n",
    "            elif any(word in labels[i] for word in fibreglass_contains):\n",
    "                label_map[\"Fibreglass\"].append(labels[i])\n",
    "            # Then check if label contains any wood words.\n",
    "            elif any(word in labels[i] for word in wood_contains):\n",
    "                label_map[\"Wood\"].append(labels[i])\n",
    "            # Then check if label contains any metal words.\n",
    "            elif any(word in labels[i] for word in metal_contains):\n",
    "                label_map[\"Metal\"].append(labels[i])\n",
    "            # Then check if label contains any plastic words.\n",
    "            elif any(word in labels[i] for word in plastic_contains):\n",
    "                label_map[\"Plastic\"].append(labels[i])\n",
    "            else:\n",
    "                label_map[\"NaN\"].append(labels[i])\n",
    "\n",
    "    # Reverse the label map.\n",
    "    reverse_label_map = {}\n",
    "    for key in label_map:\n",
    "        for value in label_map[key]:\n",
    "            reverse_label_map[value] = key\n",
    "    reverse_label_map = {k: v for k, v in sorted(reverse_label_map.items(), key=lambda item: item[1])}\n",
    "\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        data['Construction'][i] = reverse_label_map[Construction_Classes[data['Construction'][i]].lower().strip().replace(\".\",\" \").replace(\"/\",\" \").replace(\",\",\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ballast_type(data):\n",
    "    labels = []\n",
    "    for i in range(len(data)):\n",
    "        label = Ballast_Type_Classes[data['Ballast Type'][i]].lower().replace(\".\",\" \").replace(\"*\",\" \").strip()\n",
    "        if label not in labels:\n",
    "            labels.append(label)\n",
    "    label_map = {\n",
    "                    \"Lead\" : [],\n",
    "                    \"Iron\" : [],\n",
    "                    \"Concrete\" : [],\n",
    "                    \"Steel\" : [],\n",
    "                    \"Water\" : [],\n",
    "                    \"Stone\" : [],\n",
    "                    \"Mix\" : [],\n",
    "                    \"NaN\" : [],\n",
    "                }\n",
    "    mix_contains = [\"mix\" , \"mixed\" , \"or\" , \"/\" , \"and\" , \"with\" , \"varies\" , \"anything\" , \"%\" , \"+\" , \",\" , \";\" , \":\"]\n",
    "    lead_contains = [\"lead\" , \"lea\" , ]\n",
    "    iron_contains = [\"iron\"]\n",
    "    concrete_contains = [\"concrete\" , \"cement\"]\n",
    "    steel_contains = [\"steel\" , \"SS\"]\n",
    "    water_contains = [\"water\"]\n",
    "    stone_contains = [\"stone\"]\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] not in label_map:\n",
    "            # First check if label contains any mix words.\n",
    "            if any(word in labels[i] for word in mix_contains):\n",
    "                label_map[\"Mix\"].append(labels[i])\n",
    "            # Then check if label contains any lead words.\n",
    "            elif any(word in labels[i] for word in lead_contains):\n",
    "                label_map[\"Lead\"].append(labels[i])\n",
    "            # Then check if label contains any iron words.\n",
    "            elif any(word in labels[i] for word in iron_contains):\n",
    "                label_map[\"Iron\"].append(labels[i])\n",
    "            # Then check if label contains any concrete words.\n",
    "            elif any(word in labels[i] for word in concrete_contains):\n",
    "                label_map[\"Concrete\"].append(labels[i])\n",
    "            # Then check if label contains any steel words.\n",
    "            elif any(word in labels[i] for word in steel_contains):\n",
    "                label_map[\"Steel\"].append(labels[i])\n",
    "            # Then check if label contains any water words.\n",
    "            elif any(word in labels[i] for word in water_contains):\n",
    "                label_map[\"Water\"].append(labels[i])\n",
    "            # Then check if label contains any stone words.\n",
    "            elif any(word in labels[i] for word in stone_contains):\n",
    "                label_map[\"Stone\"].append(labels[i])\n",
    "            else:\n",
    "                label_map[\"NaN\"].append(labels[i])\n",
    "\n",
    "    # Reverse the label map.\n",
    "    reverse_label_map = {}\n",
    "    for key in label_map:\n",
    "        for value in label_map[key]:\n",
    "            reverse_label_map[value] = key\n",
    "    reverse_label_map = {k: v for k, v in sorted(reverse_label_map.items(), key=lambda item: item[1])}\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        data['Ballast Type'][i] = reverse_label_map[Ballast_Type_Classes[data['Ballast Type'][i]].lower().replace(\".\",\" \").replace(\"*\",\" \").strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sailboatdata_data = pd.read_csv(data_dir+\"sailboatdata_data_hashed.csv\")\n",
    "boat24_data = pd.read_csv(data_dir+\"boat24_data_hashed.csv\")\n",
    "image_search_data = pd.read_csv(data_dir+\"image_search_data_hashed.csv\")\n",
    "data_list = [sailboatdata_data , boat24_data , image_search_data]\n",
    "\n",
    "for data in data_list:\n",
    "    clean_ballast_type(data)\n",
    "    clean_construction(data)\n",
    "\n",
    "# Make a label2id map for ballast type. and construction.\n",
    "ballast_types = (sailboatdata_data['Ballast Type'].dropna().unique().tolist())\n",
    "construction_types = (sailboatdata_data['Construction'].dropna().unique().tolist())\n",
    "ballast_types.remove(\"NaN\")\n",
    "construction_types.remove(\"NaN\")\n",
    "ballast_types = [\"NaN\"] + ballast_types\n",
    "construction_types = [\"NaN\"] + construction_types\n",
    "\n",
    "ballast_type_id2label = {v : k for k , v in zip(ballast_types , range(-1 , len(ballast_types)))}\n",
    "construction_id2label = {v : k for k , v in zip(construction_types , range(-1 , len(construction_types)))}\n",
    "\n",
    "# Make an id2label map for ballast type. and construction.\n",
    "ballast_type_label2id = {v : k for k , v in ballast_type_id2label.items()}\n",
    "construction_label2id = {v : k for k , v in construction_id2label.items()}\n",
    "\n",
    "# Save the label2id and id2label maps.\n",
    "with open(data_dir+\"ballast_type_label2id.json\" , \"w\") as f:\n",
    "    json.dump(ballast_type_label2id , f)\n",
    "with open(data_dir+\"ballast_type_id2label.json\" , \"w\") as f:\n",
    "    json.dump(ballast_type_id2label , f)\n",
    "with open(data_dir+\"construction_label2id.json\" , \"w\") as f:\n",
    "    json.dump(construction_label2id , f)\n",
    "with open(data_dir+\"construction_id2label.json\" , \"w\") as f:\n",
    "    json.dump(construction_id2label , f)\n",
    "\n",
    "# Map the labels to ids.\n",
    "for data in data_list:\n",
    "    for i in range(len(data)):\n",
    "        if data['Ballast Type'][i] is not None:\n",
    "            data['Ballast Type'][i] = ballast_type_label2id[data['Ballast Type'][i]]\n",
    "        if data['Construction'][i] is not None:\n",
    "            data['Construction'][i] = construction_label2id[data['Construction'][i]]\n",
    "\n",
    "# Save the data.\n",
    "sailboatdata_data.to_csv(data_dir+\"sailboatdata_data_hashed_cleaned.csv\" , index=False)\n",
    "boat24_data.to_csv(data_dir+\"boat24_data_hashed_cleaned.csv\" , index=False)\n",
    "image_search_data.to_csv(data_dir+\"image_search_data_hashed_cleaned.csv\" , index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train , data_test = train_test_split(data_clean , test_size = 0.2 , random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"data/train/\"\n",
    "test_dir = \"data/test/\"\n",
    "\n",
    "if not(os.path.exists(train_dir)):\n",
    "    print(\"Making dir\" , train_dir)\n",
    "    makedirs(train_dir)\n",
    "\n",
    "if not(os.path.exists(test_dir)):\n",
    "    print(\"Making dir\" , test_dir)\n",
    "    makedirs(test_dir)\n",
    "\n",
    "\n",
    "for image in data_train['img_path']:\n",
    "    try:\n",
    "        img = Image.open(img_dir+image)\n",
    "        img = img.convert('RGB')\n",
    "        img.save(train_dir+image)\n",
    "    except:\n",
    "        print(\"Image not found\" , image)\n",
    "        data_train = data_train[data_train['img_path'] != image]\n",
    "\n",
    "for image in data_test['img_path']:\n",
    "    try:\n",
    "        img = Image.open(img_dir+image)\n",
    "        img = img.convert('RGB')\n",
    "        img.save(test_dir+image)\n",
    "    except:\n",
    "        print(\"Image not found\" , image)\n",
    "        data_test = data_test[data_test['img_path'] != image]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.to_csv(data_dir+\"boat_data_train.csv\" , index = False)\n",
    "data_test.to_csv(data_dir+\"boat_data_test.csv\" , index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = pd.DataFrame(cleaned_data , columns = data_raw.columns)\n",
    "\n",
    "col_list = ['Hull Type' , 'Rigging Type' , 'Construction' , 'Ballast Type' , 'Designer' , 'name']\n",
    "\n",
    "for column in data_clean.columns:\n",
    "    if column in col_list:\n",
    "        path = data_dir+\"/labels/\"+column.replace(\" \",\"_\")\n",
    "        data_clean[column] = data_clean[column].astype('category')\n",
    "        classes = dict( zip(data_clean[column] , data_clean[column].cat.codes))\n",
    "        with open(path+\".json\", \"w\") as outfile:\n",
    "            json.dump(classes, outfile , indent = 4)\n",
    "        data_clean[column] = data_clean[column].cat.codes\n",
    "\n",
    "data_clean.to_csv(data_dir+\"boat_data_clean.csv\" , index = False)\n",
    "data_train , data_test = train_test_split(data_clean , test_size = 0.2 , random_state = 42)\n",
    "data_train.to_csv(data_dir+\"boat_data_train.csv\" , index = False)\n",
    "data_test.to_csv(data_dir+\"boat_data_test.csv\" , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"images/00002.jpg\"\n",
    "# df = data_train\n",
    "# idx = df.index[(df['img_path'] == file_path.split(\"/\")[1])].values[0]\n",
    "\n",
    "# print(\n",
    "# Hull_Type_Classes[df['Hull Type'][idx]],[df['Hull Type'][idx]],\"\\n\"+\n",
    "# Rigging_Type_Classes[df['Rigging Type'][idx]],[df['Rigging Type'][idx]],\"\\n\"+\n",
    "# Construction_Classes[df['Construction'][idx]],[df['Construction'][idx]],\"\\n\"+\n",
    "# Ballast_Type_Classes[df['Ballast Type'][idx]],[df['Ballast Type'][idx]],\"\\n\"+\n",
    "# Designer_Classes[df['Designer'][idx]],[df['Designer'][idx]],\"\\n\"+\n",
    "# Name_Classes[df['name'][idx]],[df['name'][idx]],\"\\n\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Hull Type' , 'Rigging Type' , 'Construction' , 'Ballast Type' , 'Designer' , 'name']\n",
    "for label in labels:\n",
    "    sailboatdata_data = pd.read_csv(\"data/boat_data_clean.csv\")\n",
    "    sailboatdata_data = sailboatdata_data.dropna(subset=[label])\n",
    "    # drop labels with less than 2 entries\n",
    "    \n",
    "    sailboatdata_data = sailboatdata_data.groupby(label).filter(lambda x: len(x) > 1)\n",
    "    sailboatdata_data = sailboatdata_data.drop_duplicates(subset=[label])\n",
    "    print(label,sailboatdata_data.__len__())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "edf259275ad4a72d4dd5b452264ad5fb2b635233dff2a31edc6ebc740e55e21b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
