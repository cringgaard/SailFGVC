{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset , concatenate_datasets\n",
    "from transformers import AutoImageProcessor\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "from transformers import DefaultDataCollator\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModel , AutoModelForImageClassification, TrainingArguments, Trainer , ImageClassificationPipeline , ResNetForImageClassification , ResNetModel , ResNetConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "from transformers import pipeline\n",
    "from sklearn import metrics\n",
    "import json\n",
    "import PIL\n",
    "from data.clean_classes import *\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image as mpimg\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "data_dir = \"data/\"\n",
    "img_dir = \"E:/data/images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_PROJECT'] = \"Sailboat FGVC\"\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\"\n",
    "# os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "os.environ[\"WANDB_START_METHOD\"]='thread'\n",
    "wandb_project = \"Sailboat FGVC Clean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset boats_dataset (C:\\Users\\chris\\.cache\\huggingface\\datasets\\cringgaard___boats_dataset\\default\\0.0.0\\3780f35f24ee5458f59c11c69640a6f7f9001aaabc6ce51227831bd076a1ce4e)\n",
      "Using custom data configuration default\n",
      "Reusing dataset boats_dataset (C:\\Users\\chris\\.cache\\huggingface\\datasets\\cringgaard___boats_dataset\\default\\0.0.0\\3780f35f24ee5458f59c11c69640a6f7f9001aaabc6ce51227831bd076a1ce4e)\n"
     ]
    }
   ],
   "source": [
    "access_token = \"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\"\n",
    "write_token = \"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\"\n",
    "# login(token=access_token)\n",
    "dataset_boat24 = load_dataset(\"cringgaard/boats_dataset\" , use_auth_token=access_token, split=\"boat24\")\n",
    "dataset = load_dataset(\"cringgaard/boats_dataset\" , use_auth_token=access_token, split=\"sailboatdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "# checkpoint = \"google/vit-base-patch16-224\"\n",
    "# model_name = \"ViT\"\n",
    "model_dir = \"D:/models/\"\n",
    "checkpoint = \"microsoft/resnet-18\"\n",
    "model_name = \"ResNet18\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "label_types = [\"Hull Type\" , \"Rigging Type\" ,  \"Construction\" , \"Ballast Type\" , \"Designer\"]\n",
    "# label_types = [\"Rigging Type\" , \"Construction\" , \"Ballast Type\" , \"Designer\"]\n",
    "# label_types = [\"Hull Type\" , \"Rigging Type\"]\n",
    "label_maps = {\n",
    "    \"Hull Type\" : Hull_Type_Classes,\n",
    "    \"Rigging Type\" : Rigging_Type_Classes,\n",
    "    \"Construction\" : Construction_Classes,\n",
    "    \"Ballast Type\" : Ballast_Type_Classes,\n",
    "    \"Designer\" : Designer_Classes\n",
    "}\n",
    "# label_types = [\"Ballast Type\" , \"Designer\"]\n",
    "# label_types = [\"Designer\"]\n",
    "# losses = [\"CE\" , \"WeightedCE\"]\n",
    "# losses = [\"WeightedCE\"]\n",
    "losses = [\"CE\"]\n",
    "batch_sizes = [16]\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "\n",
    "    # print(\"Predictions\",predictions)\n",
    "    # print(\"Labels\",labels)\n",
    "\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "\n",
    "    if len(np.shape(labels)) > 1:\n",
    "        # print(\"Several Labels\")\n",
    "        labels = np.transpose(labels)[-1]\n",
    "        # print(\"Labels\",labels)\n",
    "\n",
    "    metrics = {}\n",
    "    metrics.update(accuracy.compute(predictions=predictions, references=labels))\n",
    "    metrics.update(f1.compute(predictions=predictions, references=labels , average=\"macro\"))\n",
    "    metrics.update(precision.compute(predictions=predictions, references=labels , average=\"macro\"))\n",
    "    metrics.update(recall.compute(predictions=predictions, references=labels , average=\"macro\"))\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_metrics_multitask(eval_pred):\n",
    "        predictions = eval_pred.predictions\n",
    "        labels = np.transpose(eval_pred.label_ids)\n",
    "        metrics = {}\n",
    "\n",
    "        # for i , head in enumerate(labels.keys()):\n",
    "        #     preds[head] = predictions[i]\n",
    "        \n",
    "        # predictions = preds\n",
    "\n",
    "        # for head in labels.keys():\n",
    "        for i , head in enumerate(label_types):\n",
    "            predicted_labels = predictions[i].argmax(axis=1)\n",
    "            # print(\"Predicted Labels :\",predicted_labels)\n",
    "            # print(\"True Labels :\",labels[i])\n",
    "            # print(\"Predicted Labels Shape :\",predicted_labels.shape)\n",
    "            # print(\"True Labels Shape :\",labels[i].shape)\n",
    "            f1 = f1_score(labels[i], predicted_labels, average='macro')\n",
    "            accuracy = accuracy_score(labels[i], predicted_labels)\n",
    "            metrics[f'{label_types[i]}_f1'] = f1\n",
    "            metrics[f'{label_types[i]}_accuracy'] = accuracy\n",
    "\n",
    "        return metrics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testModel = MultitaskViT()\n",
    "# outputs = testModel(**{'pixel_values' : test_images})\n",
    "# print([x.shape for x in outputs])\n",
    "# # compute_metrics_multitask(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for gradient_accumulation_step in batch_sizes:\n",
    "# wandb.init(project=\"Sailboat FGVC\", name=model_name+\"_multitask\")\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# dataset_specific = dataset['full'].train_test_split(test_size=0.2, shuffle=True, seed=43)\n",
    "\n",
    "# def transforms(examples):\n",
    "#     examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"img_path\"]]\n",
    "#     del examples[\"img_path\"]\n",
    "#     del examples[\"name\"]\n",
    "#     return examples\n",
    "\n",
    "\n",
    "# # id2label = {float(i): label for i, label in enumerate(label_types)}\n",
    "# # label2id = {label: float(i) for i, label in enumerate(label_types)}\n",
    "\n",
    "\n",
    "# dataset_specific = dataset_specific.with_transform(transforms)\n",
    "# # dataset_specific.set_format(type=\"torch\")\n",
    "# data_collator = DefaultDataCollator()\n",
    "\n",
    "# model = MultitaskViT()\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"E:/models/\"+model_name+\"_multitask\",\n",
    "#     report_to=\"wandb\",\n",
    "#     remove_unused_columns=False,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     save_strategy=\"steps\",\n",
    "#     learning_rate=5e-5,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     gradient_accumulation_steps=2,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=100,\n",
    "#     warmup_ratio=0.1,\n",
    "#     logging_steps=10,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"f1\",\n",
    "#     # no_cuda=True\n",
    "#     # push_to_hub=True,\n",
    "# )\n",
    "\n",
    "# trainer = MultiTaskTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     data_collator=data_collator,\n",
    "#     train_dataset=dataset_specific[\"train\"],\n",
    "#     eval_dataset=dataset_specific[\"test\"],\n",
    "#     tokenizer=image_processor,\n",
    "#     compute_metrics=compute_metrics_multitask,\n",
    "    \n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "# wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label_type in label_types:\n",
    "#     name = \"Baseline_\"+label_type\n",
    "#     # wandb.init(project=\"Sailboat FGVC\", name=name)\n",
    "#     torch.cuda.empty_cache()\n",
    "#     c_names = dataset.column_names[1:]\n",
    "#     c_names.remove(label_type)\n",
    "#     dataset_specific = dataset.remove_columns(c_names)\n",
    "\n",
    "#     labels = dataset.features[label_type].names\n",
    "#     id2label = {int(i): label for i, label in enumerate(labels)}\n",
    "#     label2id = {label : int(i) for i, label in enumerate(labels)}\n",
    "\n",
    "#     dataset_specific = dataset_specific.train_test_split(test_size=0.2, shuffle=True, seed=43)\n",
    "\n",
    "#     labels_train_counts = np.bincount(dataset_specific['train'][label_type] , minlength=len(labels))\n",
    "#     labels_to_remove = np.where(labels_train_counts < 2)[0] # remove labels with less than 2 examples\n",
    "#     dataset_specific['train'] = dataset_specific['train'].filter(lambda x: x[label_type] not in labels_to_remove)\n",
    "#     dataset_specific['test'] = dataset_specific['test'].filter(lambda x: x[label_type] not in labels_to_remove)\n",
    "#     labels_train_counts = np.bincount(dataset_specific['train'][label_type] , minlength=len(labels))\n",
    "#     y_pred = labels_train_counts/labels_train_counts.sum()\n",
    "#     y_pred = (np.array([y_pred]*len(dataset_specific['test'][label_type])))\n",
    "#     baseline_metrics = compute_metrics([y_pred, dataset_specific['test'][label_type]])\n",
    "#     baseline_metrics = {\"eval/\"+ key: val for key, val in baseline_metrics.items()}\n",
    "#     print(baseline_metrics)\n",
    "#     # wandb.log(baseline_metrics)\n",
    "#     wandb.log\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_size in batch_sizes:\n",
    "#     for loss in losses:\n",
    "#         for label_type in label_types: \n",
    "#             tags = [model_name , label_type, loss, str(batch_size)]\n",
    "#             name = \"_\".join(tags)\n",
    "#             wandb.init(project=wandb_project, name=name , group = label_type , tags = tags)\n",
    "#             torch.cuda.empty_cache()\n",
    "#             c_names = dataset.column_names[1:]\n",
    "#             c_names.remove(label_type)\n",
    "#             # Map labels to ids using label map\n",
    "#             dataset_specific = dataset.remove_columns(c_names)\n",
    "#             labels = dataset.features[label_type].names\n",
    "\n",
    "#             dataset_specific = dataset_specific.train_test_split(test_size=0.2, shuffle=True, seed=43)\n",
    "\n",
    "#             labels_train_counts = np.bincount(dataset_specific['train'][label_type] , minlength=len(labels))\n",
    "#             labels_test_counts = np.bincount(dataset_specific['test'][label_type] , minlength=len(labels))\n",
    "#             labels_to_remove = np.where(labels_train_counts < 1)[0] # remove labels with less than 2 examples\n",
    "#             labels_to_remove = np.union1d(labels_to_remove, np.where(labels_test_counts < 1)[0])\n",
    "#             # dataset_specific['train'] = dataset_specific['train'].filter(lambda x: x[label_type] not in labels_to_remove)\n",
    "#             dataset_specific['test'] = dataset_specific['test'].filter(lambda x: x[label_type] not in labels_to_remove)\n",
    "\n",
    "#             id2label = {int(i): label for i, label in enumerate(labels)}\n",
    "#             label2id = {label : int(i) for i, label in enumerate(labels)}\n",
    "\n",
    "#             dataset_specific['train'] = dataset_specific['train'].filter(lambda x: id2label[x[label_type]] not in [\"NaN\"])\n",
    "#             dataset_specific['test'] = dataset_specific['test'].filter(lambda x: id2label[x[label_type]] not in [\"NaN\"])\n",
    "\n",
    "#             id2label = {int(i): label for i, label in enumerate(labels)}\n",
    "#             label2id = {label : int(i) for i, label in enumerate(labels)}\n",
    "\n",
    "#             labels = [id2label[label2id[x]] for x in labels]\n",
    "            \n",
    "#             labels_train_counts = np.bincount(dataset_specific['train'][label_type] , minlength=len(labels))\n",
    "#             labels_test_counts = np.bincount(dataset_specific['test'][label_type] , minlength=len(labels))\n",
    "\n",
    "#             if loss == \"WeightedCE\":\n",
    "#                 weights = np.array([1 if x == 0 else x for x in labels_train_counts])\n",
    "#                 weights = (1/weights)\n",
    "#                 weights /= weights.sum()\n",
    "#                 weights = torch.tensor(weights, dtype=torch.float , device=torch.device(\"cuda:0\"))\n",
    "\n",
    "#                 class WeightedCETrainer(Trainer):\n",
    "#                     def __init__(self, *args, **kwargs):\n",
    "#                         super().__init__(*args, **kwargs)\n",
    "#                     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#                         labels = inputs.get(\"labels\")\n",
    "#                         labels.to(torch.device(\"cuda:0\"))\n",
    "#                         outputs = model(**inputs)\n",
    "#                         logits = outputs.get(\"logits\")\n",
    "#                         # loss_fct = nn.CrossEntropyLoss(weight=weights , label_smoothing=0.1)\n",
    "#                         loss_fct = nn.CrossEntropyLoss(weight=weights)\n",
    "#                         loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "#                         return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "#             def transforms(examples):\n",
    "#                 examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"img_path\"]]\n",
    "#                 examples[\"labels\"] = examples[label_type]\n",
    "#                 del examples[label_type]\n",
    "#                 del examples[\"img_path\"]\n",
    "#                 return examples\n",
    "\n",
    "#             data_collator = DefaultDataCollator()\n",
    "\n",
    "#             model = ResNetForImageClassification.from_pretrained(\n",
    "#                 checkpoint,\n",
    "#                 num_labels=len(labels),\n",
    "#                 id2label=id2label,\n",
    "#                 label2id=label2id,\n",
    "#                 use_auth_token=access_token,\n",
    "#                 ignore_mismatched_sizes=True,\n",
    "#             )\n",
    "\n",
    "#             training_args = TrainingArguments(\n",
    "#                 output_dir=model_dir+name,\n",
    "#                 report_to=\"wandb\",\n",
    "#                 remove_unused_columns=False,\n",
    "#                 evaluation_strategy=\"epoch\",\n",
    "#                 logging_strategy=\"epoch\",\n",
    "#                 save_strategy=\"epoch\",\n",
    "#                 # eval_steps = 10,\n",
    "#                 # logging_steps = 10,\n",
    "#                 # save_steps = 10,\n",
    "#                 save_total_limit=10,\n",
    "#                 learning_rate=5e-5,\n",
    "#                 per_device_train_batch_size=batch_size,\n",
    "#                 gradient_accumulation_steps=1,\n",
    "#                 per_device_eval_batch_size=batch_size,\n",
    "#                 num_train_epochs=EPOCHS,\n",
    "#                 warmup_ratio=0.1,\n",
    "#                 load_best_model_at_end=True,\n",
    "#                 metric_for_best_model=\"f1\",\n",
    "#                 # label_smoothing_factor=0.1,\n",
    "#                 # no_cuda=True\n",
    "#                 # push_to_hub=True,\n",
    "#                 # hub_strategy=\"end\",\n",
    "#                 # hub_model_id=\"boats_dataset\",\n",
    "#                 # hub_token=write_token,\n",
    "#             )\n",
    "#             if loss == \"CE\":\n",
    "#                 trainer = Trainer(\n",
    "#                 model=model,\n",
    "#                 args=training_args,\n",
    "#                 data_collator=data_collator,\n",
    "#                 train_dataset=dataset_specific[\"train\"].with_transform(transforms),\n",
    "#                 eval_dataset=dataset_specific[\"test\"].with_transform(transforms),\n",
    "#                 tokenizer=image_processor,\n",
    "#                 compute_metrics=compute_metrics,\n",
    "#                 )\n",
    "#             elif loss == \"WeightedCE\":\n",
    "#                 trainer = WeightedCETrainer(\n",
    "#                     model=model,\n",
    "#                     args=training_args,\n",
    "#                     data_collator=data_collator,\n",
    "#                     train_dataset=dataset_specific[\"train\"].with_transform(transforms),\n",
    "#                     eval_dataset=dataset_specific[\"test\"].with_transform(transforms),\n",
    "#                     tokenizer=image_processor,\n",
    "#                     compute_metrics=compute_metrics,\n",
    "#                 )\n",
    "#             # Plot Label Distribution For Training Data\n",
    "#             fig1 = plt.figure()\n",
    "#             ax = fig1.add_axes([0,0,1,1])\n",
    "#             ax.bar([label2id[x] for x in labels], labels_train_counts/labels_train_counts.sum()) # Normalized\n",
    "#             ax.set_ylabel(\"Number of examples normalised\")\n",
    "#             ax.set_title(\"Label Distribution\")\n",
    "#             wandb.log({\"Label Distribution Train\": (fig1)})\n",
    "\n",
    "#             # Plot Label Distribution For Test Data\n",
    "#             fig2 = plt.figure()\n",
    "#             ax = fig2.add_axes([0,0,1,1])\n",
    "#             ax.bar([label2id[x] for x in labels], labels_test_counts/labels_test_counts.sum()) # Normalized\n",
    "#             ax.set_ylabel(\"Number of examples normalised\")\n",
    "#             ax.set_title(\"Label Distribution\")\n",
    "#             wandb.log({\"Label Distribution Test\": (fig2)})\n",
    "\n",
    "#             # Log label2id\n",
    "#             wandb.log({\"Labels\": wandb.Table(data = list(zip(label2id.keys() , label2id.values())) , columns=[\"Label\" , \"ID\"])})\n",
    "\n",
    "#             # Train Model\n",
    "#             trainer.train()\n",
    "\n",
    "#             # Save Model\n",
    "#             trainer.save_model(model_dir+name)\n",
    "\n",
    "#             pipeline = ImageClassificationPipeline(model=trainer.model, feature_extractor = trainer.tokenizer , framework=\"pt\", device=0)\n",
    "#             predict_data = dataset_specific['test'].select(np.random.randint(0, len(dataset_specific['test']), batch_size))\n",
    "#             images = [predict_data['img_path'][i] for i in range(batch_size)]\n",
    "#             predictions = pipeline(images)\n",
    "#             prediction_table = []\n",
    "#             for i in range(len(predictions)):\n",
    "#                 prediction_table.append([wandb.Image(images[i]) , predictions[i] , id2label[predict_data[label_type][i]]])\n",
    "#             columns = [\"Image\" , \"Label Predictions\" , \"True Label\"]\n",
    "#             wandb.log({\"Image Predicitions\" : wandb.Table(data=prediction_table, columns=columns)})\n",
    "\n",
    "#             # Plot confusion matrix\n",
    "#             y_pred = trainer.predict(dataset_specific['test'].with_transform(transforms)).predictions.argmax(-1)\n",
    "#             y_true = dataset_specific[\"test\"][label_type]\n",
    "#             wandb.log({\"Confusion Matrix\": wandb.sklearn.plot_confusion_matrix(y_true, y_pred, labels=labels)})\n",
    "#             wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multitask Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "from transformers.models.resnet.modeling_resnet import ImageClassifierOutputWithNoAttention\n",
    "from typing import Optional\n",
    "\n",
    "class ResNetCustomModel(PreTrainedModel):\n",
    "    def __init__(self, config, num_classes_list):\n",
    "        super().__init__(config)\n",
    "        self.num_classes_list = num_classes_list\n",
    "        self.resnet = ResNetModel.from_pretrained(\"microsoft/resnet-18\")\n",
    "        self.heads = nn.ModuleList([nn.Sequential(nn.Flatten() , nn.Linear(512, num_classes)) for num_classes in num_classes_list])\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> ImageClassifierOutputWithNoAttention:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.resnet(pixel_values, output_hidden_states=output_hidden_states, return_dict=return_dict)\n",
    "        pooled_outputs = outputs.pooler_output if return_dict else outputs[1]\n",
    "        class_logits = []\n",
    "        for head in self.heads:\n",
    "            class_logits.append(head(pooled_outputs))\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = 0\n",
    "            i = 0\n",
    "            for logits in class_logits:\n",
    "                loss += self.criterion(logits, torch.transpose(labels,0,1)[i])\n",
    "                i += 1\n",
    "            loss = loss / len(class_logits)\n",
    "        \n",
    "        return ImageClassifierOutputWithNoAttention(loss=loss, logits=class_logits, hidden_states=outputs.hidden_states)\n",
    "\n",
    "\n",
    "from transformers import PretrainedConfig\n",
    "class ResNetCustomModelConfig(PretrainedConfig):\n",
    "    def __init__(self, num_classes_list = [Hull_Type_Classes.__len__(),Rigging_Type_Classes.__len__(),Construction_Classes.__len__(),Ballast_Type_Classes.__len__(),Designer_Classes.__len__()], label2id = None , **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes_list = num_classes_list\n",
    "        self.hidden_size = 512  # Specify the hidden size of the model\n",
    "        self.num_labels = sum(num_classes_list)  # Total number of labels across all classification heads\n",
    "        self.label2id = label2id if label2id is not None else {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make a list containing all combinations of the list of label types\n",
    "# import itertools\n",
    "# label_combinations = list(itertools.combinations(label_types, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label_combination in label_combinations:\n",
    "#     label_types = [label_combination[0] , label_combination[1]]\n",
    "#     print(label_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_image_search = load_dataset(\"cringgaard/boats_dataset\" , use_auth_token=access_token, split=\"image_search\")\n",
    "# for batch_size in batch_sizes:\n",
    "#     # for label_combination in label_combinations:\n",
    "#         # label_types = [label_combination[0] , label_combination[1]]\n",
    "#         label_types = [\"Hull Type\" , \"Rigging Type\" ,  \"Construction\" , \"Ballast Type\" , \"Designer\"]\n",
    "#         tags = [model_name , 'multitask' , \"image_search\"]+label_types\n",
    "#         name = name = \"_\".join(tags)\n",
    "#         wandb.init(project=wandb_project, name=name , tags=tags)\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#         dataset_specific = dataset_image_search.remove_columns('name')\n",
    "\n",
    "#         dataset_specific = dataset_specific.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "\n",
    "\n",
    "#         for label_type in label_types:\n",
    "#             labels = np.unique(dataset_specific['train'][label_type])\n",
    "#             labels_train_counts = np.bincount(dataset_specific['train'][label_type] , minlength=len(labels))\n",
    "#             labels_test_counts = np.bincount(dataset_specific['test'][label_type] , minlength=len(labels))\n",
    "#             labels_to_remove = np.where(labels_train_counts < 1)[0] # remove labels with less than 2 examples\n",
    "#             labels_to_remove = np.union1d(labels_to_remove, np.where(labels_test_counts < 1)[0])\n",
    "#             # dataset_specific['train'] = dataset_specific['train'].filter(lambda x: x[label_type] not in labels_to_remove)\n",
    "#             dataset_specific['test'] = dataset_specific['test'].filter(lambda x: x[label_type] not in labels_to_remove)            \n",
    "#             labels_train_counts = np.bincount(dataset_specific['train'][label_type] , minlength=len(labels))\n",
    "#             labels_test_counts = np.bincount(dataset_specific['test'][label_type] , minlength=len(labels))\n",
    "\n",
    "#         def transforms(examples):\n",
    "#             examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"img_path\"]]\n",
    "#             # examples[\"labels\"] = {label_type : examples[label_type] for label_type in label_types}\n",
    "#             labs = [examples[label_type] for label_type in label_types]\n",
    "#             examples['labels'] = list(map(list, zip(*labs)))\n",
    "#             # Remove all other keys in dictionary\n",
    "#             for key in list(examples.keys()):\n",
    "#                 if key not in [\"pixel_values\" , \"labels\"]:\n",
    "#                     del examples[key]\n",
    "#             return examples\n",
    "        \n",
    "#         data_collator = DefaultDataCollator()\n",
    "#         device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#         num_classes_list = [\n",
    "#                 Hull_Type_Classes.__len__(),\n",
    "#                 Rigging_Type_Classes.__len__(),\n",
    "#                 Construction_Classes.__len__(),\n",
    "#                 Ballast_Type_Classes.__len__(),\n",
    "#                 Designer_Classes.__len__()\n",
    "#         ]\n",
    "#         label2id = {\n",
    "#             \"Hull Type\" : {v : k for k, v in Hull_Type_Classes.items()},\n",
    "#             \"Rigging Type\" : {v : k for k, v in Rigging_Type_Classes.items()},\n",
    "#             \"Construction\" : {v : k for k, v in Construction_Classes.items()},\n",
    "#             \"Ballast Type\" : {v : k for k, v in Ballast_Type_Classes.items()},\n",
    "#             \"Designer\" : {v : k for k, v in Designer_Classes.items()}\n",
    "#         }\n",
    "#         id2label = {\n",
    "#             \"Hull Type\" : Hull_Type_Classes,\n",
    "#             \"Rigging Type\" : Rigging_Type_Classes,\n",
    "#             \"Construction\" : Construction_Classes,\n",
    "#             \"Ballast Type\" : Ballast_Type_Classes,\n",
    "#             \"Designer\" : Designer_Classes\n",
    "#         }\n",
    "\n",
    "#         config = ResNetCustomModelConfig(num_classes_list, label2id=label2id)\n",
    "#         model = ResNetCustomModel(config , num_classes_list=num_classes_list)\n",
    "\n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir=model_dir+name,\n",
    "#             report_to=\"wandb\",\n",
    "#             remove_unused_columns=False,\n",
    "#             evaluation_strategy=\"epoch\",\n",
    "#             logging_strategy=\"epoch\",\n",
    "#             save_strategy=\"epoch\",\n",
    "#             eval_steps = 10,\n",
    "#             logging_steps = 10,\n",
    "#             # save_steps = 10,\n",
    "#             save_total_limit=1,\n",
    "#             learning_rate=5e-5,\n",
    "#             per_device_train_batch_size=batch_size,\n",
    "#             gradient_accumulation_steps=1,\n",
    "#             per_device_eval_batch_size=batch_size,\n",
    "#             num_train_epochs=EPOCHS,\n",
    "#             warmup_ratio=0.1,\n",
    "#         )\n",
    "\n",
    "#         trainer = Trainer(\n",
    "#             model=model,\n",
    "#             args=training_args,\n",
    "#             data_collator=data_collator,\n",
    "#             train_dataset=dataset_specific[\"train\"].with_transform(transforms),\n",
    "#             eval_dataset=dataset_specific[\"test\"].with_transform(transforms),\n",
    "#             tokenizer=image_processor,\n",
    "#             compute_metrics = compute_metrics_multitask,\n",
    "#         )\n",
    "#         trainer.train()\n",
    "#         trainer.save_model(model_dir+name)\n",
    "#         wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_image_search = load_dataset(\"cringgaard/boats_dataset\" , use_auth_token=access_token, split=\"image_search\")\n",
    "# label_types = [\"Hull Type\" , \"Rigging Type\" ,  \"Construction\" , \"Ballast Type\" , \"Designer\"]\n",
    "# for batch_size in batch_sizes:\n",
    "#     # for label_combination in label_combinations:\n",
    "#         # label_types = [label_combination[0] , label_combination[1]]\n",
    "#         name = model_name+\"_Multitask\" #_Image_Search\"\n",
    "#         for label_type in label_types:\n",
    "#             name += \"_\"+label_type\n",
    "#         wandb.init(project=wandb_project, name=name , tags = [model_name , 'multitask']+label_types)\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#         # c_names = dataset.column_names[1:]\n",
    "#         # c_names.remove(label_types[0])\n",
    "#         # c_names.remove(label_types[1])\n",
    "\n",
    "#         dataset_specific = dataset.remove_columns('name')\n",
    "\n",
    "#         dataset_specific = dataset.train_test_split(test_size=0.2, shuffle=True, seed=43)\n",
    "#         dataset_specific['train'] = concatenate_datasets([dataset_specific['train'] , dataset_image_search])\n",
    "\n",
    "\n",
    "#         for label_type in label_types:\n",
    "#             labels = np.unique(dataset_specific['train'][label_type])\n",
    "#             labels_train_counts = np.bincount(dataset_specific['train'][label_type] , minlength=len(labels))\n",
    "#             labels_test_counts = np.bincount(dataset_specific['test'][label_type] , minlength=len(labels))\n",
    "#             labels_to_remove = np.where(labels_train_counts < 1)[0] # remove labels with less than 2 examples\n",
    "#             labels_to_remove = np.union1d(labels_to_remove, np.where(labels_test_counts < 1)[0])\n",
    "#             # dataset_specific['train'] = dataset_specific['train'].filter(lambda x: x[label_type] not in labels_to_remove)\n",
    "#             dataset_specific['test'] = dataset_specific['test'].filter(lambda x: x[label_type] not in labels_to_remove)            \n",
    "#             labels_train_counts = np.bincount(dataset_specific['train'][label_type] , minlength=len(labels))\n",
    "#             labels_test_counts = np.bincount(dataset_specific['test'][label_type] , minlength=len(labels))\n",
    "\n",
    "#         def transforms(examples):\n",
    "#             examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"img_path\"]]\n",
    "#             # Remove all other keys in dictionary\n",
    "#             for key in list(examples.keys()):\n",
    "#                 if key not in label_types+[\"pixel_values\"]:\n",
    "#                     del examples[key]\n",
    "#             return examples\n",
    "        \n",
    "#         data_collator = DefaultDataCollator()\n",
    "#         device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#         model = MultitaskResnet(device)\n",
    "\n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir=model_dir+name,\n",
    "#             report_to=\"wandb\",\n",
    "#             remove_unused_columns=False,\n",
    "#             evaluation_strategy=\"epoch\",\n",
    "#             logging_strategy=\"epoch\",\n",
    "#             save_strategy=\"epoch\",\n",
    "#             eval_steps = 10,\n",
    "#             logging_steps = 10,\n",
    "#             # save_steps = 10,\n",
    "#             save_total_limit=1,\n",
    "#             learning_rate=5e-5,\n",
    "#             per_device_train_batch_size=batch_size,\n",
    "#             gradient_accumulation_steps=1,\n",
    "#             per_device_eval_batch_size=batch_size,\n",
    "#             num_train_epochs=EPOCHS,\n",
    "#             warmup_ratio=0.1,\n",
    "#         )\n",
    "\n",
    "#         trainer = MultitaskTrainer(\n",
    "#             model=model,\n",
    "#             args=training_args,\n",
    "#             data_collator=data_collator,\n",
    "#             train_dataset=dataset_specific[\"train\"].with_transform(transforms),\n",
    "#             eval_dataset=dataset_specific[\"test\"].with_transform(transforms),\n",
    "#             tokenizer=image_processor,\n",
    "#             compute_metrics = MultitaskTrainer.compute_metrics,\n",
    "#             label_types=label_types,\n",
    "#         )\n",
    "#         print(trainer.label_types)\n",
    "#         trainer.train()\n",
    "#         trainer.save_model(model_dir+name)\n",
    "\n",
    "#         from collections import namedtuple\n",
    "#         predictions = trainer.predict(dataset_specific['test'].with_transform(transforms)).predictions\n",
    "#         labels = {}\n",
    "#         for label_type in label_types:\n",
    "#             labels[label_type] = (dataset_specific['test'][label_type])\n",
    "#         # Make tuple of labels\n",
    "#         labels = tuple(labels.values())\n",
    "#         eval_pred = namedtuple(\"EvalPrediction\", [\"predictions\", \"label_ids\"])\n",
    "#         metrics = trainer.compute_metrics(trainer , eval_pred(predictions, labels))\n",
    "#         print(metrics)\n",
    "#         wandb.log(metrics)\n",
    "#         wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = MultitaskTrainer.compute_metrics(trainer , eval_pred(predictions, labels))\n",
    "# print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import namedtuple\n",
    "# predictions = trainer.predict(dataset_specific['test'].with_transform(transforms)).predictions\n",
    "# labels = (dataset_specific['test'][label_types[0]] , dataset_specific['test'][label_types[1]], dataset_specific['test'][label_types[2]] , dataset_specific['test'][label_types[3]] , dataset_specific['test'][label_types[4]])\n",
    "# eval_pred = namedtuple(\"EvalPrediction\", [\"predictions\", \"label_ids\"])\n",
    "# metrics = compute_metrics_multitask(eval_pred(predictions, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lab_types = list(dataset_specific['test'].features.items())[1:-1]\n",
    "# for i in range(len(lab_types)):\n",
    "#     lab_types[i] = lab_types[i][0]\n",
    "# # Construct a dictionary of label_type and dataset labels\n",
    "# labels = {}\n",
    "# for label in lab_types:\n",
    "#     labels[label] = {label : dataset_specific['test'][label]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = {key : value for key , value in dataset_specific['test'].features.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Data Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boat24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_size in batch_sizes:\n",
    "#     for label_type in label_types:\n",
    "#         tags = [model_name , label_type , 'boat24']\n",
    "#         name = \"_\".join(tags)\n",
    "#         wandb.init(project=wandb_project, name=name , group = label_type , tags = tags)\n",
    "#         torch.cuda.empty_cache()\n",
    "#         c_names = dataset.column_names[1:]\n",
    "#         c_names.remove(label_type)\n",
    "#         dataset_specific = dataset.remove_columns(c_names)\n",
    "#         labels = dataset.features[label_type].names\n",
    "#         dataset_boat24_specific = dataset_boat24.remove_columns(c_names)\n",
    "\n",
    "#         dataset_specific = dataset_specific.train_test_split(test_size=0.2, shuffle=True, seed=43)  # 80-20 split for train and test\n",
    "#         dataset_specific['train'] = concatenate_datasets([dataset_specific['train'] , dataset_boat24_specific]) # add boat24 dataset to training set\n",
    "        \n",
    "#         labels_train_counts = np.bincount(dataset_specific['train'][label_type] , minlength=len(labels))\n",
    "#         labels_test_counts = np.bincount(dataset_specific['test'][label_type] , minlength=len(labels))\n",
    "#         labels_to_remove = np.where(labels_train_counts < 1)[0] # remove labels with less than 2 examples\n",
    "#         labels_to_remove = np.union1d(labels_to_remove, np.where(labels_test_counts < 1)[0])\n",
    "#         # dataset_specific['train'] = dataset_specific['train'].filter(lambda x: x[label_type] not in labels_to_remove)\n",
    "#         dataset_specific['test'] = dataset_specific['test'].filter(lambda x: x[label_type] not in labels_to_remove)\n",
    "\n",
    "#         id2label = {int(i): label for i, label in enumerate(labels)}\n",
    "#         label2id = {label : int(i) for i, label in enumerate(labels)}\n",
    "\n",
    "#         dataset_specific['train'] = dataset_specific['train'].filter(lambda x: id2label[x[label_type]] not in [\"NaN\"])\n",
    "#         dataset_specific['test'] = dataset_specific['test'].filter(lambda x: id2label[x[label_type]] not in [\"NaN\"])\n",
    "\n",
    "#         id2label = {int(i): label for i, label in enumerate(labels)}\n",
    "#         label2id = {label : int(i) for i, label in enumerate(labels)}\n",
    "        \n",
    "#         labels_train_counts = np.bincount(dataset_specific['train'][label_type] , minlength=len(labels))\n",
    "#         labels_test_counts = np.bincount(dataset_specific['test'][label_type] , minlength=len(labels))\n",
    "\n",
    "#         labels = [id2label[label2id[x]] for x in labels]\n",
    "        \n",
    "\n",
    "#         def transforms(examples):\n",
    "#             examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"img_path\"]]\n",
    "#             examples[\"labels\"] = examples[label_type]\n",
    "#             del examples[label_type]\n",
    "#             del examples[\"img_path\"]\n",
    "#             return examples\n",
    "#         data_collator = DefaultDataCollator()\n",
    "\n",
    "#         model = ResNetForImageClassification.from_pretrained(\n",
    "#             checkpoint,\n",
    "#             num_labels=len(labels),\n",
    "#             id2label=id2label,\n",
    "#             label2id=label2id,\n",
    "#             use_auth_token=access_token,\n",
    "#             ignore_mismatched_sizes=True,\n",
    "#         )\n",
    "\n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir=model_dir+name,\n",
    "#             report_to=\"wandb\",\n",
    "#             remove_unused_columns=False,\n",
    "#             evaluation_strategy=\"epoch\",\n",
    "#             logging_strategy=\"epoch\",\n",
    "#             save_strategy=\"epoch\",\n",
    "#             # eval_steps = 10,\n",
    "#             # logging_steps = 10,\n",
    "#             # save_steps = 10,\n",
    "#             save_total_limit=30,\n",
    "#             learning_rate=5e-5,\n",
    "#             per_device_train_batch_size=batch_size,\n",
    "#             gradient_accumulation_steps=1,\n",
    "#             per_device_eval_batch_size=batch_size,\n",
    "#             num_train_epochs=EPOCHS,\n",
    "#             warmup_ratio=0.1,\n",
    "#             load_best_model_at_end=True,\n",
    "#             metric_for_best_model=\"f1\",\n",
    "#             # label_smoothing_factor=0.1,\n",
    "#             # no_cuda=True\n",
    "#             # push_to_hub=True,\n",
    "#             # hub_strategy=\"end\",\n",
    "#             # hub_model_id=\"boats_dataset\",\n",
    "#             # hub_token=write_token,\n",
    "#         )\n",
    "\n",
    "#         trainer = Trainer(\n",
    "#             model=model,\n",
    "#             args=training_args,\n",
    "#             data_collator=data_collator,\n",
    "#             train_dataset=dataset_specific[\"train\"].with_transform(transforms),\n",
    "#             eval_dataset=dataset_specific[\"test\"].with_transform(transforms),\n",
    "#             tokenizer=image_processor,\n",
    "#             compute_metrics=compute_metrics,\n",
    "#         )\n",
    "#         # Plot Label Distribution For Training Data\n",
    "#         fig1 = plt.figure()\n",
    "#         ax = fig1.add_axes([0,0,1,1])\n",
    "#         ax.bar([label2id[x] for x in labels], labels_train_counts/labels_train_counts.sum()) # Normalized\n",
    "#         ax.set_ylabel(\"Number of examples normalised\")\n",
    "#         ax.set_title(\"Label Distribution\")\n",
    "#         wandb.log({\"Label Distribution Train\": (fig1)})\n",
    "\n",
    "#         # Plot Label Distribution For Test Data\n",
    "#         fig2 = plt.figure()\n",
    "#         ax = fig2.add_axes([0,0,1,1])\n",
    "#         ax.bar([label2id[x] for x in labels], labels_test_counts/labels_test_counts.sum()) # Normalized\n",
    "#         ax.set_ylabel(\"Number of examples normalised\")\n",
    "#         ax.set_title(\"Label Distribution\")\n",
    "#         wandb.log({\"Label Distribution Test\": (fig2)})\n",
    "\n",
    "#         # Log label2id\n",
    "#         wandb.log({\"Labels\": wandb.Table(data = list(zip(label2id.keys() , label2id.values())) , columns=[\"Label\" , \"ID\"])})\n",
    "\n",
    "#         # Train Model\n",
    "#         trainer.train()\n",
    "\n",
    "#         # Save Model\n",
    "#         trainer.save_model(model_dir+name)\n",
    "\n",
    "#         pipeline = ImageClassificationPipeline(model=trainer.model, feature_extractor = trainer.tokenizer , framework=\"pt\", device=0)\n",
    "#         predict_data = dataset_specific['test'].select(np.random.randint(0, len(dataset_specific['test']), batch_size))\n",
    "#         images = [predict_data['img_path'][i] for i in range(batch_size)]\n",
    "#         predictions = pipeline(images)\n",
    "#         prediction_table = []\n",
    "#         for i in range(len(predictions)):\n",
    "#             prediction_table.append([wandb.Image(images[i]) , predictions[i] , id2label[predict_data[label_type][i]]])\n",
    "#         columns = [\"Image\" , \"Label Predictions\" , \"True Label\"]\n",
    "#         wandb.log({\"Image Predicitions\" : wandb.Table(data=prediction_table, columns=columns)})\n",
    "\n",
    "#         # Plot confusion matrix\n",
    "#         y_pred = trainer.predict(dataset_specific['test'].with_transform(transforms)).predictions.argmax(-1)\n",
    "#         y_true = dataset_specific[\"test\"][label_type]\n",
    "#         wandb.log({\"Confusion Matrix\": wandb.sklearn.plot_confusion_matrix(y_true, y_pred, labels=labels)})\n",
    "#         wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_image_search = load_dataset(\"cringgaard/boats_dataset\" , use_auth_token=access_token, split=\"image_search\")\n",
    "# for batch_size in batch_sizes:\n",
    "#     for label_type in label_types:\n",
    "#         tags = [model_name , label_type , 'image_search' , 'EPOCHS'+str(EPOCHS)]\n",
    "#         name = \"_\".join(tags)\n",
    "#         wandb.init(project=wandb_project, name=name , group = label_type , tags = tags)\n",
    "#         torch.cuda.empty_cache()\n",
    "#         c_names = dataset.column_names[1:]\n",
    "#         c_names.remove(label_type)\n",
    "#         dataset_specific = dataset.remove_columns(c_names)\n",
    "#         labels = dataset.features[label_type].names\n",
    "#         dataset_image_search_specific = dataset_image_search.remove_columns(c_names)\n",
    "\n",
    "#         dataset_specific = dataset_specific.train_test_split(test_size=0.2, shuffle=True, seed=43)  # 80-20 split for train and test\n",
    "#         dataset_specific['train'] = concatenate_datasets([dataset_specific['train'] , dataset_image_search_specific]) # add image_search dataset to training set\n",
    "        \n",
    "#         labels_train_counts = np.bincount(dataset_specific['train'][label_type] , minlength=len(labels))\n",
    "#         labels_test_counts = np.bincount(dataset_specific['test'][label_type] , minlength=len(labels))\n",
    "#         labels_to_remove = np.where(labels_train_counts < 1)[0] # remove labels with less than 2 examples\n",
    "#         labels_to_remove = np.union1d(labels_to_remove, np.where(labels_test_counts < 1)[0])\n",
    "#         # dataset_specific['train'] = dataset_specific['train'].filter(lambda x: x[label_type] not in labels_to_remove)\n",
    "#         dataset_specific['test'] = dataset_specific['test'].filter(lambda x: x[label_type] not in labels_to_remove)\n",
    "\n",
    "#         id2label = {int(i): label for i, label in enumerate(labels)}\n",
    "#         label2id = {label : int(i) for i, label in enumerate(labels)}\n",
    "\n",
    "#         dataset_specific['train'] = dataset_specific['train'].filter(lambda x: id2label[x[label_type]] not in [\"NaN\"])\n",
    "#         dataset_specific['test'] = dataset_specific['test'].filter(lambda x: id2label[x[label_type]] not in [\"NaN\"])\n",
    "\n",
    "#         id2label = {int(i): label for i, label in enumerate(labels)}\n",
    "#         label2id = {label : int(i) for i, label in enumerate(labels)}\n",
    "        \n",
    "#         labels_train_counts = np.bincount(dataset_specific['train'][label_type] , minlength=len(labels))\n",
    "#         labels_test_counts = np.bincount(dataset_specific['test'][label_type] , minlength=len(labels))\n",
    "\n",
    "#         labels = [id2label[label2id[x]] for x in labels]\n",
    "        \n",
    "\n",
    "#         def transforms(examples):\n",
    "#             examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"img_path\"]]\n",
    "#             examples[\"labels\"] = examples[label_type]\n",
    "#             del examples[label_type]\n",
    "#             del examples[\"img_path\"]\n",
    "#             return examples\n",
    "#         data_collator = DefaultDataCollator()\n",
    "\n",
    "#         model = ResNetForImageClassification.from_pretrained(\n",
    "#             checkpoint,\n",
    "#             num_labels=len(labels),\n",
    "#             id2label=id2label,\n",
    "#             label2id=label2id,\n",
    "#             use_auth_token=access_token,\n",
    "#             ignore_mismatched_sizes=True,\n",
    "#         )\n",
    "\n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir=model_dir+name,\n",
    "#             report_to=\"wandb\",\n",
    "#             remove_unused_columns=False,\n",
    "#             evaluation_strategy=\"epoch\",\n",
    "#             logging_strategy=\"epoch\",\n",
    "#             save_strategy=\"epoch\",\n",
    "#             # eval_steps = 10,\n",
    "#             # logging_steps = 10,\n",
    "#             # save_steps = 10,\n",
    "#             save_total_limit=30,\n",
    "#             learning_rate=5e-5,\n",
    "#             per_device_train_batch_size=batch_size,\n",
    "#             gradient_accumulation_steps=1,\n",
    "#             per_device_eval_batch_size=batch_size,\n",
    "#             num_train_epochs=EPOCHS,\n",
    "#             warmup_ratio=0.1,\n",
    "#             load_best_model_at_end=True,\n",
    "#             metric_for_best_model=\"f1\",\n",
    "#             # label_smoothing_factor=0.1,\n",
    "#             # no_cuda=True\n",
    "#             # push_to_hub=True,\n",
    "#             # hub_strategy=\"end\",\n",
    "#             # hub_model_id=\"boats_dataset\",\n",
    "#             # hub_token=write_token,\n",
    "#         )\n",
    "\n",
    "#         trainer = Trainer(\n",
    "#             model=model,\n",
    "#             args=training_args,\n",
    "#             data_collator=data_collator,\n",
    "#             train_dataset=dataset_specific[\"train\"].with_transform(transforms),\n",
    "#             eval_dataset=dataset_specific[\"test\"].with_transform(transforms),\n",
    "#             tokenizer=image_processor,\n",
    "#             compute_metrics=compute_metrics,\n",
    "#         )\n",
    "#         # Plot Label Distribution For Training Data\n",
    "#         fig1 = plt.figure()\n",
    "#         ax = fig1.add_axes([0,0,1,1])\n",
    "#         ax.bar([label2id[x] for x in labels], labels_train_counts/labels_train_counts.sum()) # Normalized\n",
    "#         ax.set_ylabel(\"Number of examples normalised\")\n",
    "#         ax.set_title(\"Label Distribution\")\n",
    "#         wandb.log({\"Label Distribution Train\": (fig1)})\n",
    "\n",
    "#         # Plot Label Distribution For Test Data\n",
    "#         fig2 = plt.figure()\n",
    "#         ax = fig2.add_axes([0,0,1,1])\n",
    "#         ax.bar([label2id[x] for x in labels], labels_test_counts/labels_test_counts.sum()) # Normalized\n",
    "#         ax.set_ylabel(\"Number of examples normalised\")\n",
    "#         ax.set_title(\"Label Distribution\")\n",
    "#         wandb.log({\"Label Distribution Test\": (fig2)})\n",
    "\n",
    "#         # Log label2id\n",
    "#         wandb.log({\"Labels\": wandb.Table(data = list(zip(label2id.keys() , label2id.values())) , columns=[\"Label\" , \"ID\"])})\n",
    "\n",
    "#         # Train Model\n",
    "#         trainer.train()\n",
    "\n",
    "#         # Save Model\n",
    "#         trainer.save_model(model_dir+name)\n",
    "\n",
    "#         pipeline = ImageClassificationPipeline(model=trainer.model, feature_extractor = trainer.tokenizer , framework=\"pt\", device=0)\n",
    "#         predict_data = dataset_specific['test'].select(np.random.randint(0, len(dataset_specific['test']), 100))\n",
    "#         images = [predict_data['img_path'][i] for i in range(100)]\n",
    "#         predictions = pipeline(images)\n",
    "#         prediction_table = []\n",
    "#         for i in range(len(predictions)):\n",
    "#             prediction_table.append([wandb.Image(images[i]) , predictions[i] , id2label[predict_data[label_type][i]]])\n",
    "#         columns = [\"Image\" , \"Label Predictions\" , \"True Label\"]\n",
    "#         wandb.log({\"Image Predicitions\" : wandb.Table(data=prediction_table, columns=columns)})\n",
    "\n",
    "#         # Plot confusion matrix\n",
    "#         y_pred = trainer.predict(dataset_specific['test'].with_transform(transforms)).predictions.argmax(-1)\n",
    "#         y_true = dataset_specific[\"test\"][label_type]\n",
    "#         wandb.log({\"Confusion Matrix\": wandb.sklearn.plot_confusion_matrix(y_true, y_pred, labels=labels)})\n",
    "#         wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For subnetworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_image_search = load_dataset(\"cringgaard/boats_dataset\" , use_auth_token=access_token, split=\"image_search\")\n",
    "# for batch_size in batch_sizes:\n",
    "#     for label_type in label_types:\n",
    "#         tags = [model_name , label_type , 'image_search' , 'EPOCHS'+str(EPOCHS)]\n",
    "#         name = \"_\".join(tags)\n",
    "#         wandb.init(project=wandb_project, name=name , group = label_type , tags = tags)\n",
    "#         torch.cuda.empty_cache()\n",
    "#         c_names = dataset.column_names[1:]\n",
    "#         c_names.remove(label_type)\n",
    "#         dataset_specific = dataset_image_search.remove_columns(c_names)\n",
    "#         labels = dataset.features[label_type].names\n",
    "\n",
    "#         dataset_specific = dataset_specific.train_test_split(test_size=0.1 , seed=42)\n",
    "        \n",
    "#         labels_train_counts = np.bincount(dataset_specific['train'][label_type] , minlength=len(labels))\n",
    "#         labels_test_counts = np.bincount(dataset_specific['test'][label_type] , minlength=len(labels))\n",
    "#         labels_to_remove = np.where(labels_train_counts < 1)[0] # remove labels with less than 2 examples\n",
    "#         labels_to_remove = np.union1d(labels_to_remove, np.where(labels_test_counts < 1)[0])\n",
    "#         # dataset_specific['train'] = dataset_specific['train'].filter(lambda x: x[label_type] not in labels_to_remove)\n",
    "#         dataset_specific['test'] = dataset_specific['test'].filter(lambda x: x[label_type] not in labels_to_remove)\n",
    "\n",
    "#         id2label = {int(i): label for i, label in enumerate(labels)}\n",
    "#         label2id = {label : int(i) for i, label in enumerate(labels)}\n",
    "\n",
    "#         dataset_specific['train'] = dataset_specific['train'].filter(lambda x: id2label[x[label_type]] not in [\"NaN\"])\n",
    "#         dataset_specific['test'] = dataset_specific['test'].filter(lambda x: id2label[x[label_type]] not in [\"NaN\"])\n",
    "\n",
    "#         id2label = {int(i): label for i, label in enumerate(labels)}\n",
    "#         label2id = {label : int(i) for i, label in enumerate(labels)}\n",
    "        \n",
    "#         labels_train_counts = np.bincount(dataset_specific['train'][label_type] , minlength=len(labels))\n",
    "#         labels_test_counts = np.bincount(dataset_specific['test'][label_type] , minlength=len(labels))\n",
    "\n",
    "#         labels = [id2label[label2id[x]] for x in labels]\n",
    "        \n",
    "\n",
    "#         def transforms(examples):\n",
    "#             examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"img_path\"]]\n",
    "#             examples[\"labels\"] = examples[label_type]\n",
    "#             del examples[label_type]\n",
    "#             del examples[\"img_path\"]\n",
    "#             return examples\n",
    "#         data_collator = DefaultDataCollator()\n",
    "\n",
    "#         model = ResNetForImageClassification.from_pretrained(\n",
    "#             checkpoint,\n",
    "#             num_labels=len(labels),\n",
    "#             id2label=id2label,\n",
    "#             label2id=label2id,\n",
    "#             use_auth_token=access_token,\n",
    "#             ignore_mismatched_sizes=True,\n",
    "#         )\n",
    "\n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir=model_dir+name,\n",
    "#             report_to=\"wandb\",\n",
    "#             remove_unused_columns=False,\n",
    "#             evaluation_strategy=\"epoch\",\n",
    "#             logging_strategy=\"epoch\",\n",
    "#             save_strategy=\"epoch\",\n",
    "#             # eval_steps = 10,\n",
    "#             # logging_steps = 10,\n",
    "#             # save_steps = 10,\n",
    "#             save_total_limit=30,\n",
    "#             learning_rate=5e-5,\n",
    "#             per_device_train_batch_size=batch_size,\n",
    "#             gradient_accumulation_steps=1,\n",
    "#             per_device_eval_batch_size=batch_size,\n",
    "#             num_train_epochs=EPOCHS,\n",
    "#             warmup_ratio=0.1,\n",
    "#             load_best_model_at_end=True,\n",
    "#             metric_for_best_model=\"f1\",\n",
    "#             # label_smoothing_factor=0.1,\n",
    "#             # no_cuda=True\n",
    "#             # push_to_hub=True,\n",
    "#             # hub_strategy=\"end\",\n",
    "#             # hub_model_id=\"boats_dataset\",\n",
    "#             # hub_token=write_token,\n",
    "#         )\n",
    "\n",
    "#         trainer = Trainer(\n",
    "#             model=model,\n",
    "#             args=training_args,\n",
    "#             data_collator=data_collator,\n",
    "#             train_dataset=dataset_specific[\"train\"].with_transform(transforms),\n",
    "#             eval_dataset=dataset_specific[\"test\"].with_transform(transforms),\n",
    "#             tokenizer=image_processor,\n",
    "#             compute_metrics=compute_metrics,\n",
    "#         )\n",
    "#         # Plot Label Distribution For Training Data\n",
    "#         fig1 = plt.figure()\n",
    "#         ax = fig1.add_axes([0,0,1,1])\n",
    "#         ax.bar([label2id[x] for x in labels], labels_train_counts/labels_train_counts.sum()) # Normalized\n",
    "#         ax.set_ylabel(\"Number of examples normalised\")\n",
    "#         ax.set_title(\"Label Distribution\")\n",
    "#         wandb.log({\"Label Distribution Train\": (fig1)})\n",
    "\n",
    "#         # Plot Label Distribution For Test Data\n",
    "#         fig2 = plt.figure()\n",
    "#         ax = fig2.add_axes([0,0,1,1])\n",
    "#         ax.bar([label2id[x] for x in labels], labels_test_counts/labels_test_counts.sum()) # Normalized\n",
    "#         ax.set_ylabel(\"Number of examples normalised\")\n",
    "#         ax.set_title(\"Label Distribution\")\n",
    "#         wandb.log({\"Label Distribution Test\": (fig2)})\n",
    "\n",
    "#         # Log label2id\n",
    "#         wandb.log({\"Labels\": wandb.Table(data = list(zip(label2id.keys() , label2id.values())) , columns=[\"Label\" , \"ID\"])})\n",
    "\n",
    "#         # Train Model\n",
    "#         trainer.train()\n",
    "\n",
    "#         # Save Model\n",
    "#         trainer.save_model(model_dir+name)\n",
    "\n",
    "#         pipeline = ImageClassificationPipeline(model=trainer.model, feature_extractor = trainer.tokenizer , framework=\"pt\", device=0)\n",
    "#         predict_data = dataset_specific['test'].select(np.random.randint(0, len(dataset_specific['test']), 100))\n",
    "#         images = [predict_data['img_path'][i] for i in range(100)]\n",
    "#         predictions = pipeline(images)\n",
    "#         prediction_table = []\n",
    "#         for i in range(len(predictions)):\n",
    "#             prediction_table.append([wandb.Image(images[i]) , predictions[i] , id2label[predict_data[label_type][i]]])\n",
    "#         columns = [\"Image\" , \"Label Predictions\" , \"True Label\"]\n",
    "#         wandb.log({\"Image Predicitions\" : wandb.Table(data=prediction_table, columns=columns)})\n",
    "\n",
    "#         # Plot confusion matrix\n",
    "#         y_pred = trainer.predict(dataset_specific['test'].with_transform(transforms)).predictions.argmax(-1)\n",
    "#         y_true = dataset_specific[\"test\"][label_type]\n",
    "#         wandb.log({\"Confusion Matrix\": wandb.sklearn.plot_confusion_matrix(y_true, y_pred, labels=labels)})\n",
    "#         wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boat Class Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End to End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #launch tensorboard\n",
    "# %load_ext tensorboard\n",
    "# EPOCHS = 100\n",
    "# dataset_image_search = load_dataset(\"cringgaard/boats_dataset\" , use_auth_token=access_token, split=\"image_search\")\n",
    "# for batch_size in batch_sizes:\n",
    "#         tags = [model_name , \"Boat_Class\"]\n",
    "#         name = \"_\".join(tags)\n",
    "#         wandb.init(project=wandb_project, name=name , tags=[\"Boat_Class\"])\n",
    "#         torch.cuda.empty_cache()\n",
    "#         c_names = dataset.column_names[1:]\n",
    "#         c_names.remove('name')\n",
    "#         # NOTE boat24 has wrong labels\n",
    "#         # dataset_specific_test = dataset.remove_columns(c_names)\n",
    "#         # dataset_boat24_specific_train = dataset_boat24.remove_columns(c_names)\n",
    "#         # dataset_image_search_specific_train = dataset_image_search.remove_columns(c_names)\n",
    "#         dataset_specific = dataset_image_search.remove_columns(c_names)\n",
    "#         # Split into train and eval\n",
    "#         dataset_specific = dataset_specific.train_test_split(test_size=0.1 , seed=42)\n",
    "        \n",
    "#         labels = dataset.features['name'].names\n",
    "#         id2label = {int(i): label for i, label in enumerate(labels)}\n",
    "#         label2id = {label : int(i) for i, label in enumerate(labels)}\n",
    "\n",
    "#         labels_train_counts = np.bincount(dataset_specific['train']['name'] , minlength=len(labels))\n",
    "#         labels_test_counts = np.bincount(dataset_specific['test']['name'] , minlength=len(labels))\n",
    "\n",
    "#         # weights = np.array([1 if x == 0 else x for x in labels_train_counts])\n",
    "#         # weights = (1/weights)\n",
    "#         # weights /= weights.sum()\n",
    "#         # weights = torch.tensor(weights, dtype=torch.float , device=torch.device(\"cuda:0\"))\n",
    "\n",
    "#         # class WeightedCETrainer(Trainer):\n",
    "#         #     def __init__(self, *args, **kwargs):\n",
    "#         #         super().__init__(*args, **kwargs)\n",
    "#         #     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         #         labels = inputs.get(\"labels\")\n",
    "#         #         labels.to(torch.device(\"cuda:0\"))\n",
    "#         #         outputs = model(**inputs)\n",
    "#         #         logits = outputs.get(\"logits\")\n",
    "#         #         # loss_fct = nn.CrossEntropyLoss(weight=weights , label_smoothing=0.1)\n",
    "#         #         loss_fct = nn.CrossEntropyLoss(weight=weights)\n",
    "#         #         loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "#         #         return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "#         def transforms(examples):\n",
    "#             examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"img_path\"]]\n",
    "#             examples[\"labels\"] = examples['name']\n",
    "#             del examples[\"name\"]\n",
    "#             del examples[\"img_path\"]\n",
    "#             return examples\n",
    "#         data_collator = DefaultDataCollator()\n",
    "\n",
    "#         model = AutoModelForImageClassification.from_pretrained(\n",
    "#             checkpoint,\n",
    "#             num_labels=len(labels),\n",
    "#             id2label=id2label,\n",
    "#             label2id=label2id,\n",
    "#             use_auth_token=access_token,\n",
    "#             ignore_mismatched_sizes=True,\n",
    "#         )\n",
    "\n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir=model_dir+name,\n",
    "#             report_to=\"tensorboard\",\n",
    "#             remove_unused_columns=False,\n",
    "#             evaluation_strategy=\"epoch\",\n",
    "#             logging_strategy=\"epoch\",\n",
    "#             save_strategy=\"epoch\",\n",
    "#             # eval_steps = 10,\n",
    "#             # logging_steps = 10,\n",
    "#             # save_steps = 10,\n",
    "#             save_total_limit=30,\n",
    "#             learning_rate=5e-5,\n",
    "#             per_device_train_batch_size=batch_size,\n",
    "#             gradient_accumulation_steps=1,\n",
    "#             per_device_eval_batch_size=batch_size,\n",
    "#             num_train_epochs=EPOCHS,\n",
    "#             warmup_ratio=0.1,\n",
    "#             load_best_model_at_end=True,\n",
    "#             metric_for_best_model=\"accuracy\",\n",
    "#             # label_smoothing_factor=0.1,\n",
    "#             # no_cuda=True\n",
    "#             # push_to_hub=True,\n",
    "#             # hub_strategy=\"end\",\n",
    "#             # hub_model_id=\"boats_dataset\",\n",
    "#             # hub_token=write_token,\n",
    "#         )\n",
    "\n",
    "#         trainer = Trainer(\n",
    "#             model=model,\n",
    "#             args=training_args,\n",
    "#             data_collator=data_collator,\n",
    "#             train_dataset=dataset_specific['train'].with_transform(transforms),\n",
    "#             eval_dataset=dataset_specific['test'].with_transform(transforms),\n",
    "#             tokenizer=image_processor,\n",
    "#             compute_metrics=compute_metrics,\n",
    "#         )\n",
    "#         # # Plot Label Distribution For Training Data\n",
    "#         # fig1 = plt.figure()\n",
    "#         # ax = fig1.add_axes([0,0,1,1])\n",
    "#         # ax.bar([label2id[x] for x in labels], labels_train_counts/dataset_specific['train'].__len__()) # Normalized\n",
    "#         # ax.set_ylabel(\"Number of examples normalised\")\n",
    "#         # ax.set_title(\"Label Distribution\")\n",
    "#         # wandb.log({\"Label Distribution Train\": (fig1)})\n",
    "\n",
    "#         # # Plot Label Distribution For Test Data\n",
    "#         # fig2 = plt.figure()\n",
    "#         # ax = fig2.add_axes([0,0,1,1])\n",
    "#         # ax.bar([label2id[x] for x in labels], labels_test_counts/dataset_specific['test'].__len__()) # Normalized\n",
    "#         # ax.set_ylabel(\"Number of examples normalised\")\n",
    "#         # ax.set_title(\"Label Distribution\")\n",
    "#         # wandb.log({\"Label Distribution Test\": (fig2)})\n",
    "\n",
    "#         # # Log label2id\n",
    "#         # wandb.log({\"Labels\": wandb.Table(data = list(zip(label2id.keys() , label2id.values())) , columns=[\"Label\" , \"ID\"])})\n",
    "\n",
    "#         # Train Model\n",
    "#         trainer.train()\n",
    "\n",
    "#         # Save Model\n",
    "#         trainer.save_model(model_dir+name+\"/best_model\")\n",
    "\n",
    "#         pipeline = ImageClassificationPipeline(model=trainer.model, feature_extractor = trainer.tokenizer , framework=\"pt\", device=0)\n",
    "#         predict_data = dataset_specific['test'].select(np.random.randint(0, len(dataset_specific['test']), 100))\n",
    "#         images = [predict_data['img_path'][i] for i in range(100)]\n",
    "#         predictions = pipeline(images)\n",
    "#         prediction_table = []\n",
    "#         for i in range(len(predictions)):\n",
    "#             prediction_table.append([wandb.Image(images[i]) , predictions[i] , id2label[predict_data['name'][i]]])\n",
    "#         columns = [\"Image\" , \"Label Predictions\" , \"True Label\"]\n",
    "#         wandb.log({\"Image Predicitions\" : wandb.Table(data=prediction_table, columns=columns)})\n",
    "\n",
    "#         metrics = trainer.evaluate(dataset_specific['test'].with_transform(transforms))\n",
    "#         wandb.log(metrics)\n",
    "#         wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [model_name , label_types[0], losses[0], str(16)]\n",
    "name = \"_\".join(tags)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subnetworks Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BoatClassifier(nn.Module):\n",
    "#     def __init__(self, device):\n",
    "#         super(BoatClassifier, self).__init__()\n",
    "#         self.device = device\n",
    "#         self.subnetworks = {\n",
    "#             \"Hull Type\" : ResNetForImageClassification.from_pretrained(model_dir+\"ResNet18_Hull Type_image_search_EPOCHS20\"),\n",
    "#             \"Rigging Type\": ResNetForImageClassification.from_pretrained(model_dir+\"ResNet18_Rigging Type_image_search_EPOCHS20\"),\n",
    "#             \"Construction\": ResNetForImageClassification.from_pretrained(model_dir+\"ResNet18_Construction_image_search_EPOCHS20\"),\n",
    "#             \"Ballast Type\": ResNetForImageClassification.from_pretrained(model_dir+\"ResNet18_Ballast Type_image_search_EPOCHS20\"),\n",
    "#             \"Designer\": ResNetForImageClassification.from_pretrained(model_dir+\"ResNet18_Designer_image_search_EPOCHS20\"),\n",
    "#         }\n",
    "#         subnetwork_output_size = 0\n",
    "#         for key in self.subnetworks.keys():\n",
    "#             for param in self.subnetworks[key].parameters():\n",
    "#                 param.requires_grad = False\n",
    "#             self.subnetworks[key].to(self.device)\n",
    "#             subnetwork_output_size += self.subnetworks[key].classifier[1].out_features\n",
    "\n",
    "#         self.object_model = ResNetModel.from_pretrained(checkpoint)\n",
    "#         self.object_model.to(self.device)\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             # nn.Flatten() ,\n",
    "#             nn.Linear(subnetwork_output_size+self.object_model.config.hidden_sizes[-1] , Name_Classes.__len__())\n",
    "#         )\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#     def forward(self, **inputs):\n",
    "#         # Run through the 5 subnetworks and get the outputs without gradients\n",
    "#         with torch.no_grad():\n",
    "#             subnetwork_outputs = []\n",
    "#             for key in self.subnetworks.keys():\n",
    "#                 subnetwork_outputs.append((self.subnetworks[key].forward(inputs['pixel_values']).logits))\n",
    "#         # Run through the object model and get the output with gradients\n",
    "#         object_model_output = self.object_model(inputs['pixel_values'])\n",
    "#         # Concatenate the outputs\n",
    "#         x = torch.cat(subnetwork_outputs + [self.flatten(object_model_output.pooler_output)] , dim=1)\n",
    "\n",
    "\n",
    "#         # Run through the linear layers\n",
    "#         x = self.classifier(x)\n",
    "#         # Run through softmax\n",
    "#         x = self.softmax(x)\n",
    "#         if inputs['labels'] is not None:\n",
    "#             loss = nn.CrossEntropyLoss()(x, inputs['labels'])\n",
    "#             return {\"loss\": loss, \"logits\": x}\n",
    "#         return {\"logits\": x}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #launch tensorboard\n",
    "# %load_ext tensorboard\n",
    "# EPOCHS = 100\n",
    "# dataset_image_search = load_dataset(\"cringgaard/boats_dataset\" , use_auth_token=access_token, split=\"image_search\")\n",
    "# for batch_size in batch_sizes:\n",
    "#         tags = [model_name , \"Boat_Class\" , \"Subnetworks\"]\n",
    "#         name = \"_\".join(tags)\n",
    "#         wandb.init(project=wandb_project, name=name , tags=[\"Boat_Class\"])\n",
    "#         torch.cuda.empty_cache()\n",
    "#         c_names = dataset.column_names[1:]\n",
    "#         c_names.remove('name')\n",
    "#         # NOTE boat24 has wrong labels\n",
    "#         # dataset_specific_test = dataset.remove_columns(c_names)\n",
    "#         # dataset_boat24_specific_train = dataset_boat24.remove_columns(c_names)\n",
    "#         # dataset_image_search_specific_train = dataset_image_search.remove_columns(c_names)\n",
    "#         dataset_specific = dataset_image_search.remove_columns(c_names)\n",
    "\n",
    "#         # Debugging dataset\n",
    "#         dataset_specific = dataset_specific.select(range(1000))\n",
    "\n",
    "#         # Split into train and eval\n",
    "#         dataset_specific = dataset_specific.train_test_split(test_size=0.1 , seed=42)\n",
    "        \n",
    "#         labels = dataset.features['name'].names\n",
    "#         id2label = {int(i): label for i, label in enumerate(labels)}\n",
    "#         label2id = {label : int(i) for i, label in enumerate(labels)}\n",
    "\n",
    "#         labels_train_counts = np.bincount(dataset_specific['train']['name'] , minlength=len(labels))\n",
    "#         labels_test_counts = np.bincount(dataset_specific['test']['name'] , minlength=len(labels))\n",
    "\n",
    "#         # weights = np.array([1 if x == 0 else x for x in labels_train_counts])\n",
    "#         # weights = (1/weights)\n",
    "#         # weights /= weights.sum()\n",
    "#         # weights = torch.tensor(weights, dtype=torch.float , device=torch.device(\"cuda:0\"))\n",
    "\n",
    "#         # class WeightedCETrainer(Trainer):\n",
    "#         #     def __init__(self, *args, **kwargs):\n",
    "#         #         super().__init__(*args, **kwargs)\n",
    "#         #     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         #         labels = inputs.get(\"labels\")\n",
    "#         #         labels.to(torch.device(\"cuda:0\"))\n",
    "#         #         outputs = model(**inputs)\n",
    "#         #         logits = outputs.get(\"logits\")\n",
    "#         #         # loss_fct = nn.CrossEntropyLoss(weight=weights , label_smoothing=0.1)\n",
    "#         #         loss_fct = nn.CrossEntropyLoss(weight=weights)\n",
    "#         #         loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "#         #         return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "#         def transforms(examples):\n",
    "#             examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"img_path\"]]\n",
    "#             examples[\"labels\"] = examples['name']\n",
    "#             del examples[\"name\"]\n",
    "#             del examples[\"img_path\"]\n",
    "#             return examples\n",
    "        \n",
    "#         data_collator = DefaultDataCollator()\n",
    "#         device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#         model = BoatClassifier(device)\n",
    "\n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir=model_dir+name,\n",
    "#             report_to=\"tensorboard\",\n",
    "#             remove_unused_columns=False,\n",
    "#             evaluation_strategy=\"epoch\",\n",
    "#             logging_strategy=\"epoch\",\n",
    "#             save_strategy=\"epoch\",\n",
    "#             # eval_steps = 10,\n",
    "#             # logging_steps = 10,\n",
    "#             # save_steps = 10,\n",
    "#             save_total_limit=30,\n",
    "#             learning_rate=5e-5,\n",
    "#             per_device_train_batch_size=batch_size,\n",
    "#             gradient_accumulation_steps=1,\n",
    "#             per_device_eval_batch_size=batch_size,\n",
    "#             num_train_epochs=EPOCHS,\n",
    "#             warmup_ratio=0.1,\n",
    "#             load_best_model_at_end=True,\n",
    "#             metric_for_best_model=\"accuracy\",\n",
    "#             # label_smoothing_factor=0.1,\n",
    "#             # no_cuda=True\n",
    "#             # push_to_hub=True,\n",
    "#             # hub_strategy=\"end\",\n",
    "#             # hub_model_id=\"boats_dataset\",\n",
    "#             # hub_token=write_token,\n",
    "#         )\n",
    "\n",
    "#         trainer = Trainer(\n",
    "#             model=model,\n",
    "#             args=training_args,\n",
    "#             data_collator=data_collator,\n",
    "#             train_dataset=dataset_specific['train'].with_transform(transforms),\n",
    "#             eval_dataset=dataset_specific['test'].with_transform(transforms),\n",
    "#             tokenizer=image_processor,\n",
    "#             compute_metrics=compute_metrics,\n",
    "#         )\n",
    "#         # # Plot Label Distribution For Training Data\n",
    "#         # fig1 = plt.figure()\n",
    "#         # ax = fig1.add_axes([0,0,1,1])\n",
    "#         # ax.bar([label2id[x] for x in labels], labels_train_counts/dataset_specific['train'].__len__()) # Normalized\n",
    "#         # ax.set_ylabel(\"Number of examples normalised\")\n",
    "#         # ax.set_title(\"Label Distribution\")\n",
    "#         # wandb.log({\"Label Distribution Train\": (fig1)})\n",
    "\n",
    "#         # # Plot Label Distribution For Test Data\n",
    "#         # fig2 = plt.figure()\n",
    "#         # ax = fig2.add_axes([0,0,1,1])\n",
    "#         # ax.bar([label2id[x] for x in labels], labels_test_counts/dataset_specific['test'].__len__()) # Normalized\n",
    "#         # ax.set_ylabel(\"Number of examples normalised\")\n",
    "#         # ax.set_title(\"Label Distribution\")\n",
    "#         # wandb.log({\"Label Distribution Test\": (fig2)})\n",
    "\n",
    "#         # # Log label2id\n",
    "#         # wandb.log({\"Labels\": wandb.Table(data = list(zip(label2id.keys() , label2id.values())) , columns=[\"Label\" , \"ID\"])})\n",
    "\n",
    "#         # Train Model\n",
    "#         trainer.train()\n",
    "\n",
    "#         # Save Model\n",
    "#         trainer.save_model(model_dir+name+\"/best_model\")\n",
    "\n",
    "#         pipeline = ImageClassificationPipeline(model=trainer.model, feature_extractor = trainer.tokenizer , framework=\"pt\", device=0)\n",
    "#         predict_data = dataset_specific['test'].select(np.random.randint(0, len(dataset_specific['test']), 100))\n",
    "#         images = [predict_data['img_path'][i] for i in range(100)]\n",
    "#         predictions = pipeline(images)\n",
    "#         prediction_table = []\n",
    "#         for i in range(len(predictions)):\n",
    "#             prediction_table.append([wandb.Image(images[i]) , predictions[i] , id2label[predict_data['name'][i]]])\n",
    "#         columns = [\"Image\" , \"Label Predictions\" , \"True Label\"]\n",
    "#         wandb.log({\"Image Predicitions\" : wandb.Table(data=prediction_table, columns=columns)})\n",
    "\n",
    "#         metrics = trainer.evaluate(dataset_specific['test'].with_transform(transforms))\n",
    "#         wandb.log(metrics)\n",
    "#         wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset boats_dataset (C:\\Users\\chris\\.cache\\huggingface\\datasets\\cringgaard___boats_dataset\\default\\0.0.0\\3780f35f24ee5458f59c11c69640a6f7f9001aaabc6ce51227831bd076a1ce4e)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcringgaard\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\chris\\OneDrive\\Dokumenter\\GitHub\\SailFGVC\\wandb\\run-20230602_164712-3cuxghqj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/cringgaard/Sailboat%20FGVC%20Clean/runs/3cuxghqj\" target=\"_blank\">ViT_Boat_Class_ViT</a></strong> to <a href=\"https://wandb.ai/cringgaard/Sailboat%20FGVC%20Clean\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\cringgaard___boats_dataset\\default\\0.0.0\\3780f35f24ee5458f59c11c69640a6f7f9001aaabc6ce51227831bd076a1ce4e\\cache-6ba3f8e86ac10bfb.arrow and C:\\Users\\chris\\.cache\\huggingface\\datasets\\cringgaard___boats_dataset\\default\\0.0.0\\3780f35f24ee5458f59c11c69640a6f7f9001aaabc6ce51227831bd076a1ce4e\\cache-2d2e014a9e7eea82.arrow\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([7108, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([7108]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 73926\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 462100\n",
      "  Number of trainable parameters = 91264708\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef46d131d4d484691ed4c0381113788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/462100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.956, 'learning_rate': 5e-06, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef73f667b48484480cbfd2ecbcc83ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-4621\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-4621\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.884149551391602, 'eval_accuracy': 0.0, 'eval_f1': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_runtime': 185.3746, 'eval_samples_per_second': 44.31, 'eval_steps_per_second': 2.773, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-4621\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-4621\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.8007, 'learning_rate': 1e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47754655b6e144c69cefd00120945e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-9242\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-9242\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.747682571411133, 'eval_accuracy': 0.0008522035549062576, 'eval_f1': 0.00018491675598685035, 'eval_precision': 0.00022341446980268025, 'eval_recall': 0.0009232264334305151, 'eval_runtime': 124.216, 'eval_samples_per_second': 66.127, 'eval_steps_per_second': 4.138, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-9242\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-9242\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.4755, 'learning_rate': 1.5e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5b2ad5ddf940ecafb163ba80497106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-13863\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-13863\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.389898300170898, 'eval_accuracy': 0.003165327489651814, 'eval_f1': 0.0014139176666663207, 'eval_precision': 0.0014220326084224434, 'eval_recall': 0.003283755070504153, 'eval_runtime': 141.5598, 'eval_samples_per_second': 58.025, 'eval_steps_per_second': 3.631, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-13863\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-13863\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.0246, 'learning_rate': 2e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464aab10f2fc4338a2ac4f9b6a237833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-18484\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-18484\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.025288581848145, 'eval_accuracy': 0.008400292184075968, 'eval_f1': 0.0038171201122529376, 'eval_precision': 0.0037192121547170516, 'eval_recall': 0.008524904214559387, 'eval_runtime': 233.913, 'eval_samples_per_second': 35.116, 'eval_steps_per_second': 2.197, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-18484\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-18484\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.5835, 'learning_rate': 2.5e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14375931649a4ae9b755eed07860ac0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-23105\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-23105\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.7087016105651855, 'eval_accuracy': 0.01643535427319211, 'eval_f1': 0.008287111246320632, 'eval_precision': 0.008237709949389186, 'eval_recall': 0.01645766039647856, 'eval_runtime': 123.8366, 'eval_samples_per_second': 66.329, 'eval_steps_per_second': 4.151, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-23105\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-23105\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.1042, 'learning_rate': 3e-05, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f602724918424ab17b84258122269d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-27726\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-27726\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.349783420562744, 'eval_accuracy': 0.030679327976625273, 'eval_f1': 0.0165313811861633, 'eval_precision': 0.01605155580011795, 'eval_recall': 0.030596642326655432, 'eval_runtime': 140.2243, 'eval_samples_per_second': 58.578, 'eval_steps_per_second': 3.666, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-27726\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-27726\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.5783, 'learning_rate': 3.5e-05, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe2f7af7f784adf9e95b5205af68c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-32347\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-32347\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.032502174377441, 'eval_accuracy': 0.04723642561480399, 'eval_f1': 0.027745165904315642, 'eval_precision': 0.028170801163112207, 'eval_recall': 0.04366469937292512, 'eval_runtime': 145.1522, 'eval_samples_per_second': 56.589, 'eval_steps_per_second': 3.541, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-32347\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-32347\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.9949, 'learning_rate': 4e-05, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7cbe73d29e495caa85d04c7ea9532c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-36968\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-36968\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.701854705810547, 'eval_accuracy': 0.07219381543705868, 'eval_f1': 0.04427740475095939, 'eval_precision': 0.0444026092069138, 'eval_recall': 0.06635802469135803, 'eval_runtime': 144.2117, 'eval_samples_per_second': 56.958, 'eval_steps_per_second': 3.564, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-36968\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-36968\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.366, 'learning_rate': 4.5e-05, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec51c259dbc74d3dbc1c405082453270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-41589\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-41589\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.386385917663574, 'eval_accuracy': 0.09934258582907232, 'eval_f1': 0.06429219435307354, 'eval_precision': 0.06683799032006779, 'eval_recall': 0.08842293906810035, 'eval_runtime': 146.5167, 'eval_samples_per_second': 56.062, 'eval_steps_per_second': 3.508, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-41589\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-41589\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6858, 'learning_rate': 5e-05, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c6ea9f4dd44dc58f5ded20be27fbd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-46210\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-46210\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.117397308349609, 'eval_accuracy': 0.1212563915266618, 'eval_f1': 0.08105644139440364, 'eval_precision': 0.0836395402165243, 'eval_recall': 0.10524904214559386, 'eval_runtime': 144.1045, 'eval_samples_per_second': 57.0, 'eval_steps_per_second': 3.567, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-46210\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-46210\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9775, 'learning_rate': 4.9444444444444446e-05, 'epoch': 11.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d91780cb5844e4cb1ae094cb34e1173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-50831\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-50831\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.897875785827637, 'eval_accuracy': 0.1434136839542245, 'eval_f1': 0.09927230615809937, 'eval_precision': 0.10446225452609308, 'eval_recall': 0.12379657249809908, 'eval_runtime': 144.0862, 'eval_samples_per_second': 57.008, 'eval_steps_per_second': 3.567, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-50831\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-50831\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2707, 'learning_rate': 4.888888888888889e-05, 'epoch': 12.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4145d97e449f4557854c0d2a8153b434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-55452\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-55452\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.755012512207031, 'eval_accuracy': 0.15631848064280496, 'eval_f1': 0.10964192311636815, 'eval_precision': 0.11749260844511232, 'eval_recall': 0.13096139505535478, 'eval_runtime': 146.3388, 'eval_samples_per_second': 56.13, 'eval_steps_per_second': 3.512, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-55452\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-55452\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6499, 'learning_rate': 4.8333333333333334e-05, 'epoch': 13.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb7c8bdf5a4457da917ea3e308fcc00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-60073\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-60073\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.716735363006592, 'eval_accuracy': 0.17348429510591673, 'eval_f1': 0.12397607123135983, 'eval_precision': 0.13303324782951856, 'eval_recall': 0.14661889963167585, 'eval_runtime': 150.321, 'eval_samples_per_second': 54.643, 'eval_steps_per_second': 3.419, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-60073\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-60073\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0933, 'learning_rate': 4.7777777777777784e-05, 'epoch': 14.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f00903777d44d8994139803e594579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-64694\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-64694\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.6765666007995605, 'eval_accuracy': 0.18298027757487217, 'eval_f1': 0.13111575592817065, 'eval_precision': 0.1411243268252136, 'eval_recall': 0.1518332196452933, 'eval_runtime': 153.3478, 'eval_samples_per_second': 53.565, 'eval_steps_per_second': 3.352, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-64694\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-64694\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6502, 'learning_rate': 4.722222222222222e-05, 'epoch': 15.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddaba76eed14d7c893595fbe7fe7f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-69315\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-69315\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.651655197143555, 'eval_accuracy': 0.18882395909422936, 'eval_f1': 0.13335859750954088, 'eval_precision': 0.1447393485129334, 'eval_recall': 0.15339679301943454, 'eval_runtime': 148.1412, 'eval_samples_per_second': 55.447, 'eval_steps_per_second': 3.47, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-69315\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-69315\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3132, 'learning_rate': 4.666666666666667e-05, 'epoch': 16.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c8b391d3364b3ebf4f089eb290020d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-73936\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-73936\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.719595909118652, 'eval_accuracy': 0.18188458728999268, 'eval_f1': 0.12945872236287356, 'eval_precision': 0.14119544487836758, 'eval_recall': 0.14697941275734053, 'eval_runtime': 147.358, 'eval_samples_per_second': 55.742, 'eval_steps_per_second': 3.488, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-73936\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-73936\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0692, 'learning_rate': 4.6111111111111115e-05, 'epoch': 17.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9eaead34b24e558cd3497f44bb71ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-78557\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-78557\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.7837066650390625, 'eval_accuracy': 0.19588507426345264, 'eval_f1': 0.14200352598790097, 'eval_precision': 0.1556167507763975, 'eval_recall': 0.1591881793478261, 'eval_runtime': 147.4394, 'eval_samples_per_second': 55.711, 'eval_steps_per_second': 3.486, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-78557\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-78557\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8999, 'learning_rate': 4.555555555555556e-05, 'epoch': 18.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95cfb30dcf944d90a4f0e506aa7d85ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-83178\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-83178\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.821348190307617, 'eval_accuracy': 0.19332846359873387, 'eval_f1': 0.13768478326829955, 'eval_precision': 0.1499040334583317, 'eval_recall': 0.1580886433276232, 'eval_runtime': 151.8091, 'eval_samples_per_second': 54.107, 'eval_steps_per_second': 3.386, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-83178\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-83178\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7804, 'learning_rate': 4.5e-05, 'epoch': 19.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8f94f9addd4e94b81a97b8c16e0ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-87799\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-87799\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.872054576873779, 'eval_accuracy': 0.18675432188945704, 'eval_f1': 0.13613220103856175, 'eval_precision': 0.14961696674620584, 'eval_recall': 0.15272481862662393, 'eval_runtime': 147.8969, 'eval_samples_per_second': 55.539, 'eval_steps_per_second': 3.475, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-87799\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-87799\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6916, 'learning_rate': 4.4444444444444447e-05, 'epoch': 20.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ff7f56043644fd9425fb47a16f66c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-92420\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-92420\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.920873641967773, 'eval_accuracy': 0.19284149013878743, 'eval_f1': 0.1375141180438185, 'eval_precision': 0.14831847840394125, 'eval_recall': 0.15710498110227336, 'eval_runtime': 146.3278, 'eval_samples_per_second': 56.134, 'eval_steps_per_second': 3.513, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-92420\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-92420\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.636, 'learning_rate': 4.388888888888889e-05, 'epoch': 21.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef995525f4f4b1ca9a2ec50042d857b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-97041\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-97041\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.961556434631348, 'eval_accuracy': 0.19028487947406866, 'eval_f1': 0.13828346046889092, 'eval_precision': 0.1496341039387397, 'eval_recall': 0.15673572196751004, 'eval_runtime': 145.238, 'eval_samples_per_second': 56.555, 'eval_steps_per_second': 3.539, 'epoch': 21.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-97041\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-97041\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5919, 'learning_rate': 4.3333333333333334e-05, 'epoch': 22.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec44a99317d40c68dc4918f87fe26f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-101662\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-101662\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.0099897384643555, 'eval_accuracy': 0.19612856099342585, 'eval_f1': 0.14167390388143186, 'eval_precision': 0.15546377532644062, 'eval_recall': 0.15875720583248557, 'eval_runtime': 147.7768, 'eval_samples_per_second': 55.584, 'eval_steps_per_second': 3.478, 'epoch': 22.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-101662\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-101662\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.537, 'learning_rate': 4.277777777777778e-05, 'epoch': 23.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940f474e0bff4ced8520a24c0cfc2d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-106283\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-106283\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.0364179611206055, 'eval_accuracy': 0.19418066715364013, 'eval_f1': 0.14261984001868394, 'eval_precision': 0.15676053658457603, 'eval_recall': 0.1596820809248555, 'eval_runtime': 150.1167, 'eval_samples_per_second': 54.717, 'eval_steps_per_second': 3.424, 'epoch': 23.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-106283\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-106283\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5047, 'learning_rate': 4.222222222222222e-05, 'epoch': 24.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b03562b95564fe29a93f5ac457d070d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-110904\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-110904\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.088176250457764, 'eval_accuracy': 0.19625030435841245, 'eval_f1': 0.1405501458248711, 'eval_precision': 0.15220096753063786, 'eval_recall': 0.15908706677937448, 'eval_runtime': 145.2411, 'eval_samples_per_second': 56.554, 'eval_steps_per_second': 3.539, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-110904\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-110904\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4901, 'learning_rate': 4.166666666666667e-05, 'epoch': 25.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830c30dd94b6407690641a8e3cb7391f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-115525\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-115525\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.06094217300415, 'eval_accuracy': 0.1983199415631848, 'eval_f1': 0.1430921134665628, 'eval_precision': 0.1569611861462082, 'eval_recall': 0.16051338529312095, 'eval_runtime': 147.1304, 'eval_samples_per_second': 55.828, 'eval_steps_per_second': 3.493, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-115525\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-115525\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4541, 'learning_rate': 4.111111111111111e-05, 'epoch': 26.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a54b431fb942eaa9f7e9796aed40ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-120146\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-120146\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.156139373779297, 'eval_accuracy': 0.19357195032870708, 'eval_f1': 0.14134733530169497, 'eval_precision': 0.15700574497406922, 'eval_recall': 0.15755846957311534, 'eval_runtime': 149.6441, 'eval_samples_per_second': 54.89, 'eval_steps_per_second': 3.435, 'epoch': 26.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-120146\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-120146\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4398, 'learning_rate': 4.055555555555556e-05, 'epoch': 27.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef60b9649dd348d7aea4860137f4181f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-124767\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-124767\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.156641483306885, 'eval_accuracy': 0.19235451667884101, 'eval_f1': 0.13955286186747243, 'eval_precision': 0.15407775850674205, 'eval_recall': 0.1568697324509305, 'eval_runtime': 146.74, 'eval_samples_per_second': 55.977, 'eval_steps_per_second': 3.503, 'epoch': 27.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-124767\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-124767\\preprocessor_config.json\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4291, 'learning_rate': 4e-05, 'epoch': 28.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bcb252a68d74dceb4a3ef11813a6b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to D:/models/ViT_Boat_Class_ViT\\checkpoint-129388\n",
      "Configuration saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-129388\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.202596664428711, 'eval_accuracy': 0.19369369369369369, 'eval_f1': 0.14226432379654091, 'eval_precision': 0.15626666022404226, 'eval_recall': 0.157976774339027, 'eval_runtime': 144.1344, 'eval_samples_per_second': 56.988, 'eval_steps_per_second': 3.566, 'epoch': 28.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-129388\\pytorch_model.bin\n",
      "Image processor saved in D:/models/ViT_Boat_Class_ViT\\checkpoint-129388\\preprocessor_config.json\n"
     ]
    }
   ],
   "source": [
    "#launch tensorboard\n",
    "%load_ext tensorboard\n",
    "EPOCHS = 100\n",
    "model_name = \"ViT\"\n",
    "checkpoint = \"google/vit-base-patch16-224\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "\n",
    "dataset_image_search = load_dataset(\"cringgaard/boats_dataset\" , use_auth_token=access_token, split=\"image_search\")\n",
    "for batch_size in batch_sizes:\n",
    "        tags = [model_name , \"Boat_Class\" , \"ViT\"]\n",
    "        name = \"_\".join(tags)\n",
    "        wandb.init(project=wandb_project, name=name , tags=tags)\n",
    "        torch.cuda.empty_cache()\n",
    "        c_names = dataset.column_names[1:]\n",
    "        c_names.remove('name')\n",
    "        # NOTE boat24 has wrong labels\n",
    "        # dataset_specific_test = dataset.remove_columns(c_names)\n",
    "        # dataset_boat24_specific_train = dataset_boat24.remove_columns(c_names)\n",
    "        # dataset_image_search_specific_train = dataset_image_search.remove_columns(c_names)\n",
    "        dataset_specific = dataset_image_search.remove_columns(c_names)\n",
    "\n",
    "\n",
    "        # # Debugging dataset\n",
    "        # dataset_specific = dataset_specific.select(range(1000))\n",
    "\n",
    "        # Split into train and eval\n",
    "        dataset_specific = dataset_specific.train_test_split(test_size=0.1 , seed=42)\n",
    "        \n",
    "        labels = dataset.features['name'].names\n",
    "        id2label = {int(i): label for i, label in enumerate(labels)}\n",
    "        label2id = {label : int(i) for i, label in enumerate(labels)}\n",
    "\n",
    "        labels_train_counts = np.bincount(dataset_specific['train']['name'] , minlength=len(labels))\n",
    "        labels_test_counts = np.bincount(dataset_specific['test']['name'] , minlength=len(labels))\n",
    "\n",
    "        # weights = np.array([1 if x == 0 else x for x in labels_train_counts])\n",
    "        # weights = (1/weights)\n",
    "        # weights /= weights.sum()\n",
    "        # weights = torch.tensor(weights, dtype=torch.float , device=torch.device(\"cuda:0\"))\n",
    "\n",
    "        # class WeightedCETrainer(Trainer):\n",
    "        #     def __init__(self, *args, **kwargs):\n",
    "        #         super().__init__(*args, **kwargs)\n",
    "        #     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        #         labels = inputs.get(\"labels\")\n",
    "        #         labels.to(torch.device(\"cuda:0\"))\n",
    "        #         outputs = model(**inputs)\n",
    "        #         logits = outputs.get(\"logits\")\n",
    "        #         # loss_fct = nn.CrossEntropyLoss(weight=weights , label_smoothing=0.1)\n",
    "        #         loss_fct = nn.CrossEntropyLoss(weight=weights)\n",
    "        #         loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        #         return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        def transforms(examples):\n",
    "            examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"img_path\"]]\n",
    "            examples[\"labels\"] = examples['name']\n",
    "            del examples[\"name\"]\n",
    "            del examples[\"img_path\"]\n",
    "            return examples\n",
    "        \n",
    "        data_collator = DefaultDataCollator()\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # model = BoatClassifier(device)\n",
    "        model = AutoModelForImageClassification.from_pretrained(\n",
    "             checkpoint,\n",
    "             num_labels=len(labels),\n",
    "             id2label=id2label,\n",
    "             label2id=label2id,\n",
    "             ignore_mismatched_sizes=True,\n",
    "             )\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=model_dir+name,\n",
    "            report_to=\"tensorboard\",\n",
    "            remove_unused_columns=False,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            logging_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            # eval_steps = 10,\n",
    "            # logging_steps = 10,\n",
    "            # save_steps = 10,\n",
    "            save_total_limit=30,\n",
    "            learning_rate=5e-5,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            gradient_accumulation_steps=1,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=EPOCHS,\n",
    "            warmup_ratio=0.1,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            # label_smoothing_factor=0.1,\n",
    "            # no_cuda=True\n",
    "            # push_to_hub=True,\n",
    "            # hub_strategy=\"end\",\n",
    "            # hub_model_id=\"boats_dataset\",\n",
    "            # hub_token=write_token,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=dataset_specific['train'].with_transform(transforms),\n",
    "            eval_dataset=dataset_specific['test'].with_transform(transforms),\n",
    "            tokenizer=image_processor,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        # # Plot Label Distribution For Training Data\n",
    "        # fig1 = plt.figure()\n",
    "        # ax = fig1.add_axes([0,0,1,1])\n",
    "        # ax.bar([label2id[x] for x in labels], labels_train_counts/dataset_specific['train'].__len__()) # Normalized\n",
    "        # ax.set_ylabel(\"Number of examples normalised\")\n",
    "        # ax.set_title(\"Label Distribution\")\n",
    "        # wandb.log({\"Label Distribution Train\": (fig1)})\n",
    "\n",
    "        # # Plot Label Distribution For Test Data\n",
    "        # fig2 = plt.figure()\n",
    "        # ax = fig2.add_axes([0,0,1,1])\n",
    "        # ax.bar([label2id[x] for x in labels], labels_test_counts/dataset_specific['test'].__len__()) # Normalized\n",
    "        # ax.set_ylabel(\"Number of examples normalised\")\n",
    "        # ax.set_title(\"Label Distribution\")\n",
    "        # wandb.log({\"Label Distribution Test\": (fig2)})\n",
    "\n",
    "        # # Log label2id\n",
    "        # wandb.log({\"Labels\": wandb.Table(data = list(zip(label2id.keys() , label2id.values())) , columns=[\"Label\" , \"ID\"])})\n",
    "\n",
    "        # Train Model\n",
    "        trainer.train()\n",
    "\n",
    "        # Save Model\n",
    "        trainer.save_model(model_dir+name+\"/best_model\")\n",
    "\n",
    "        pipeline = ImageClassificationPipeline(model=trainer.model, feature_extractor = trainer.tokenizer , framework=\"pt\", device=0)\n",
    "        predict_data = dataset_specific['test'].select(np.random.randint(0, len(dataset_specific['test']), 100))\n",
    "        images = [predict_data['img_path'][i] for i in range(100)]\n",
    "        predictions = pipeline(images)\n",
    "        prediction_table = []\n",
    "        for i in range(len(predictions)):\n",
    "            prediction_table.append([wandb.Image(images[i]) , predictions[i] , id2label[predict_data['name'][i]]])\n",
    "        columns = [\"Image\" , \"Label Predictions\" , \"True Label\"]\n",
    "        wandb.log({\"Image Predicitions\" : wandb.Table(data=prediction_table, columns=columns)})\n",
    "\n",
    "        metrics = trainer.evaluate(dataset_specific['test'].with_transform(transforms))\n",
    "        wandb.log(metrics)\n",
    "        wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multitask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import PreTrainedModel\n",
    "# from transformers.models.resnet.modeling_resnet import ImageClassifierOutputWithNoAttention\n",
    "# from typing import Optional\n",
    "\n",
    "# class MultitaskBoatClassifier(PreTrainedModel):\n",
    "#     def __init__(self, config, num_classes_list , label2id , id2label):\n",
    "#         super().__init__(config)\n",
    "#         self.num_classes_list = num_classes_list\n",
    "#         self.label2id = label2id\n",
    "#         self.id2label = id2label\n",
    "#         self.resnet = ResNetModel.from_pretrained(\"microsoft/resnet-18\" , num_labels =Name_Classes.__len__() , label2id=self.label2id , id2label=self.id2label)\n",
    "#         self.heads = nn.ModuleList([nn.Sequential(nn.Flatten() , nn.Linear(512, num_classes)) for num_classes in num_classes_list])\n",
    "#         self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         pixel_values: Optional[torch.FloatTensor] = None,\n",
    "#         labels: Optional[torch.LongTensor] = None,\n",
    "#         output_hidden_states: Optional[bool] = None,\n",
    "#         return_dict: Optional[bool] = None,\n",
    "#     ) -> ImageClassifierOutputWithNoAttention:\n",
    "#         r\"\"\"\n",
    "#         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "#             Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n",
    "#             config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "#         \"\"\"\n",
    "#         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "#         outputs = self.resnet(pixel_values, output_hidden_states=output_hidden_states, return_dict=return_dict)\n",
    "#         pooled_outputs = outputs.pooler_output if return_dict else outputs[1]\n",
    "#         class_logits = []\n",
    "#         for head in self.heads:\n",
    "#             class_logits.append(head(pooled_outputs))\n",
    "\n",
    "#         loss = None\n",
    "\n",
    "#         if labels is not None:\n",
    "#             loss = 0\n",
    "#             i = 0\n",
    "#             for logits in class_logits:\n",
    "#                 loss += self.criterion(logits, torch.transpose(labels,0,1)[i])\n",
    "#                 i += 1\n",
    "#             loss = loss / len(class_logits)\n",
    "        \n",
    "#         # print(class_logits[-1])\n",
    "#         # print(class_logits[-1].shape)\n",
    "#         return ImageClassifierOutputWithNoAttention(loss=loss, logits=class_logits[-1], hidden_states=outputs.hidden_states)\n",
    "\n",
    "\n",
    "# from transformers import PretrainedConfig\n",
    "# class MultitaskBoatClassifierConfig(PretrainedConfig):\n",
    "#     def __init__(self, num_classes_list = [Hull_Type_Classes.__len__(),Rigging_Type_Classes.__len__(),Construction_Classes.__len__(),Ballast_Type_Classes.__len__(),Designer_Classes.__len__() , Name_Classes.__len__()], label2id = None , **kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.num_classes_list = num_classes_list\n",
    "#         self.hidden_size = 512  # Specify the hidden size of the model\n",
    "#         self.num_labels = sum(num_classes_list)  # Total number of labels across all classification heads\n",
    "#         self.label2id = label2id if label2id is not None else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #launch tensorboard\n",
    "# %load_ext tensorboard\n",
    "\n",
    "# dataset_image_search = load_dataset(\"cringgaard/boats_dataset\" , use_auth_token=access_token, split=\"image_search\")\n",
    "# for batch_size in batch_sizes:\n",
    "#     # for label_combination in label_combinations:\n",
    "#         # label_types = [label_combination[0] , label_combination[1]]\n",
    "#         label_types = [\"Hull Type\" , \"Rigging Type\" ,  \"Construction\" , \"Ballast Type\" , \"Designer\"]\n",
    "#         tags = [model_name , \"Boat_Class\" , \"multitask\"]\n",
    "#         name = \"_\".join(tags)\n",
    "#         wandb.init(project=wandb_project, name=name , tags=tags)\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#         # FOR DEBUGGING\n",
    "#         # dataset_specific = dataset_image_search.select(np.random.randint(0, len(dataset_image_search), 1000))\n",
    "#         # dataset_specific = dataset_specific.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "\n",
    "\n",
    "#         dataset_specific = dataset_image_search.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "\n",
    "#         for label_type in label_types:\n",
    "#             labels = np.unique(dataset_specific['train'][label_type])\n",
    "#             labels_train_counts = np.bincount(dataset_specific['train'][label_type] , minlength=len(labels))\n",
    "#             labels_test_counts = np.bincount(dataset_specific['test'][label_type] , minlength=len(labels))\n",
    "#             labels_to_remove = np.where(labels_train_counts < 1)[0] # remove labels with less than 2 examples\n",
    "#             labels_to_remove = np.union1d(labels_to_remove, np.where(labels_test_counts < 1)[0])\n",
    "#             # dataset_specific['train'] = dataset_specific['train'].filter(lambda x: x[label_type] not in labels_to_remove)\n",
    "#             dataset_specific['test'] = dataset_specific['test'].filter(lambda x: x[label_type] not in labels_to_remove)            \n",
    "#             labels_train_counts = np.bincount(dataset_specific['train'][label_type] , minlength=len(labels))\n",
    "#             labels_test_counts = np.bincount(dataset_specific['test'][label_type] , minlength=len(labels))\n",
    "\n",
    "#         def transforms(examples):\n",
    "#             examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"img_path\"]]\n",
    "#             # examples[\"labels\"] = {label_type : examples[label_type] for label_type in label_types}\n",
    "#             labs = [examples[label_type] for label_type in label_types+[\"name\"]]\n",
    "#             examples['labels'] = list(map(list, zip(*labs)))\n",
    "#             # Remove all other keys in dictionary\n",
    "#             for key in list(examples.keys()):\n",
    "#                 if key not in [\"pixel_values\" , \"labels\"]:\n",
    "#                     del examples[key]\n",
    "#             return examples\n",
    "        \n",
    "#         data_collator = DefaultDataCollator()\n",
    "#         device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#         num_classes_list = [\n",
    "#                 Hull_Type_Classes.__len__(),\n",
    "#                 Rigging_Type_Classes.__len__(),\n",
    "#                 Construction_Classes.__len__(),\n",
    "#                 Ballast_Type_Classes.__len__(),\n",
    "#                 Designer_Classes.__len__(),\n",
    "#                 Name_Classes.__len__()\n",
    "#         ]\n",
    "\n",
    "#         labels = dataset.features['name'].names\n",
    "#         id2label = {int(i): label for i, label in enumerate(labels)}\n",
    "#         label2id = {label : int(i) for i, label in enumerate(labels)}\n",
    "        \n",
    "\n",
    "#         config = MultitaskBoatClassifierConfig(num_classes_list, label2id=label2id , id2label=id2label)\n",
    "#         model = MultitaskBoatClassifier(config , num_classes_list=num_classes_list , label2id=label2id , id2label=id2label)\n",
    "\n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir=model_dir+name,\n",
    "#             report_to=\"tensorboard\",\n",
    "#             remove_unused_columns=False,\n",
    "#             evaluation_strategy=\"epoch\",\n",
    "#             logging_strategy=\"epoch\",\n",
    "#             save_strategy=\"epoch\",\n",
    "#             eval_steps = 10,\n",
    "#             logging_steps = 10,\n",
    "#             # save_steps = 10,\n",
    "#             save_total_limit=1,\n",
    "#             learning_rate=5e-5,\n",
    "#             per_device_train_batch_size=batch_size,\n",
    "#             gradient_accumulation_steps=1,\n",
    "#             per_device_eval_batch_size=batch_size,\n",
    "#             num_train_epochs=EPOCHS,\n",
    "#             warmup_ratio=0.1,\n",
    "#         )\n",
    "\n",
    "#         trainer = Trainer(\n",
    "#             model=model,\n",
    "#             args=training_args,\n",
    "#             data_collator=data_collator,\n",
    "#             train_dataset=dataset_specific[\"train\"].with_transform(transforms),\n",
    "#             eval_dataset=dataset_specific[\"test\"].with_transform(transforms),\n",
    "#             tokenizer=image_processor,\n",
    "#             compute_metrics = compute_metrics,\n",
    "#         )\n",
    "\n",
    "#         trainer.train()\n",
    "\n",
    "#         # Save Model\n",
    "#         trainer.save_model(model_dir+name+\"/best_model\")\n",
    "\n",
    "#         pipeline = ImageClassificationPipeline(model=trainer.model, feature_extractor = trainer.tokenizer , framework=\"pt\", device=0)\n",
    "#         predict_data = dataset_specific['test'].select(np.random.randint(0, len(dataset_specific['test']), 100))\n",
    "#         images = [predict_data['img_path'][i] for i in range(100)]\n",
    "#         predictions = pipeline(images)\n",
    "#         prediction_table = []\n",
    "#         for i in range(len(predictions)):\n",
    "#             prediction_table.append([wandb.Image(images[i]) , predictions[i] , id2label[predict_data['name'][i]]])\n",
    "#         columns = [\"Image\" , \"Label Predictions\" , \"True Label\"]\n",
    "#         wandb.log({\"Image Predicitions\" : wandb.Table(data=prediction_table, columns=columns)})\n",
    "\n",
    "#         metrics = trainer.evaluate(dataset_specific['test'].with_transform(transforms))\n",
    "#         wandb.log(metrics)\n",
    "#         wandb.finish()\n",
    "#         wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
