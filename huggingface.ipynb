{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoImageProcessor\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "from transformers import DefaultDataCollator\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModel , AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "from transformers import pipeline\n",
    "\n",
    "from data.classes import *\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_PROJECT'] = \"Sailboat FGVC\"\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset boats_dataset (C:\\Users\\chris\\.cache\\huggingface\\datasets\\cringgaard___boats_dataset\\default\\0.0.0\\cf118af5708518fea28486aed25e2f1632c5b8d5e716255a840c5c012a2b161b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d9ac845fc74344887c2c7424f98508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "access_token = \"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\"\n",
    "# login(token=access_token)\n",
    "dataset = load_dataset(\"cringgaard/boats_dataset\" , use_auth_token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"google/vit-base-patch16-224\"\n",
    "model_name = \"ViT\"\n",
    "# checkpoint = \"microsoft/resnet-18\"\n",
    "# model_name = \"ResNet18\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_types = [\"Hull Type\" , \"Rigging Type\" ,  \"Construction\" , \"Ballast Type\" , \"Designer\"]\n",
    "batch_sizes = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    metrics = {}\n",
    "    metrics.update(accuracy.compute(predictions=predictions, references=labels))\n",
    "    metrics.update(f1.compute(predictions=predictions, references=labels , average=\"macro\"))\n",
    "    metrics.update(precision.compute(predictions=predictions, references=labels , average=\"macro\"))\n",
    "    metrics.update(recall.compute(predictions=predictions, references=labels , average=\"macro\"))\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics_multitask(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    f1_score = 0\n",
    "    precision_score = 0\n",
    "    recall_score = 0\n",
    "    accuracy_score = 0\n",
    "    for i , label in enumerate(label_types):\n",
    "        predictions, labels = eval_pred[1]\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        accuracy_score += accuracy.compute(predictions=predictions, references=labels).values()\n",
    "        f1_score += f1.compute(predictions=predictions, references=labels , average=\"macro\").values()\n",
    "        precision_score += precision.compute(predictions=predictions, references=labels , average=\"macro\").values()\n",
    "        recall_score += recall.compute(predictions=predictions, references=labels , average=\"macro\").values()\n",
    "        \n",
    "    accuracy_score /= len(label_types)\n",
    "    f1_score /= len(label_types)\n",
    "    precision_score /= len(label_types)\n",
    "    recall_score /= len(label_types)\n",
    "    metrics = {'accuracy' : accuracy_score , 'f1' : f1_score , 'precision' : precision_score , 'recall' : recall_score}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.3333333333333333,\n",
       " 'f1': 0.2222222222222222,\n",
       " 'precision': 0.16666666666666666,\n",
       " 'recall': 0.3333333333333333}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics((np.array([[0.1,0.3,0.6],[0.9,0.05,0.05],[0.9,0.1,0.05]]),np.array([1,0,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data_multitask = np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.compute_metrics_multitask(eval_pred)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics_multitask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# class WeightedLossTrainer(Trainer):\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "\n",
    "#     def compute_loss(self, model, inputs):\n",
    "        \n",
    "#         custom_loss = \n",
    "#         return custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "test_images = [_transforms(img.convert(\"RGB\")) for img in dataset['full'][0:16][\"img_path\"]]\n",
    "test_images = torch.stack(test_images)\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModel.from_pretrained(checkpoint)\n",
    "# model(torch.stack(test_images))['last_hidden_state']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskViT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultitaskViT, self).__init__()\n",
    "        self.base_model = AutoModel.from_pretrained(checkpoint , id2label = None , label2id = None)\n",
    "        self.linear1 = nn.Linear(768, 1024)\n",
    "        self.SoftMax = nn.Softmax(dim=1)\n",
    "        self.Hull_Type = nn.Linear(1024, (Hull_Type_Classes.__len__()))\n",
    "        self.Rigging_Type = nn.Linear(1024, (Rigging_Type_Classes.__len__()))\n",
    "        self.Construction = nn.Linear(1024, (Construction_Classes.__len__()))\n",
    "        self.Ballast_Type = nn.Linear(1024, (Ballast_Type_Classes.__len__()))\n",
    "        self.Designer = nn.Linear(1024, (Designer_Classes.__len__()))\n",
    "\n",
    "        \n",
    "    def forward(self, **inputs):\n",
    "        outputs = self.base_model(inputs['pixel_values'])['pooler_output']\n",
    "        outputs = nn.GELU()(outputs)\n",
    "        outputs = self.linear1(outputs)\n",
    "        hull_type = self.SoftMax(self.Hull_Type(outputs))\n",
    "        rigging_type = self.SoftMax(self.Rigging_Type(outputs))\n",
    "        construction = self.SoftMax(self.Construction(outputs))\n",
    "        ballast_type = self.SoftMax(self.Ballast_Type(outputs))\n",
    "        designer = self.SoftMax(self.Designer(outputs))\n",
    "        return hull_type, rigging_type, construction, ballast_type, designer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def compute_loss(self, model, inputs):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        model_output = model(**inputs)\n",
    "        total_loss = 0\n",
    "        for i in range (len(model_output)):    \n",
    "            total_loss += criterion(model_output[i], inputs[label_types[i]])\n",
    "        return total_loss\n",
    "    \n",
    "    def compute_metrics(self, eval_pred):\n",
    "        return compute_metrics_multitask(eval_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [16, 67] at entry 0 and [16, 38] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chris\\OneDrive\\Dokumenter\\GitHub\\SailFGVC\\huggingface.ipynb Cell 16\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chris/OneDrive/Dokumenter/GitHub/SailFGVC/huggingface.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m testModel \u001b[39m=\u001b[39m MultitaskViT()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/chris/OneDrive/Dokumenter/GitHub/SailFGVC/huggingface.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m outputs \u001b[39m=\u001b[39m testModel(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m'\u001b[39m : test_images})\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chris/OneDrive/Dokumenter/GitHub/SailFGVC/huggingface.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m([x\u001b[39m.\u001b[39mshape \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m outputs])\n",
      "File \u001b[1;32mc:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\chris\\OneDrive\\Dokumenter\\GitHub\\SailFGVC\\huggingface.ipynb Cell 16\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/OneDrive/Dokumenter/GitHub/SailFGVC/huggingface.ipynb#X24sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m ballast_type \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mSoftMax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mBallast_Type(outputs))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/OneDrive/Dokumenter/GitHub/SailFGVC/huggingface.ipynb#X24sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m designer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mSoftMax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDesigner(outputs))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chris/OneDrive/Dokumenter/GitHub/SailFGVC/huggingface.ipynb#X24sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack([hull_type, rigging_type, construction, ballast_type, designer])\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [16, 67] at entry 0 and [16, 38] at entry 1"
     ]
    }
   ],
   "source": [
    "testModel = MultitaskViT()\n",
    "outputs = testModel(**{'pixel_values' : test_images})\n",
    "print([x.shape for x in outputs])\n",
    "# compute_metrics_multitask(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcringgaard\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\chris\\OneDrive\\Dokumenter\\GitHub\\SailFGVC\\wandb\\run-20230418_155019-24tsrfvw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/cringgaard/Sailboat%20FGVC/runs/24tsrfvw\" target=\"_blank\">ViT_multitask</a></strong> to <a href=\"https://wandb.ai/cringgaard/Sailboat%20FGVC\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\cringgaard___boats_dataset\\default\\0.0.0\\cf118af5708518fea28486aed25e2f1632c5b8d5e716255a840c5c012a2b161b\\cache-c8764ee587ba8997.arrow and C:\\Users\\chris\\.cache\\huggingface\\datasets\\cringgaard___boats_dataset\\default\\0.0.0\\cf118af5708518fea28486aed25e2f1632c5b8d5e716255a840c5c012a2b161b\\cache-ccd9fe3ba27f29a3.arrow\n",
      "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `WANDB_LOG_MODEL` from true to `end` instead\n",
      "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8323\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 26000\n",
      "  Number of trainable parameters = 90435179\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a71075cbfa44fa9b3d36ab5190d2c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2081\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 26.6211, 'learning_rate': 1.9230769230769234e-07, 'epoch': 0.04}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc94b0c1810b436d96bc54be8bc90c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 21.6004, 'eval_samples_per_second': 96.341, 'eval_steps_per_second': 6.065, 'epoch': 0.04}\n"
     ]
    }
   ],
   "source": [
    "# for gradient_accumulation_step in batch_sizes:\n",
    "wandb.init(project=\"Sailboat FGVC\", name=model_name+\"_multitask\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "dataset_specific = dataset['full'].train_test_split(test_size=0.2, shuffle=True, seed=43)\n",
    "\n",
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"img_path\"]]\n",
    "    del examples[\"img_path\"]\n",
    "    del examples[\"name\"]\n",
    "    return examples\n",
    "\n",
    "\n",
    "# id2label = {float(i): label for i, label in enumerate(label_types)}\n",
    "# label2id = {label: float(i) for i, label in enumerate(label_types)}\n",
    "\n",
    "\n",
    "dataset_specific = dataset_specific.with_transform(transforms)\n",
    "# dataset_specific.set_format(type=\"torch\")\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "model = MultitaskViT()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"E:/models/\"+model_name+\"_multitask\",\n",
    "    report_to=\"wandb\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=100,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    # no_cuda=True\n",
    "    # push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = MultiTaskTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset_specific[\"train\"],\n",
    "    eval_dataset=dataset_specific[\"test\"],\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics_multitask,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for size in batch_sizes:\n",
    "#     for label_type in label_types:\n",
    "#         wandb.init(project=\"Sailboat FGVC\", name=model_name+\"_\"+label_type+\"_BatchSize_\"+str(size*16))\n",
    "#         torch.cuda.empty_cache()\n",
    "#         c_names = dataset[\"full\"].column_names[1:]\n",
    "#         c_names.remove(label_type)\n",
    "#         dataset_specific = dataset.remove_columns(c_names)\n",
    "\n",
    "#         labels = dataset_specific[\"full\"].unique(label_type)\n",
    "#         dataset_specific = dataset_specific['full'].train_test_split(test_size=0.2, shuffle=True, seed=43)\n",
    "\n",
    "#         labels_train = dataset_specific[\"train\"].unique(label_type)\n",
    "#         labels_test = dataset_specific[\"test\"].unique(label_type)\n",
    "\n",
    "#         print(sorted(labels_train))\n",
    "#         print(sorted(labels_test))\n",
    "#         labels_to_remove = [value for value in labels_test if value not in labels_train]\n",
    "#         print(labels_to_remove)\n",
    "#         # dataset_specific['test'] = dataset_specific[\"test\"].filter(lambda x: x[label_type] not in labels_to_remove)\n",
    "        \n",
    "#         # labels_test = dataset_specific[\"test\"].unique(label_type)\n",
    "#         # print(sorted(labels_test))\n",
    "\n",
    "\n",
    "\n",
    "#         labels = dataset['full'].features[label_type].names\n",
    "#         print(labels)\n",
    "\n",
    "#         def transforms(examples):\n",
    "#             examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"img_path\"]]\n",
    "#             examples[\"labels\"] = examples[label_type]\n",
    "#             del examples[label_type]\n",
    "#             del examples[\"img_path\"]\n",
    "#             return examples\n",
    "\n",
    "#         dataset_specific = dataset_specific.with_transform(transforms)\n",
    "#         data_collator = DefaultDataCollator()\n",
    "\n",
    "#         model = AutoModelForImageClassification.from_pretrained(\n",
    "#             checkpoint,\n",
    "#             num_labels=len(labels),\n",
    "#             # id2label=id2label,\n",
    "#             # label2id=label2id,\n",
    "#             use_auth_token=access_token,\n",
    "#             ignore_mismatched_sizes=True,\n",
    "#         )\n",
    "\n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir=\"E:/models/\"+model_name+\"_\"+label_type+\"_BatchSize_\"+str(size*16),\n",
    "#             report_to=\"wandb\",\n",
    "#             remove_unused_columns=False,\n",
    "#             evaluation_strategy=\"epoch\",\n",
    "#             save_strategy=\"epoch\",\n",
    "#             learning_rate=5e-5,\n",
    "#             per_device_train_batch_size=16,\n",
    "#             gradient_accumulation_steps=size,\n",
    "#             per_device_eval_batch_size=16,\n",
    "#             num_train_epochs=15,\n",
    "#             warmup_ratio=0.1,\n",
    "#             logging_steps=10,\n",
    "#             load_best_model_at_end=True,\n",
    "#             metric_for_best_model=\"f1\",\n",
    "#             # no_cuda=True\n",
    "#             # push_to_hub=True,\n",
    "#         )\n",
    "\n",
    "#         trainer = Trainer(\n",
    "#             model=model,\n",
    "#             args=training_args,\n",
    "#             data_collator=data_collator,\n",
    "#             train_dataset=dataset_specific[\"train\"],\n",
    "#             eval_dataset=dataset_specific[\"test\"],\n",
    "#             tokenizer=image_processor,\n",
    "#             compute_metrics=compute_metrics,\n",
    "#         )\n",
    "\n",
    "#         trainer.train()\n",
    "#         wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
